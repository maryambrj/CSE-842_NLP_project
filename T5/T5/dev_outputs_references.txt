Paper Summary This paper proposes an unsupervised learning model in which the network predicts what its state would look like at the next time step (at input layer and potentially other layers). When these states are observed, an error signal is computed by comparing the predictions and the observations. This error signal is fed back into the model. The authors show that this model is able to make good predictions on a toy dataset of rotating 3D faces as well as on natural videos. They also show that these features help perform supervised tasks. Strengths - The model is an interesting embodiment of the idea of predictive coding implemented using a end-to-end backpropable recurrent neural network architecture. - The idea of feeding forward an error signal is perhaps not used as widely as it could be, and this work shows a compelling example of using it. - Strong empirical results and relevant comparisons show that the model works well. - The authors present a detailed ablative analysis of the proposed model. Weaknesses - The model (esp. in Fig 1) is presented as a generalized predictive model where next step predictions are made at each layer. However, as discovered by running the experiments, only the predictions at the input layer are the ones that actually matter and the optimal choice seems to be to turn off the error signal from the higher layers. While the authors intend to address this in future work, I think this point merits some more discussion in the current work, given the way this model is presented. - The network currently lacks stochasticity and does not model the future as a multimodal distribution (However, this is mentioned as potential future work). Quality The experiments are well-designed and a detailed analysis is provided in the appendix. Clarity The paper is well-written and easy to follow. Originality Some deep models have previously been proposed that use predictive coding. However, the proposed model is most probably novel in the way it feds back the error signal and implements the entire model as a single differentiable network. Significance This paper will be of wide interest to the growing set of researchers working in unsupervised learning of time series. This helps draw attention to predictive coding as an important learning paradigm. Overall Good paper with detailed and well-designed experiments. The idea of feeding forward the error signal is not being used as much as it could be in our community. This work helps to draw the community's attention to this idea.
This paper proposes an interesting architecture for predicting future frames of videos using end-to-end trained deep predictive coding. The architecture is well presented and the paper is clearly written. The experiments are extensive and convincing, include ablation analyses, and show that this architecture performs well compared to other current methods. Overall, this is an interesting, solid contribution.
An interesting architecture that accumulates and continuously corrects mistakes as you see more and more of a video sequence. Clarity: The video you generated seems very helpful towards understanding the information flow in your network, it would be nice to link to it from the paper. "Our model with hyperparameters optimized for KITTI underperforms the model of Finn et al. (2016), but outperforms the previous state-of-the-art model by Mathieu et al. (2016)." It is not clear how different are the train and test sequences at the moment, since standard benchmarks do not really exist for video prediction and each author picks his/her favorite. Underperforming Finn et al 206 at the H3.6m Walking videos is a bit disappointing.
Learning about the physical structure and semantics of the world from video (without supervision) is a very hot area in computer vision and machine learning. In this paper, the authors investigate how the prediction of future image frames (inherently unsupervised) can help to deduce object/s structure and it's properties (in this case single object pose, category, and steering angle, (after a supervised linear readout step)) I enjoyed reading this paper, it is clear, interesting and proposes an original network architecture (PredNet) for video frame prediction that has produced promising results on both synthetic and natural images. Moreover, the extensive experimental evaluation and analysis the authors provide puts it on solid ground to which others can compare. The weaknesses: - the link to predictive coding should be better explained in the paper if it is to be used as a motivation for the prednet model. - any idea that the proposed method is learning an implicit model' of the objects' that make up the scene' is vague and far fetched, but it sounds great. Minor comment: Next to the number of labeled training examples (Fig.5), it would be interesting to see how much unsupervised training data was used to train your representations.
Paper Summary This paper proposes an unsupervised learning model in which the network predicts what its state would look like at the next time step (at input layer and potentially other layers). When these states are observed, an error signal is computed by comparing the predictions and the observations. This error signal is fed back into the model. The authors show that this model is able to make good predictions on a toy dataset of rotating 3D faces as well as on natural videos. They also show that these features help perform supervised tasks. Strengths - The model is an interesting embodiment of the idea of predictive coding implemented using a end-to-end backpropable recurrent neural network architecture. - The idea of feeding forward an error signal is perhaps not used as widely as it could be, and this work shows a compelling example of using it. - Strong empirical results and relevant comparisons show that the model works well. - The authors present a detailed ablative analysis of the proposed model. Weaknesses - The model (esp. in Fig 1) is presented as a generalized predictive model where next step predictions are made at each layer. However, as discovered by running the experiments, only the predictions at the input layer are the ones that actually matter and the optimal choice seems to be to turn off the error signal from the higher layers. While the authors intend to address this in future work, I think this point merits some more discussion in the current work, given the way this model is presented. - The network currently lacks stochasticity and does not model the future as a multimodal distribution (However, this is mentioned as potential future work). Quality The experiments are well-designed and a detailed analysis is provided in the appendix. Clarity The paper is well-written and easy to follow. Originality Some deep models have previously been proposed that use predictive coding. However, the proposed model is most probably novel in the way it feds back the error signal and implements the entire model as a single differentiable network. Significance This paper will be of wide interest to the growing set of researchers working in unsupervised learning of time series. This helps draw attention to predictive coding as an important learning paradigm. Overall Good paper with detailed and well-designed experiments. The idea of feeding forward the error signal is not being used as much as it could be in our community. This work helps to draw the community's attention to this idea.
In response to the helpful comments and questions, we have made several changes to the manuscript: 1. In our original manuscript, we primarily compared the PredNet to a CNN-LSTM Encoder-Decoder, which we chose because it serves as a tight control for the more novel elements of our architecture. However, we agree that it is useful to compare against other published architectures. One reason that this isn’t a trivial task is because a standard benchmark for next frame prediction arguably has yet to be established. Another issue is that published models are often optimized for performance on particular datasets, so evaluating competing models on KITTI/CalTech isn’t necessarily fair to those models. Searching the very recent literature, we found that the most relevant comparison to make is probably against the DFN model by Brabandere et al. (2016), which was recently presented at NIPS and was developed concurrently with our work. One of their experiments was on a 64x64 pixel, grayscale car-cam dataset. Training our KITTI model on this dataset, we outperform their results by 29%. To compare against another concurrently developed model, also published at NIPS 2016, we have additionally evaluated on the Human3.6M dataset (Ionescu et al., 2014). Our model with hyperparameters optimized for KITTI underperforms the model of Finn et al. (2016), but outperforms the previous state-of-the-art model by Mathieu et al. (2016). We have added all of these comparisons to the appendix. 2. To make the main text more clear and concise, and to properly explain all of the necessary details, we have moved portions of the steering angle analysis to the appendix. Our main point has been to demonstrate that our model learns a representation of important underlying factors, using other models as points of reference, so we have emphasized this. At the reviewer’s suggestion, we have added a video clip to help illustrate the flow of information in the network:
What's the difference between this work and "Deep Predictive Coding Networks" (ref [4]). Please explain it clearly in the text. Why not comparing the performance of prednet with the previous work of authors(ref[25])? The improvement in MSE of prednet over previous frame prediction is 0.005. How significant is this? Maybe you could provide images showing the difference between images in t+1 and t, and between t+1 and your prediction. This could reveal in which regions prednet does better than previous frame prediction.
Paper Summary This paper proposes an unsupervised learning model in which the network predicts what its state would look like at the next time step (at input layer and potentially other layers). When these states are observed, an error signal is computed by comparing the predictions and the observations. This error signal is fed back into the model. The authors show that this model is able to make good predictions on a toy dataset of rotating 3D faces as well as on natural videos. They also show that these features help perform supervised tasks. Strengths - The model is an interesting embodiment of the idea of predictive coding implemented using a end-to-end backpropable recurrent neural network architecture. - The idea of feeding forward an error signal is perhaps not used as widely as it could be, and this work shows a compelling example of using it. - Strong empirical results and relevant comparisons show that the model works well. - The authors present a detailed ablative analysis of the proposed model. Weaknesses - The model (esp. in Fig 1) is presented as a generalized predictive model where next step predictions are made at each layer. However, as discovered by running the experiments, only the predictions at the input layer are the ones that actually matter and the optimal choice seems to be to turn off the error signal from the higher layers. While the authors intend to address this in future work, I think this point merits some more discussion in the current work, given the way this model is presented. - The network currently lacks stochasticity and does not model the future as a multimodal distribution (However, this is mentioned as potential future work). Quality The experiments are well-designed and a detailed analysis is provided in the appendix. Clarity The paper is well-written and easy to follow. Originality Some deep models have previously been proposed that use predictive coding. However, the proposed model is most probably novel in the way it feds back the error signal and implements the entire model as a single differentiable network. Significance This paper will be of wide interest to the growing set of researchers working in unsupervised learning of time series. This helps draw attention to predictive coding as an important learning paradigm. Overall Good paper with detailed and well-designed experiments. The idea of feeding forward the error signal is not being used as much as it could be in our community. This work helps to draw the community's attention to this idea.
This paper proposes an interesting architecture for predicting future frames of videos using end-to-end trained deep predictive coding. The architecture is well presented and the paper is clearly written. The experiments are extensive and convincing, include ablation analyses, and show that this architecture performs well compared to other current methods. Overall, this is an interesting, solid contribution.
An interesting architecture that accumulates and continuously corrects mistakes as you see more and more of a video sequence. Clarity: The video you generated seems very helpful towards understanding the information flow in your network, it would be nice to link to it from the paper. "Our model with hyperparameters optimized for KITTI underperforms the model of Finn et al. (2016), but outperforms the previous state-of-the-art model by Mathieu et al. (2016)." It is not clear how different are the train and test sequences at the moment, since standard benchmarks do not really exist for video prediction and each author picks his/her favorite. Underperforming Finn et al 206 at the H3.6m Walking videos is a bit disappointing.
Learning about the physical structure and semantics of the world from video (without supervision) is a very hot area in computer vision and machine learning. In this paper, the authors investigate how the prediction of future image frames (inherently unsupervised) can help to deduce object/s structure and it's properties (in this case single object pose, category, and steering angle, (after a supervised linear readout step)) I enjoyed reading this paper, it is clear, interesting and proposes an original network architecture (PredNet) for video frame prediction that has produced promising results on both synthetic and natural images. Moreover, the extensive experimental evaluation and analysis the authors provide puts it on solid ground to which others can compare. The weaknesses: - the link to predictive coding should be better explained in the paper if it is to be used as a motivation for the prednet model. - any idea that the proposed method is learning an implicit model' of the objects' that make up the scene' is vague and far fetched, but it sounds great. Minor comment: Next to the number of labeled training examples (Fig.5), it would be interesting to see how much unsupervised training data was used to train your representations.
Paper Summary This paper proposes an unsupervised learning model in which the network predicts what its state would look like at the next time step (at input layer and potentially other layers). When these states are observed, an error signal is computed by comparing the predictions and the observations. This error signal is fed back into the model. The authors show that this model is able to make good predictions on a toy dataset of rotating 3D faces as well as on natural videos. They also show that these features help perform supervised tasks. Strengths - The model is an interesting embodiment of the idea of predictive coding implemented using a end-to-end backpropable recurrent neural network architecture. - The idea of feeding forward an error signal is perhaps not used as widely as it could be, and this work shows a compelling example of using it. - Strong empirical results and relevant comparisons show that the model works well. - The authors present a detailed ablative analysis of the proposed model. Weaknesses - The model (esp. in Fig 1) is presented as a generalized predictive model where next step predictions are made at each layer. However, as discovered by running the experiments, only the predictions at the input layer are the ones that actually matter and the optimal choice seems to be to turn off the error signal from the higher layers. While the authors intend to address this in future work, I think this point merits some more discussion in the current work, given the way this model is presented. - The network currently lacks stochasticity and does not model the future as a multimodal distribution (However, this is mentioned as potential future work). Quality The experiments are well-designed and a detailed analysis is provided in the appendix. Clarity The paper is well-written and easy to follow. Originality Some deep models have previously been proposed that use predictive coding. However, the proposed model is most probably novel in the way it feds back the error signal and implements the entire model as a single differentiable network. Significance This paper will be of wide interest to the growing set of researchers working in unsupervised learning of time series. This helps draw attention to predictive coding as an important learning paradigm. Overall Good paper with detailed and well-designed experiments. The idea of feeding forward the error signal is not being used as much as it could be in our community. This work helps to draw the community's attention to this idea.
In response to the helpful comments and questions, we have made several changes to the manuscript: 1. In our original manuscript, we primarily compared the PredNet to a CNN-LSTM Encoder-Decoder, which we chose because it serves as a tight control for the more novel elements of our architecture. However, we agree that it is useful to compare against other published architectures. One reason that this isn’t a trivial task is because a standard benchmark for next frame prediction arguably has yet to be established. Another issue is that published models are often optimized for performance on particular datasets, so evaluating competing models on KITTI/CalTech isn’t necessarily fair to those models. Searching the very recent literature, we found that the most relevant comparison to make is probably against the DFN model by Brabandere et al. (2016), which was recently presented at NIPS and was developed concurrently with our work. One of their experiments was on a 64x64 pixel, grayscale car-cam dataset. Training our KITTI model on this dataset, we outperform their results by 29%. To compare against another concurrently developed model, also published at NIPS 2016, we have additionally evaluated on the Human3.6M dataset (Ionescu et al., 2014). Our model with hyperparameters optimized for KITTI underperforms the model of Finn et al. (2016), but outperforms the previous state-of-the-art model by Mathieu et al. (2016). We have added all of these comparisons to the appendix. 2. To make the main text more clear and concise, and to properly explain all of the necessary details, we have moved portions of the steering angle analysis to the appendix. Our main point has been to demonstrate that our model learns a representation of important underlying factors, using other models as points of reference, so we have emphasized this. At the reviewer’s suggestion, we have added a video clip to help illustrate the flow of information in the network:
What's the difference between this work and "Deep Predictive Coding Networks" (ref [4]). Please explain it clearly in the text. Why not comparing the performance of prednet with the previous work of authors(ref[25])? The improvement in MSE of prednet over previous frame prediction is 0.005. How significant is this? Maybe you could provide images showing the difference between images in t+1 and t, and between t+1 and your prediction. This could reveal in which regions prednet does better than previous frame prediction.
Summary: This paper introduces a heuristic approach for training a deep directed generative model, where similar to the transition operator of a Markov chain each layer samples from the same conditional distribution. Similar to optimizing a variational lower bound, the approach is to approximate the gradient by replacing the posterior over latents with an alternative distribution. However, the approximating distribution is not updated to improve the lower bound but heuristically constructed in each step. A further difference to variational optimization is that the conditional distributions are optimized greedily rather than following the gradient of the joint log-likelihood. Review: The proposed approach is interesting and to me seems worth exploring more. Given that there are approaches for training the same class of models which are 1) theoretically more sound, 2) of similar computational complexity, and 3) work well in practice (e.g. Rezende & Mohamed, 2015), I am nevertheless not sure of its potential to generate impact. My bigger concern, however, is that the empirical evaluation is still quite limited. I appreciate the authors included proper estimates of the log-likelihood. This will enable and encourage future comparisons with this method on continuous MNIST. However, the authors should point out that the numbers taken from Wu et al. (2016) are not representative of the performance of a VAE. (From the paper: “Therefore, the log-likelihood values we report should not be compared directly against networks which have a more flexible observation model.” “Such observation models can easily achieve much higher log-likelihood scores, [...].”) Comparisons with inpainting results using other methods would have been nice. How practical is the proposed approach compared to other approaches? Similar to the diffusion approach by Sohl-Dickstein et al. (2015), the proposed approach seems to be both efficient and effective for inpainting. Not making this a bigger point and performing the proper evaluations seems like a missed opportunity. Minor: – I am missing citations for “ordered visible dimension sampling” – Typos and frequent incorrect use of citet and citep
Infusion training is a new, somewhat heuristic, procedure for training deep generative models. It's an interesting novel idea and a good paper, which has also been improved after the authors have been responsive to reviewer feedback.
We updated the paper, the main changes are: Added better log-likelihood estimates (one stochastic lower bound and one based on importance sampling) Added curves showing that log-likelihood bound improves as infusion training progresses Added references to related and relevant works: Rezende & Mohamed, 2015; Salimans et al. 2015; Dinh et al. 2014; Wu et al. 2016. Added results tables for likelihood comparison with models from the literature (Parzen estimates by Sohl-Dickstein et al. 2015 and AIS estimates by Wu et al. 2016) Added further experimental details. Added an Appendix containing details regarding the infusion rate schedule as well as examples of infused and sampled chains on cifar10 and celebA Corrected the typos mentioned by the reviewers
This paper trains a generative model which transforms noise into model samples by a gradual denoising process. It is similar to a generative model based on diffusion. Unlike the diffusion approach: - It uses only a small number of denoising steps, and is thus far more computationally efficient. - Rather than consisting of a reverse trajectory, the conditional chain for the approximate posterior jumps to q(z(0) | x), and then runs in the same direction as the generative model. This allows the inference chain to behave like a perturbation around the generative model, that pulls it towards the data. (This also seems somewhat related to ladder networks.) - There is no tractable variational bound on the log likelihood. I liked the idea, and found the visual sample quality given a short chain impressive. The inpainting results were particularly nice, since one shot inpainting is not possible under most generative modeling frameworks. It would be much more convincing to have a log likelihood comparison that doesn't depend on Parzen likelihoods. Detailed comments follow: Sec. 2: "theta(0) the" -> "theta(0) be the" "theta(t) the" -> "theta(t) be the" "what we will be using" -> "which we will be doing" I like that you infer q(z0|x), and then run inference in the same order as the generative chain. This reminds me slightly of ladder networks. "q*. Having learned" -> "q*. [paragraph break] Having learned" Sec 3.3: "learn to inverse" -> "learn to reverse" Sec. 4: "For each experiments" -> "For each experiment" How sensitive are your results to infusion rate? Sec. 5: "appears to provide more accurate models" I don't think you showed this -- there's no direct comparison to the Sohl-Dickstein paper. Fig 4. -- neat!
The paper presents a method for training a generative model via an iterative denoising procedure. The denoising process is initialized with a random sample from a crude approximation to the data distribution and produces a high quality sample via multiple denoising steps. Training is performed by setting-up a Markov chain that slowly blends propositions from the current denoising model with a real example from the data distribution; using this chain the current denoising model is updated towards reproducing the changed, "better", samples from the blending process. This is a clearly written paper that considers an interesting approach for training generative models. I was intrigued by the simplicity of the presented approach and really enjoyed reading the paper. The proposed method is novel although it has clear ties to other recent work aiming to use denoising models for sampling from distributions such as the work by Sohl-Dickstein and the recent work on using DAEs as generative models. I think this general direction of research is important. The proposed procedure takes inspiration from the perspective of generating samples by minimizing an energy function via transitions along a Markov chain and, if successful, it can potentially sidestep many problems of current procedures for training directed generative models such as: - convergence and mode coverage problems as in generative adversarial networks - problems with modeling multi-modal distributions which can arise when a too restrictive approximate inference model is paired with a powerful generative model That being said, another method that seems promising for addressing these issues that also has some superficially similarity to the presented work is the idea of combining Hamiltonian Monte Carlo inference with variational inference as in [1]. As such I am not entirely convinced that the method presented here will be able to perform better than the mentioned paper; although it might be simpler to train. Similarly, although I agree that using a MCMC chain to generate samples via a MC-EM like procedure is likely very costly I am not convinced such a procedure won't at least also work reasonably well for the simple MNIST example. In general a more direct comparison between different inference methods using an MCMC chain like procedure would be nice to have but I understand that this is perhaps out of the scope of this paper. One thing that I would have expected, however, is a direct comparison to the procedure from Sohl-Dickstein in terms of sampling steps and generation quality as it is so directly related. Other major points (good and bad): - Although in general the method is explained well some training details are missing. Most importantly it is never mentioned how alpha or omega are set (I am assuming omega is 0.01 as that is the increase mentioned in the experimental setup). It is also unclear how alpha affects the capabilities of the generator. While it intuitively seems reasonable to use a small alpha over many steps to ensure slow blending of the two distributions it is not clear how necessary this is or at what point the procedure would break (I assume alpha = 1 won't work as the generator then would have to magically denoise a sample from the relatively uninformative draw from p0 ?). The authors do mention in one of the figure captions that the denoising model does not produce good samples in only 1-2 steps but that might also be an artifact of training the model with small alpha (at least I see no a priori reason for this). More experiments should be carried out here. - No infusion chains or generating chains are shown for any of the more complicated data distributions, this is unfortunate as I feel these would be interesting to look at. - The paper does a good job at evaluating the model with respect to several different metrics. The bound on the log-likelihood is nice to have as well! - Unfortunately the current approach does not come with any theoretical guarantees. It is unclear for what choices of alpha the procedure will work and whether there is some deeper connection to MCMC sampling or energy based models. In my eyes this does not subtract from the value of the paper but would perhaps be worth a short sentence in the conclusion. Minor points: - The second reference seems broken - Figure 3 starts at 100 epochs and, as a result, contains little information. Perhaps it would be more useful to show the complete training procedure and put the x-axis on a log-scale ? - The explanation regarding the convolutional networks you use makes no sense to me. You write that you use the same structure as in the "Improved GANs" paper which, unlike your model, generates samples from a
Summary: This paper introduces a heuristic approach for training a deep directed generative model, where similar to the transition operator of a Markov chain each layer samples from the same conditional distribution. Similar to optimizing a variational lower bound, the approach is to approximate the gradient by replacing the posterior over latents with an alternative distribution. However, the approximating distribution is not updated to improve the lower bound but heuristically constructed in each step. A further difference to variational optimization is that the conditional distributions are optimized greedily rather than following the gradient of the joint log-likelihood. Review: The proposed approach is interesting and to me seems worth exploring more. Given that there are approaches for training the same class of models which are 1) theoretically more sound, 2) of similar computational complexity, and 3) work well in practice (e.g. Rezende & Mohamed, 2015), I am nevertheless not sure of its potential to generate impact. My bigger concern, however, is that the empirical evaluation is still quite limited. I appreciate the authors included proper estimates of the log-likelihood. This will enable and encourage future comparisons with this method on continuous MNIST. However, the authors should point out that the numbers taken from Wu et al. (2016) are not representative of the performance of a VAE. (From the paper: “Therefore, the log-likelihood values we report should not be compared directly against networks which have a more flexible observation model.” “Such observation models can easily achieve much higher log-likelihood scores, [...].”) Comparisons with inpainting results using other methods would have been nice. How practical is the proposed approach compared to other approaches? Similar to the diffusion approach by Sohl-Dickstein et al. (2015), the proposed approach seems to be both efficient and effective for inpainting. Not making this a bigger point and performing the proper evaluations seems like a missed opportunity. Minor: – I am missing citations for “ordered visible dimension sampling” – Typos and frequent incorrect use of citet and citep
Summary: This paper introduces a heuristic approach for training a deep directed generative model, where similar to the transition operator of a Markov chain each layer samples from the same conditional distribution. Similar to optimizing a variational lower bound, the approach is to approximate the gradient by replacing the posterior over latents with an alternative distribution. However, the approximating distribution is not updated to improve the lower bound but heuristically constructed in each step. A further difference to variational optimization is that the conditional distributions are optimized greedily rather than following the gradient of the joint log-likelihood. Review: The proposed approach is interesting and to me seems worth exploring more. Given that there are approaches for training the same class of models which are 1) theoretically more sound, 2) of similar computational complexity, and 3) work well in practice (e.g. Rezende & Mohamed, 2015), I am nevertheless not sure of its potential to generate impact. My bigger concern, however, is that the empirical evaluation is still quite limited. I appreciate the authors included proper estimates of the log-likelihood. This will enable and encourage future comparisons with this method on continuous MNIST. However, the authors should point out that the numbers taken from Wu et al. (2016) are not representative of the performance of a VAE. (From the paper: “Therefore, the log-likelihood values we report should not be compared directly against networks which have a more flexible observation model.” “Such observation models can easily achieve much higher log-likelihood scores, [...].”) Comparisons with inpainting results using other methods would have been nice. How practical is the proposed approach compared to other approaches? Similar to the diffusion approach by Sohl-Dickstein et al. (2015), the proposed approach seems to be both efficient and effective for inpainting. Not making this a bigger point and performing the proper evaluations seems like a missed opportunity. Minor: – I am missing citations for “ordered visible dimension sampling” – Typos and frequent incorrect use of citet and citep
Infusion training is a new, somewhat heuristic, procedure for training deep generative models. It's an interesting novel idea and a good paper, which has also been improved after the authors have been responsive to reviewer feedback.
We updated the paper, the main changes are: Added better log-likelihood estimates (one stochastic lower bound and one based on importance sampling) Added curves showing that log-likelihood bound improves as infusion training progresses Added references to related and relevant works: Rezende & Mohamed, 2015; Salimans et al. 2015; Dinh et al. 2014; Wu et al. 2016. Added results tables for likelihood comparison with models from the literature (Parzen estimates by Sohl-Dickstein et al. 2015 and AIS estimates by Wu et al. 2016) Added further experimental details. Added an Appendix containing details regarding the infusion rate schedule as well as examples of infused and sampled chains on cifar10 and celebA Corrected the typos mentioned by the reviewers
This paper trains a generative model which transforms noise into model samples by a gradual denoising process. It is similar to a generative model based on diffusion. Unlike the diffusion approach: - It uses only a small number of denoising steps, and is thus far more computationally efficient. - Rather than consisting of a reverse trajectory, the conditional chain for the approximate posterior jumps to q(z(0) | x), and then runs in the same direction as the generative model. This allows the inference chain to behave like a perturbation around the generative model, that pulls it towards the data. (This also seems somewhat related to ladder networks.) - There is no tractable variational bound on the log likelihood. I liked the idea, and found the visual sample quality given a short chain impressive. The inpainting results were particularly nice, since one shot inpainting is not possible under most generative modeling frameworks. It would be much more convincing to have a log likelihood comparison that doesn't depend on Parzen likelihoods. Detailed comments follow: Sec. 2: "theta(0) the" -> "theta(0) be the" "theta(t) the" -> "theta(t) be the" "what we will be using" -> "which we will be doing" I like that you infer q(z0|x), and then run inference in the same order as the generative chain. This reminds me slightly of ladder networks. "q*. Having learned" -> "q*. [paragraph break] Having learned" Sec 3.3: "learn to inverse" -> "learn to reverse" Sec. 4: "For each experiments" -> "For each experiment" How sensitive are your results to infusion rate? Sec. 5: "appears to provide more accurate models" I don't think you showed this -- there's no direct comparison to the Sohl-Dickstein paper. Fig 4. -- neat!
The paper presents a method for training a generative model via an iterative denoising procedure. The denoising process is initialized with a random sample from a crude approximation to the data distribution and produces a high quality sample via multiple denoising steps. Training is performed by setting-up a Markov chain that slowly blends propositions from the current denoising model with a real example from the data distribution; using this chain the current denoising model is updated towards reproducing the changed, "better", samples from the blending process. This is a clearly written paper that considers an interesting approach for training generative models. I was intrigued by the simplicity of the presented approach and really enjoyed reading the paper. The proposed method is novel although it has clear ties to other recent work aiming to use denoising models for sampling from distributions such as the work by Sohl-Dickstein and the recent work on using DAEs as generative models. I think this general direction of research is important. The proposed procedure takes inspiration from the perspective of generating samples by minimizing an energy function via transitions along a Markov chain and, if successful, it can potentially sidestep many problems of current procedures for training directed generative models such as: - convergence and mode coverage problems as in generative adversarial networks - problems with modeling multi-modal distributions which can arise when a too restrictive approximate inference model is paired with a powerful generative model That being said, another method that seems promising for addressing these issues that also has some superficially similarity to the presented work is the idea of combining Hamiltonian Monte Carlo inference with variational inference as in [1]. As such I am not entirely convinced that the method presented here will be able to perform better than the mentioned paper; although it might be simpler to train. Similarly, although I agree that using a MCMC chain to generate samples via a MC-EM like procedure is likely very costly I am not convinced such a procedure won't at least also work reasonably well for the simple MNIST example. In general a more direct comparison between different inference methods using an MCMC chain like procedure would be nice to have but I understand that this is perhaps out of the scope of this paper. One thing that I would have expected, however, is a direct comparison to the procedure from Sohl-Dickstein in terms of sampling steps and generation quality as it is so directly related. Other major points (good and bad): - Although in general the method is explained well some training details are missing. Most importantly it is never mentioned how alpha or omega are set (I am assuming omega is 0.01 as that is the increase mentioned in the experimental setup). It is also unclear how alpha affects the capabilities of the generator. While it intuitively seems reasonable to use a small alpha over many steps to ensure slow blending of the two distributions it is not clear how necessary this is or at what point the procedure would break (I assume alpha = 1 won't work as the generator then would have to magically denoise a sample from the relatively uninformative draw from p0 ?). The authors do mention in one of the figure captions that the denoising model does not produce good samples in only 1-2 steps but that might also be an artifact of training the model with small alpha (at least I see no a priori reason for this). More experiments should be carried out here. - No infusion chains or generating chains are shown for any of the more complicated data distributions, this is unfortunate as I feel these would be interesting to look at. - The paper does a good job at evaluating the model with respect to several different metrics. The bound on the log-likelihood is nice to have as well! - Unfortunately the current approach does not come with any theoretical guarantees. It is unclear for what choices of alpha the procedure will work and whether there is some deeper connection to MCMC sampling or energy based models. In my eyes this does not subtract from the value of the paper but would perhaps be worth a short sentence in the conclusion. Minor points: - The second reference seems broken - Figure 3 starts at 100 epochs and, as a result, contains little information. Perhaps it would be more useful to show the complete training procedure and put the x-axis on a log-scale ? - The explanation regarding the convolutional networks you use makes no sense to me. You write that you use the same structure as in the "Improved GANs" paper which, unlike your model, generates samples from a
Summary: This paper introduces a heuristic approach for training a deep directed generative model, where similar to the transition operator of a Markov chain each layer samples from the same conditional distribution. Similar to optimizing a variational lower bound, the approach is to approximate the gradient by replacing the posterior over latents with an alternative distribution. However, the approximating distribution is not updated to improve the lower bound but heuristically constructed in each step. A further difference to variational optimization is that the conditional distributions are optimized greedily rather than following the gradient of the joint log-likelihood. Review: The proposed approach is interesting and to me seems worth exploring more. Given that there are approaches for training the same class of models which are 1) theoretically more sound, 2) of similar computational complexity, and 3) work well in practice (e.g. Rezende & Mohamed, 2015), I am nevertheless not sure of its potential to generate impact. My bigger concern, however, is that the empirical evaluation is still quite limited. I appreciate the authors included proper estimates of the log-likelihood. This will enable and encourage future comparisons with this method on continuous MNIST. However, the authors should point out that the numbers taken from Wu et al. (2016) are not representative of the performance of a VAE. (From the paper: “Therefore, the log-likelihood values we report should not be compared directly against networks which have a more flexible observation model.” “Such observation models can easily achieve much higher log-likelihood scores, [...].”) Comparisons with inpainting results using other methods would have been nice. How practical is the proposed approach compared to other approaches? Similar to the diffusion approach by Sohl-Dickstein et al. (2015), the proposed approach seems to be both efficient and effective for inpainting. Not making this a bigger point and performing the proper evaluations seems like a missed opportunity. Minor: – I am missing citations for “ordered visible dimension sampling” – Typos and frequent incorrect use of citet and citep
In this paper a novel approach for anomaly detection is considered for the task of intrusion detection based on system call sequence. The system call sequence is regarded as a language, and multiple lstm-rnn language models are trained and ensembled. Diversity in the ensemble is achieved by choosing different hyper parameters for each lstm-LM. The combination of the LMs is done by averaging transformations of the likelihoods. I really like the fact that no attack data is used during training, and I like the LM and ensemble approach. The only high level drawbacks I have are the following, which might have a simple answer as I'm not an expert in this field: - Relaying of system calls seems weak: If the attacker has access to some "normal" sequences of system calls, all she can fool the system by interleaving its malicious system calls with normal ones, in a way that will artificially raise the likelihood of the sequence. - A few lines covering other anomaly detection tasks, where RNNs are used, can be added to the introduction, to give a better idea about the novelty of the approach.
This is a pure application paper: an application of LSTMs to host intrusion detection systems based upon observed system calls. And from the application standpoint, I don't believe this is a bad paper, the authors seem to achieve reasonable results from the method (though admittedly, I can't really judge the quality of these results without a lot more familiarity based upon current work in intrusion detection systems). However, in terms of the ICLR audience, I simply don't believe there is enough here to warrant substantial interest. As an example, the authors highlighted the distinction between network intrusion detection systems (NIDS) and host intrusion detection systems (HIDS), and felt that one review was completely unsuitable because they didn't realize this distinction when reading the paper. This is of course a crucial distinction from the security application side, but from the algorithmic/ML side, it's simply not that important, and the fact that there _has_ been previous work exactly on LSTMs for NIDS makes this paper unlikely to have a huge impact in the ICLR community. It would be much better suited to a security conference, where the application could be judged on its own merits, and the community would likely understand much better how significant these results were. Pros: + Nice application of LSTMs to HIDS task Cons: - Nothing really novel from the algorithmic/ML side - The significance of the results are difficult to assess without more formal understanding of the problem domains The PCs have thus decided that this paper isn't ready to appear at the conference.
In order to reflect the comments from reviewers, the revised manuscript now includes additional references to related work (Introduction) and more in-depth comparison with existing methods (Section 3.2) as well as more details of our future work (Conclusion).
This paper presents an anomaly-based host intrusion detection method. LSTM RNN is used to model the system-call sequences and the averaged sequence likelihood is then used to determine anomaly, which is the attack. This paper also compares an ensemble method with two baselines as classification model. +This is is well written and more of ideas are clearly presented. +It demonstrates an interesting application of LSTM sequential modeling to HIDS problem -The overall novelty is limited considering the major technical components like LSTM RNN and ensemble method are already established. -The contribution of the proposed ensemble method needs further evaluation because it is also possible to use ensemble ideas in kNN and kMC baselines.
The authors propose using an LSTM on a sequence of system calls to perform network intrusion detection (NIDS). The idea of using neural networks (in general) for NIDS is old [1]. The idea of using some sort of NN on top of a sequence of system calls for NIDS is published [2]. The idea of using LSTMs for NIDS is published [2]. The paper in [2] operates on counts of N-grams of system calls, rather than on the raw sequence, but that pre-processing does not seem "heavy" to me. Overall, the proposed system works as well as other proposed NIDS system, and the paper checks portability (which is good). But, on the con side, I don't see this paper as adding a lot to the state-of-the-art in NIDS. Nor does is the paper well-matched to ICLR. I didn't learn a lot about representations from this paper: many people have thrown LSTM at sequence problems. Therefore, I think it's below threshold for ICLR. The authors may wish to submit to a security conference. References: 1. Debar, Herve, Monique Becker, and Didier Siboni. "A neural network component for an intrusion detection system." Research in Security and Privacy, 1992. Proceedings., 1992 IEEE Computer Society Symposium on. IEEE, 1992. 2. Creech, Gideon, and Jiankun Hu. "A semantic approach to host-based intrusion detection systems using contiguousand discontiguous system call patterns." IEEE Transactions on Computers 63.4 (2014): 807-819. 3. Staudemeyer, Ralf C. "Applying long short-term memory recurrent neural networks to intrusion detection." South African Computer Journal 56.1 (2015).
In this paper a novel approach for anomaly detection is considered for the task of intrusion detection based on system call sequence. The system call sequence is regarded as a language, and multiple lstm-rnn language models are trained and ensembled. Diversity in the ensemble is achieved by choosing different hyper parameters for each lstm-LM. The combination of the LMs is done by averaging transformations of the likelihoods. I really like the fact that no attack data is used during training, and I like the LM and ensemble approach. The only high level drawbacks I have are the following, which might have a simple answer as I'm not an expert in this field: - Relaying of system calls seems weak: If the attacker has access to some "normal" sequences of system calls, all she can fool the system by interleaving its malicious system calls with normal ones, in a way that will artificially raise the likelihood of the sequence. - A few lines covering other anomaly detection tasks, where RNNs are used, can be added to the introduction, to give a better idea about the novelty of the approach.
In this paper a novel approach for anomaly detection is considered for the task of intrusion detection based on system call sequence. The system call sequence is regarded as a language, and multiple lstm-rnn language models are trained and ensembled. Diversity in the ensemble is achieved by choosing different hyper parameters for each lstm-LM. The combination of the LMs is done by averaging transformations of the likelihoods. I really like the fact that no attack data is used during training, and I like the LM and ensemble approach. The only high level drawbacks I have are the following, which might have a simple answer as I'm not an expert in this field: - Relaying of system calls seems weak: If the attacker has access to some "normal" sequences of system calls, all she can fool the system by interleaving its malicious system calls with normal ones, in a way that will artificially raise the likelihood of the sequence. - A few lines covering other anomaly detection tasks, where RNNs are used, can be added to the introduction, to give a better idea about the novelty of the approach.
This is a pure application paper: an application of LSTMs to host intrusion detection systems based upon observed system calls. And from the application standpoint, I don't believe this is a bad paper, the authors seem to achieve reasonable results from the method (though admittedly, I can't really judge the quality of these results without a lot more familiarity based upon current work in intrusion detection systems). However, in terms of the ICLR audience, I simply don't believe there is enough here to warrant substantial interest. As an example, the authors highlighted the distinction between network intrusion detection systems (NIDS) and host intrusion detection systems (HIDS), and felt that one review was completely unsuitable because they didn't realize this distinction when reading the paper. This is of course a crucial distinction from the security application side, but from the algorithmic/ML side, it's simply not that important, and the fact that there _has_ been previous work exactly on LSTMs for NIDS makes this paper unlikely to have a huge impact in the ICLR community. It would be much better suited to a security conference, where the application could be judged on its own merits, and the community would likely understand much better how significant these results were. Pros: + Nice application of LSTMs to HIDS task Cons: - Nothing really novel from the algorithmic/ML side - The significance of the results are difficult to assess without more formal understanding of the problem domains The PCs have thus decided that this paper isn't ready to appear at the conference.
In order to reflect the comments from reviewers, the revised manuscript now includes additional references to related work (Introduction) and more in-depth comparison with existing methods (Section 3.2) as well as more details of our future work (Conclusion).
This paper presents an anomaly-based host intrusion detection method. LSTM RNN is used to model the system-call sequences and the averaged sequence likelihood is then used to determine anomaly, which is the attack. This paper also compares an ensemble method with two baselines as classification model. +This is is well written and more of ideas are clearly presented. +It demonstrates an interesting application of LSTM sequential modeling to HIDS problem -The overall novelty is limited considering the major technical components like LSTM RNN and ensemble method are already established. -The contribution of the proposed ensemble method needs further evaluation because it is also possible to use ensemble ideas in kNN and kMC baselines.
The authors propose using an LSTM on a sequence of system calls to perform network intrusion detection (NIDS). The idea of using neural networks (in general) for NIDS is old [1]. The idea of using some sort of NN on top of a sequence of system calls for NIDS is published [2]. The idea of using LSTMs for NIDS is published [2]. The paper in [2] operates on counts of N-grams of system calls, rather than on the raw sequence, but that pre-processing does not seem "heavy" to me. Overall, the proposed system works as well as other proposed NIDS system, and the paper checks portability (which is good). But, on the con side, I don't see this paper as adding a lot to the state-of-the-art in NIDS. Nor does is the paper well-matched to ICLR. I didn't learn a lot about representations from this paper: many people have thrown LSTM at sequence problems. Therefore, I think it's below threshold for ICLR. The authors may wish to submit to a security conference. References: 1. Debar, Herve, Monique Becker, and Didier Siboni. "A neural network component for an intrusion detection system." Research in Security and Privacy, 1992. Proceedings., 1992 IEEE Computer Society Symposium on. IEEE, 1992. 2. Creech, Gideon, and Jiankun Hu. "A semantic approach to host-based intrusion detection systems using contiguousand discontiguous system call patterns." IEEE Transactions on Computers 63.4 (2014): 807-819. 3. Staudemeyer, Ralf C. "Applying long short-term memory recurrent neural networks to intrusion detection." South African Computer Journal 56.1 (2015).
In this paper a novel approach for anomaly detection is considered for the task of intrusion detection based on system call sequence. The system call sequence is regarded as a language, and multiple lstm-rnn language models are trained and ensembled. Diversity in the ensemble is achieved by choosing different hyper parameters for each lstm-LM. The combination of the LMs is done by averaging transformations of the likelihoods. I really like the fact that no attack data is used during training, and I like the LM and ensemble approach. The only high level drawbacks I have are the following, which might have a simple answer as I'm not an expert in this field: - Relaying of system calls seems weak: If the attacker has access to some "normal" sequences of system calls, all she can fool the system by interleaving its malicious system calls with normal ones, in a way that will artificially raise the likelihood of the sequence. - A few lines covering other anomaly detection tasks, where RNNs are used, can be added to the introduction, to give a better idea about the novelty of the approach.
SYNOPSIS: The paper proposes a new neural network-based model for reading comprehension (reading a passage of text and answering questions based on the passage). It is similar in spirit to several other recent models, with the main exception that it is able to predict answers of different lengths, as opposed to single words/tokens/entities. The authors compare their model on the Stanford Question Answering Dataset (SQuAD), and show improvements over the baselines, while apparently lagging quite far behind the current state of the art reported on the SQuAD leaderboard. THOUGHTS: The main novelty of the method is to be able to identify phrases of different lengths as possible answers to the question. However, both approaches considered -- using a POS pattern trie tree to filter out word sequences with POS tags matching those of answers in the training set, and brute-force enumeration of all phrases up to length N -- seem somewhat orthogonal to the idea of "learning end-to-end " an answer chunk extraction model. Furthermore, as other reviews have pointed out, it seems that the linguistic features actually contribute a lot to the final accuracy (Table 3). One could argue that these are easy to obtain using standard taggers, but it takes away even more from the idea of an "end-to-end trained" system. The paper is generally well written, but there are several crucial sections in parts describing the model where it was really hard for me to follow the descriptions. In particular, the attention mechanism seems fairly standard to me in a seq2seq sense (i.e. there is nothing architecturally novel about it, as is for instance the case with the Gated Attentive Reader). I may be missing something, but even after the clarification round I still don't understand how it is novel compared to standard attention used in for instance seq2seq models. Finally, although the method is shown to outperform the baseline method reported in the original paper introducing the SQuAD dataset, it currently seems to be 12th (out of 15 systems) on the leaderboard (
The program committee appreciates the authors' response to the clarification questions and one review. Unfortunately, reviews are not leaning sufficiently towards acceptance. Reviewers have concerns about the novelty of the approach, its effectiveness in terms of empirical performance, and lack of analysis that would help determine the main contributions of the proposed approach. Authors are strongly encouraged to incorporate reviewer feedback in future iterations of the work.
SYNOPSIS: The paper proposes a new neural network-based model for reading comprehension (reading a passage of text and answering questions based on the passage). It is similar in spirit to several other recent models, with the main exception that it is able to predict answers of different lengths, as opposed to single words/tokens/entities. The authors compare their model on the Stanford Question Answering Dataset (SQuAD), and show improvements over the baselines, while apparently lagging quite far behind the current state of the art reported on the SQuAD leaderboard. THOUGHTS: The main novelty of the method is to be able to identify phrases of different lengths as possible answers to the question. However, both approaches considered -- using a POS pattern trie tree to filter out word sequences with POS tags matching those of answers in the training set, and brute-force enumeration of all phrases up to length N -- seem somewhat orthogonal to the idea of "learning end-to-end " an answer chunk extraction model. Furthermore, as other reviews have pointed out, it seems that the linguistic features actually contribute a lot to the final accuracy (Table 3). One could argue that these are easy to obtain using standard taggers, but it takes away even more from the idea of an "end-to-end trained" system. The paper is generally well written, but there are several crucial sections in parts describing the model where it was really hard for me to follow the descriptions. In particular, the attention mechanism seems fairly standard to me in a seq2seq sense (i.e. there is nothing architecturally novel about it, as is for instance the case with the Gated Attentive Reader). I may be missing something, but even after the clarification round I still don't understand how it is novel compared to standard attention used in for instance seq2seq models. Finally, although the method is shown to outperform the baseline method reported in the original paper introducing the SQuAD dataset, it currently seems to be 12th (out of 15 systems) on the leaderboard (
The paper proposed an end-to-end machine learning model called dynamic reader for the machine reading comprehension task. Compared to earlier systems, the proposed model is able to extract and rank a set of answer candidates from a given document. There are many recent models focusing on building good question answering systems by extracting phrases from a given article. It seems that there are two different aspects that are unique in this work: 1. The use of convolution model, and 2. Dynamic chunking Convolution network is often only used for modeling character-based word embeddings so I am curious about its effectiveness on representing phrases. Therefore, I wish there could be more analysis on how effective it is, as the authors do not compare the convolution framework to other alternative approaches such as LSTM. The comparisons are important, as the authors uses uni-gram, bi-gram and tri-gram information in the convolution network, and it is not clear to me that if tri-gram information is still needed for LSTM models. The dynamic chunking is a good idea, and a very similar idea is proposed in some of the recent papers such as [Kenton et al, 16], which also targets at the same dataset. However, I would like to see more analysis on the dynamic chunking. Why this approach is a good approach for representing answer chunks? Given the representation of the chunk is constructed by the first and the end word representations generated by a convolution network, I am not sure about the ability of this representation to capture the long answer phrases. The authors do not use character base embedding but use some of the previous trained NLP models. It would be interesting if the authors could show what are the advantages and disadvantages of using linguistic features compared to character embeddings. In short, there are several good ideas proposed in the paper, but the lack of proper analysis make it difficult to judge how important the proposed techniques are.
SUMMARY. The paper propose a reading-comprehension question answering system for the recent QA task where answers of a question can be either single tokens or spans in the given text passage. The model first encodes the passage and the query using a recurrent neural network. With an attention mechanism the model calculates the importance of each word on the passage with respect to each word in the question. The encoded words in the passage are concatenated with the attention; the resulting vector is re-encoded with a further RNN. Three convolutional neural networks with different filter size (1,2,3-gram) are used to further capture local features. Candidate answers are selected either matching POS patterns of answers in the training set or choosing all possible text span until a certain length. Each candidate answer has three representations, one for each n-gram representation. The compatibility of these representation with the question representation is then calculated. The scores are combined linearly and used for calculating the probability of the candidate answer being the right answer for the question. The method is tested on the SQUAD dataset and outperforms the proposed baselines. ---------- OVERALL JUDGMENT The method presented in this paper is interesting but not very motivated in some points. For example, it is not explained why in the attention mechanism it is beneficial to concatenate the original passage encoding with the attention-weighted ones. The contributions of the paper are moderately novel proposing mainly the attention mechanism and the convolutional re-encoding. In fact, combining questions and passages and score their compatibility has became a fairly standard procedure in all QA models. ---------- DETAILED COMMENTS Equation (13) i should be s, not sl. I still do not understand the sentence " the best function is to concatenate the hidden stat of the fist word in a chunk in forward RNN and that of the last word in backward RNN". The RNN is over what all the words in the chunk? in the passage? The answer the authors gave in the response does not clarify this point.
SYNOPSIS: The paper proposes a new neural network-based model for reading comprehension (reading a passage of text and answering questions based on the passage). It is similar in spirit to several other recent models, with the main exception that it is able to predict answers of different lengths, as opposed to single words/tokens/entities. The authors compare their model on the Stanford Question Answering Dataset (SQuAD), and show improvements over the baselines, while apparently lagging quite far behind the current state of the art reported on the SQuAD leaderboard. THOUGHTS: The main novelty of the method is to be able to identify phrases of different lengths as possible answers to the question. However, both approaches considered -- using a POS pattern trie tree to filter out word sequences with POS tags matching those of answers in the training set, and brute-force enumeration of all phrases up to length N -- seem somewhat orthogonal to the idea of "learning end-to-end " an answer chunk extraction model. Furthermore, as other reviews have pointed out, it seems that the linguistic features actually contribute a lot to the final accuracy (Table 3). One could argue that these are easy to obtain using standard taggers, but it takes away even more from the idea of an "end-to-end trained" system. The paper is generally well written, but there are several crucial sections in parts describing the model where it was really hard for me to follow the descriptions. In particular, the attention mechanism seems fairly standard to me in a seq2seq sense (i.e. there is nothing architecturally novel about it, as is for instance the case with the Gated Attentive Reader). I may be missing something, but even after the clarification round I still don't understand how it is novel compared to standard attention used in for instance seq2seq models. Finally, although the method is shown to outperform the baseline method reported in the original paper introducing the SQuAD dataset, it currently seems to be 12th (out of 15 systems) on the leaderboard (
The program committee appreciates the authors' response to the clarification questions and one review. Unfortunately, reviews are not leaning sufficiently towards acceptance. Reviewers have concerns about the novelty of the approach, its effectiveness in terms of empirical performance, and lack of analysis that would help determine the main contributions of the proposed approach. Authors are strongly encouraged to incorporate reviewer feedback in future iterations of the work.
SYNOPSIS: The paper proposes a new neural network-based model for reading comprehension (reading a passage of text and answering questions based on the passage). It is similar in spirit to several other recent models, with the main exception that it is able to predict answers of different lengths, as opposed to single words/tokens/entities. The authors compare their model on the Stanford Question Answering Dataset (SQuAD), and show improvements over the baselines, while apparently lagging quite far behind the current state of the art reported on the SQuAD leaderboard. THOUGHTS: The main novelty of the method is to be able to identify phrases of different lengths as possible answers to the question. However, both approaches considered -- using a POS pattern trie tree to filter out word sequences with POS tags matching those of answers in the training set, and brute-force enumeration of all phrases up to length N -- seem somewhat orthogonal to the idea of "learning end-to-end " an answer chunk extraction model. Furthermore, as other reviews have pointed out, it seems that the linguistic features actually contribute a lot to the final accuracy (Table 3). One could argue that these are easy to obtain using standard taggers, but it takes away even more from the idea of an "end-to-end trained" system. The paper is generally well written, but there are several crucial sections in parts describing the model where it was really hard for me to follow the descriptions. In particular, the attention mechanism seems fairly standard to me in a seq2seq sense (i.e. there is nothing architecturally novel about it, as is for instance the case with the Gated Attentive Reader). I may be missing something, but even after the clarification round I still don't understand how it is novel compared to standard attention used in for instance seq2seq models. Finally, although the method is shown to outperform the baseline method reported in the original paper introducing the SQuAD dataset, it currently seems to be 12th (out of 15 systems) on the leaderboard (
The paper proposed an end-to-end machine learning model called dynamic reader for the machine reading comprehension task. Compared to earlier systems, the proposed model is able to extract and rank a set of answer candidates from a given document. There are many recent models focusing on building good question answering systems by extracting phrases from a given article. It seems that there are two different aspects that are unique in this work: 1. The use of convolution model, and 2. Dynamic chunking Convolution network is often only used for modeling character-based word embeddings so I am curious about its effectiveness on representing phrases. Therefore, I wish there could be more analysis on how effective it is, as the authors do not compare the convolution framework to other alternative approaches such as LSTM. The comparisons are important, as the authors uses uni-gram, bi-gram and tri-gram information in the convolution network, and it is not clear to me that if tri-gram information is still needed for LSTM models. The dynamic chunking is a good idea, and a very similar idea is proposed in some of the recent papers such as [Kenton et al, 16], which also targets at the same dataset. However, I would like to see more analysis on the dynamic chunking. Why this approach is a good approach for representing answer chunks? Given the representation of the chunk is constructed by the first and the end word representations generated by a convolution network, I am not sure about the ability of this representation to capture the long answer phrases. The authors do not use character base embedding but use some of the previous trained NLP models. It would be interesting if the authors could show what are the advantages and disadvantages of using linguistic features compared to character embeddings. In short, there are several good ideas proposed in the paper, but the lack of proper analysis make it difficult to judge how important the proposed techniques are.
SUMMARY. The paper propose a reading-comprehension question answering system for the recent QA task where answers of a question can be either single tokens or spans in the given text passage. The model first encodes the passage and the query using a recurrent neural network. With an attention mechanism the model calculates the importance of each word on the passage with respect to each word in the question. The encoded words in the passage are concatenated with the attention; the resulting vector is re-encoded with a further RNN. Three convolutional neural networks with different filter size (1,2,3-gram) are used to further capture local features. Candidate answers are selected either matching POS patterns of answers in the training set or choosing all possible text span until a certain length. Each candidate answer has three representations, one for each n-gram representation. The compatibility of these representation with the question representation is then calculated. The scores are combined linearly and used for calculating the probability of the candidate answer being the right answer for the question. The method is tested on the SQUAD dataset and outperforms the proposed baselines. ---------- OVERALL JUDGMENT The method presented in this paper is interesting but not very motivated in some points. For example, it is not explained why in the attention mechanism it is beneficial to concatenate the original passage encoding with the attention-weighted ones. The contributions of the paper are moderately novel proposing mainly the attention mechanism and the convolutional re-encoding. In fact, combining questions and passages and score their compatibility has became a fairly standard procedure in all QA models. ---------- DETAILED COMMENTS Equation (13) i should be s, not sl. I still do not understand the sentence " the best function is to concatenate the hidden stat of the fist word in a chunk in forward RNN and that of the last word in backward RNN". The RNN is over what all the words in the chunk? in the passage? The answer the authors gave in the response does not clarify this point.
I reviewed the manuscript on December 5th. Summary: The authors investigate the phenomenon of adversarial perturbations and ask whether one may build a system to independently detect an adversarial data point -- if one could detect an adversarial example, then might prevent a machine from automatically processing it. Importantly, the authors investigate whether it is possible to build an adversarial detector which is resilient to adversarial examples built against *both* the classifier and the detector. Their results suggest that training a detector in this more difficult setting still yields gains but does not entirely resolve the problem of detecting adversarial examples. Major comments: The authors describe a novel approach for dealing with adversarial examples from a security standpoint -- namely, build an independent system to detect the adversary so a human might intervene in those cases. A potential confound of this approach is that an adversary might respond by constructing adversarial examples to fool *both* the original classifier and the new detector. If that were possible, then this approach is moot since an attacker could always outwit the original system. To their credit, the authors show that building a 'dynamic' detector to detect adversarial examples but also be resilient to an adversary mitigates this potential escalation (worse case from 55% to 70% detection rate). Even though the 'dynamic' detector demonstrates positive gains, I am concerned about overall scores. Detecting adversarial examples at this rate would not be a reliable security procedure. My second comment is about 'model transferability'. My definition of 'model transferability' is different then the one used in the paper. My definition means that one constructs an adversarial example on one network and measures how well the adversarial examples attack a second trained model -- where the second model has been trained with different initial conditions. (The author's definition of 'transferability' is based on seeing how well the detector generalizes across training methods). 'Model transferability' (per my definition) is quite important because it measures how general an adversarial example is across all models -- and not specific to a given trained model. Different methods have different levels of 'model transferability' (Kurakin et al, 2016) and I am concerned how well the detector they built would be able to detect adversarial examples across *all models* and not just the trained model in question. In other words, a good detector would be able to detect adversarial examples from any network and not just one particularly trained network. This question seems largely unaddressed in this paper but perhaps I missed some subtle point in their descriptions. Minor comments: If there were any points in the bottom-left of the Figure 2 left, then this would be very important to see -- perhaps move the legend to highlight if the area contains no points. - X-axis label is wrong in Figure 2 right. Measure the transferability of the detector? - How is sigma labeled on Figure 5? - Whenever an image is constructed to be an 'adversary', has the image actually been tested to see if it is adversarial? In other words, does the adversarial image actually result in a misclassification by the original network?
The paper explores the automatic detection of adversarial examples by training a classifier to recognize them. This is an interesting direction, even though they are obviously concerns about training an adversary to circumvent this model. Nonetheless, the experimental results presented in the paper are of interest to the ICLR audience. Many of the initial reviewer comments appear to be appropriately addressed in the revision of the paper.
We have uploaded a new revision of the paper in which we have tried to address the reviewer comments. Here is a more detailed changelog: * Fixed a bug in the ImageNet experiment: we originally applied the softmax operator twice (once before and once after selecting the ten target classes). This did not affect the accuracy of the classification network but made the network harder to fool by adversaries for similar reasons as in the defensive distillation'' approach. We have corrected the issue in the updated version of the paper by applying softmax only after selection the ten target classes. To briefly summarize the corrected results: adversaries remain detectable with an accuracy of at least 85% (with the same exception as before, the basic iterative l2-based adversary for epsilon=400). More details are contained in the updated Section 4.2. Sorry for this error in the first revision. * Fixed wrong resolution in Figure 1 (16x16 instead of 8x8). Thanks to AnonReviewer3 for noting this. * Input range specified to be [0, 255] (Section 4.1.1). Thanks to AnonReviewer1 for requesting clarification on this. * Clarified computation of adversarial detectability (footnote in Section 4.1.1). * We discuss briefly that dynamic adversaries are based on stronger assumptions than static adversaries (footnote in Section 3.3) * Clarified that we did use version 1 of DeepFool (Section 3.1) * Fixed x-axis label in Figure 2 (right). Thanks to AnonReviewer2 for noting this. * Moved legend in Figure 2 (left) to upper right corner based on suggestion of AnonReviewer2. * Clarified choices of sigma in Figure 5 * Adding more details about the dynamic adversary training method.
This paper proposes a new idea to help defending adversarial examples by training a complementary classifier to detect them. The results of the paper show that adversarial examples in fact can be easily detected. Moreover, such detector generalizes well to other similar or weaker adversarial examples. The idea of this paper is simple but non-trivial. While no final scheme is proposed in the paper how this idea can help in building defensive systems, it actually provides a potential new direction. Based on its novelty, I suggest an acceptance. My main concern of this paper is about its completeness. No effective method is reported in the paper to defend the dynamic adversaries. It could be difficult to do so, but rather the paper doesn’t seem to put much effort to investigate this part. How difficult it is to defend the dynamic adversaries is an important and interesting question following the conclusions of this paper. Such investigation may essentially help improve our understanding of adversarial examples. That being said, the novelty of this paper is still significant. Minor comment: The paper needs to improve its clarity. Some important details are skipped in the paper. For example, the paper should provide more details about the dynamic adversaries and the dynamic adversary training method.
This paper explores an important angle to adversarial examples: the detection of adversarial images and their utilization for trainig more robust networks. This takes the competition between adversaries and models to a new level. The paper presents appealing evidence for the feasibility of robustifying networks by employing the a detector subnetwork that is trained particularly for the purpose of detecting the adversaries in a terget manner rather than just making the networks themselves robust to adversarial examples. The jointly trained primary/detector system is evaluated in various scenarios including the cases when the adversary generator has access to the model and those where they are generated in a generic way. The results of the paper show good improvements with the approach and present well motived thorough analyses to back the main message. The writing is clear and concise.
I reviewed the manuscript on December 5th. Summary: The authors investigate the phenomenon of adversarial perturbations and ask whether one may build a system to independently detect an adversarial data point -- if one could detect an adversarial example, then might prevent a machine from automatically processing it. Importantly, the authors investigate whether it is possible to build an adversarial detector which is resilient to adversarial examples built against *both* the classifier and the detector. Their results suggest that training a detector in this more difficult setting still yields gains but does not entirely resolve the problem of detecting adversarial examples. Major comments: The authors describe a novel approach for dealing with adversarial examples from a security standpoint -- namely, build an independent system to detect the adversary so a human might intervene in those cases. A potential confound of this approach is that an adversary might respond by constructing adversarial examples to fool *both* the original classifier and the new detector. If that were possible, then this approach is moot since an attacker could always outwit the original system. To their credit, the authors show that building a 'dynamic' detector to detect adversarial examples but also be resilient to an adversary mitigates this potential escalation (worse case from 55% to 70% detection rate). Even though the 'dynamic' detector demonstrates positive gains, I am concerned about overall scores. Detecting adversarial examples at this rate would not be a reliable security procedure. My second comment is about 'model transferability'. My definition of 'model transferability' is different then the one used in the paper. My definition means that one constructs an adversarial example on one network and measures how well the adversarial examples attack a second trained model -- where the second model has been trained with different initial conditions. (The author's definition of 'transferability' is based on seeing how well the detector generalizes across training methods). 'Model transferability' (per my definition) is quite important because it measures how general an adversarial example is across all models -- and not specific to a given trained model. Different methods have different levels of 'model transferability' (Kurakin et al, 2016) and I am concerned how well the detector they built would be able to detect adversarial examples across *all models* and not just the trained model in question. In other words, a good detector would be able to detect adversarial examples from any network and not just one particularly trained network. This question seems largely unaddressed in this paper but perhaps I missed some subtle point in their descriptions. Minor comments: If there were any points in the bottom-left of the Figure 2 left, then this would be very important to see -- perhaps move the legend to highlight if the area contains no points. - X-axis label is wrong in Figure 2 right. Measure the transferability of the detector? - How is sigma labeled on Figure 5? - Whenever an image is constructed to be an 'adversary', has the image actually been tested to see if it is adversarial? In other words, does the adversarial image actually result in a misclassification by the original network?
I reviewed the manuscript on December 5th. Summary: The authors investigate the phenomenon of adversarial perturbations and ask whether one may build a system to independently detect an adversarial data point -- if one could detect an adversarial example, then might prevent a machine from automatically processing it. Importantly, the authors investigate whether it is possible to build an adversarial detector which is resilient to adversarial examples built against *both* the classifier and the detector. Their results suggest that training a detector in this more difficult setting still yields gains but does not entirely resolve the problem of detecting adversarial examples. Major comments: The authors describe a novel approach for dealing with adversarial examples from a security standpoint -- namely, build an independent system to detect the adversary so a human might intervene in those cases. A potential confound of this approach is that an adversary might respond by constructing adversarial examples to fool *both* the original classifier and the new detector. If that were possible, then this approach is moot since an attacker could always outwit the original system. To their credit, the authors show that building a 'dynamic' detector to detect adversarial examples but also be resilient to an adversary mitigates this potential escalation (worse case from 55% to 70% detection rate). Even though the 'dynamic' detector demonstrates positive gains, I am concerned about overall scores. Detecting adversarial examples at this rate would not be a reliable security procedure. My second comment is about 'model transferability'. My definition of 'model transferability' is different then the one used in the paper. My definition means that one constructs an adversarial example on one network and measures how well the adversarial examples attack a second trained model -- where the second model has been trained with different initial conditions. (The author's definition of 'transferability' is based on seeing how well the detector generalizes across training methods). 'Model transferability' (per my definition) is quite important because it measures how general an adversarial example is across all models -- and not specific to a given trained model. Different methods have different levels of 'model transferability' (Kurakin et al, 2016) and I am concerned how well the detector they built would be able to detect adversarial examples across *all models* and not just the trained model in question. In other words, a good detector would be able to detect adversarial examples from any network and not just one particularly trained network. This question seems largely unaddressed in this paper but perhaps I missed some subtle point in their descriptions. Minor comments: If there were any points in the bottom-left of the Figure 2 left, then this would be very important to see -- perhaps move the legend to highlight if the area contains no points. - X-axis label is wrong in Figure 2 right. Measure the transferability of the detector? - How is sigma labeled on Figure 5? - Whenever an image is constructed to be an 'adversary', has the image actually been tested to see if it is adversarial? In other words, does the adversarial image actually result in a misclassification by the original network?
The paper explores the automatic detection of adversarial examples by training a classifier to recognize them. This is an interesting direction, even though they are obviously concerns about training an adversary to circumvent this model. Nonetheless, the experimental results presented in the paper are of interest to the ICLR audience. Many of the initial reviewer comments appear to be appropriately addressed in the revision of the paper.
We have uploaded a new revision of the paper in which we have tried to address the reviewer comments. Here is a more detailed changelog: * Fixed a bug in the ImageNet experiment: we originally applied the softmax operator twice (once before and once after selecting the ten target classes). This did not affect the accuracy of the classification network but made the network harder to fool by adversaries for similar reasons as in the defensive distillation'' approach. We have corrected the issue in the updated version of the paper by applying softmax only after selection the ten target classes. To briefly summarize the corrected results: adversaries remain detectable with an accuracy of at least 85% (with the same exception as before, the basic iterative l2-based adversary for epsilon=400). More details are contained in the updated Section 4.2. Sorry for this error in the first revision. * Fixed wrong resolution in Figure 1 (16x16 instead of 8x8). Thanks to AnonReviewer3 for noting this. * Input range specified to be [0, 255] (Section 4.1.1). Thanks to AnonReviewer1 for requesting clarification on this. * Clarified computation of adversarial detectability (footnote in Section 4.1.1). * We discuss briefly that dynamic adversaries are based on stronger assumptions than static adversaries (footnote in Section 3.3) * Clarified that we did use version 1 of DeepFool (Section 3.1) * Fixed x-axis label in Figure 2 (right). Thanks to AnonReviewer2 for noting this. * Moved legend in Figure 2 (left) to upper right corner based on suggestion of AnonReviewer2. * Clarified choices of sigma in Figure 5 * Adding more details about the dynamic adversary training method.
This paper proposes a new idea to help defending adversarial examples by training a complementary classifier to detect them. The results of the paper show that adversarial examples in fact can be easily detected. Moreover, such detector generalizes well to other similar or weaker adversarial examples. The idea of this paper is simple but non-trivial. While no final scheme is proposed in the paper how this idea can help in building defensive systems, it actually provides a potential new direction. Based on its novelty, I suggest an acceptance. My main concern of this paper is about its completeness. No effective method is reported in the paper to defend the dynamic adversaries. It could be difficult to do so, but rather the paper doesn’t seem to put much effort to investigate this part. How difficult it is to defend the dynamic adversaries is an important and interesting question following the conclusions of this paper. Such investigation may essentially help improve our understanding of adversarial examples. That being said, the novelty of this paper is still significant. Minor comment: The paper needs to improve its clarity. Some important details are skipped in the paper. For example, the paper should provide more details about the dynamic adversaries and the dynamic adversary training method.
This paper explores an important angle to adversarial examples: the detection of adversarial images and their utilization for trainig more robust networks. This takes the competition between adversaries and models to a new level. The paper presents appealing evidence for the feasibility of robustifying networks by employing the a detector subnetwork that is trained particularly for the purpose of detecting the adversaries in a terget manner rather than just making the networks themselves robust to adversarial examples. The jointly trained primary/detector system is evaluated in various scenarios including the cases when the adversary generator has access to the model and those where they are generated in a generic way. The results of the paper show good improvements with the approach and present well motived thorough analyses to back the main message. The writing is clear and concise.
I reviewed the manuscript on December 5th. Summary: The authors investigate the phenomenon of adversarial perturbations and ask whether one may build a system to independently detect an adversarial data point -- if one could detect an adversarial example, then might prevent a machine from automatically processing it. Importantly, the authors investigate whether it is possible to build an adversarial detector which is resilient to adversarial examples built against *both* the classifier and the detector. Their results suggest that training a detector in this more difficult setting still yields gains but does not entirely resolve the problem of detecting adversarial examples. Major comments: The authors describe a novel approach for dealing with adversarial examples from a security standpoint -- namely, build an independent system to detect the adversary so a human might intervene in those cases. A potential confound of this approach is that an adversary might respond by constructing adversarial examples to fool *both* the original classifier and the new detector. If that were possible, then this approach is moot since an attacker could always outwit the original system. To their credit, the authors show that building a 'dynamic' detector to detect adversarial examples but also be resilient to an adversary mitigates this potential escalation (worse case from 55% to 70% detection rate). Even though the 'dynamic' detector demonstrates positive gains, I am concerned about overall scores. Detecting adversarial examples at this rate would not be a reliable security procedure. My second comment is about 'model transferability'. My definition of 'model transferability' is different then the one used in the paper. My definition means that one constructs an adversarial example on one network and measures how well the adversarial examples attack a second trained model -- where the second model has been trained with different initial conditions. (The author's definition of 'transferability' is based on seeing how well the detector generalizes across training methods). 'Model transferability' (per my definition) is quite important because it measures how general an adversarial example is across all models -- and not specific to a given trained model. Different methods have different levels of 'model transferability' (Kurakin et al, 2016) and I am concerned how well the detector they built would be able to detect adversarial examples across *all models* and not just the trained model in question. In other words, a good detector would be able to detect adversarial examples from any network and not just one particularly trained network. This question seems largely unaddressed in this paper but perhaps I missed some subtle point in their descriptions. Minor comments: If there were any points in the bottom-left of the Figure 2 left, then this would be very important to see -- perhaps move the legend to highlight if the area contains no points. - X-axis label is wrong in Figure 2 right. Measure the transferability of the detector? - How is sigma labeled on Figure 5? - Whenever an image is constructed to be an 'adversary', has the image actually been tested to see if it is adversarial? In other words, does the adversarial image actually result in a misclassification by the original network?
This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data (e.g., character->word). Their approach does not require boundary information to segment the sequence in meaningful groups (like in Chung et al., 2016). Their model is organized as a set of layers that aim at capturing the information form different “level of abstraction”. The lowest level activate the upper one and decide when to update it based on a controller (or state cell, called c). A key feature of their model is that c is a discrete variable, allowing potentially fast inference time. However, this makes their model more challenging to learn, leading to the use of the straight-through estimator by Hinton, 2012. The experiment section is thorough and their model obtain competitive performance on several challenging tasks. The qualitative results show also that their model can capture natural boundaries. Overall this paper presents a strong and novel model with promising experimental results. On a minor note, I have few remarks/complaints about the writing and the related work: - In the introduction: “One of the key principles of learning in deep neural networks as well as in the human brain” : please provide evidence for the “human brain” part of this claim. “For modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances” I believe you re missing Mikolov et al. 2010 in the references. “in spite of the fact that hierarchical multiscale structures naturally exist in many temporal data”: missing reference to Lin et al., 1996 - in the related work: “A more recent model, the clockwork RNN (CW-RNN) (Koutnk et al., 2014) extends the hierarchicalRNN (El Hihi & Bengio, 1995)” : It extends the NARX model of Lin et al. 1996, not the El Hihi & Bengio, 1995. While the above models focus on online prediction problems, where a prediction needs to be made...”: I believe there is a lot of missing references, in particular to Socher’s work or older recursive networks. “The norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2012)”: this is not the first work using gradient clipping. I believe it was introduced in Mikolov et al., 2010. Missing references: “Recurrent neural network based language model.”, Mikolov et al. 2010 “Learning long-term dependencies in NARX recurrent neural networks”, Lin et al. 1996 “Sequence labelling in structured domains with hierarchical recurrent neural networks“, Fernandez et al. 2007 “Learning sequential tasks by incrementally adding higher orders”, Ring, 1993
This extension to RNNs is clearly motivated, and the details of the proposed method are sensible. The paper would have benefitted from more experiments such as those in Figure 5 teasing out the representations learned by this model.
This paper proposes a new multiscale recurrent neural network, where each layer has different time scale, and the scale is not fixed but variable and determined by a neural network. The method is elegantly formulated within a recurrent neural network framework, and shows the state-of-the-art performance on several benchmarks. The paper is well written. Question) Can you extend it to bidirectional RNN?
The paper proposes a modified RNN architecture with multiple layers, where higher layers are only passed lower layer states if a FLUSH operation is predicted, consisting of passing up the state and reseting the lower layer's state. In order to select one of three operations at each time step, the authors propose using the straight-through estimator with a slope-annealing trick during training. Empirical results and visualizations illustrate that the modified architecture performs well at boundary detection. Pros: - Paper is well-motivated, exceptionally well-composed - Provides promising initial results on learning hierarchical representations through visualizations and thorough experiments on language modeling and handwriting generation - The annealing trick with the straight-through estimator also seems potentially useful for other tasks containing discrete variables, and the trade-off in the flush operation is innovative. Cons: - In a couple cases the paper does not fully deliver. Empirical results on computational savings are not given, and hierarchy beyond a single level (where the data contains separators such as spaces and pen up/down) does not seem to be demonstrated. - It's unclear whether better downstream performance is due to use of hierarchical information or due to the architecture changes acting as regularization, something which could hopefully be addressed.
This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data (e.g., character->word). Their approach does not require boundary information to segment the sequence in meaningful groups (like in Chung et al., 2016). Their model is organized as a set of layers that aim at capturing the information form different “level of abstraction”. The lowest level activate the upper one and decide when to update it based on a controller (or state cell, called c). A key feature of their model is that c is a discrete variable, allowing potentially fast inference time. However, this makes their model more challenging to learn, leading to the use of the straight-through estimator by Hinton, 2012. The experiment section is thorough and their model obtain competitive performance on several challenging tasks. The qualitative results show also that their model can capture natural boundaries. Overall this paper presents a strong and novel model with promising experimental results. On a minor note, I have few remarks/complaints about the writing and the related work: - In the introduction: “One of the key principles of learning in deep neural networks as well as in the human brain” : please provide evidence for the “human brain” part of this claim. “For modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances” I believe you re missing Mikolov et al. 2010 in the references. “in spite of the fact that hierarchical multiscale structures naturally exist in many temporal data”: missing reference to Lin et al., 1996 - in the related work: “A more recent model, the clockwork RNN (CW-RNN) (Koutnk et al., 2014) extends the hierarchicalRNN (El Hihi & Bengio, 1995)” : It extends the NARX model of Lin et al. 1996, not the El Hihi & Bengio, 1995. While the above models focus on online prediction problems, where a prediction needs to be made...”: I believe there is a lot of missing references, in particular to Socher’s work or older recursive networks. “The norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2012)”: this is not the first work using gradient clipping. I believe it was introduced in Mikolov et al., 2010. Missing references: “Recurrent neural network based language model.”, Mikolov et al. 2010 “Learning long-term dependencies in NARX recurrent neural networks”, Lin et al. 1996 “Sequence labelling in structured domains with hierarchical recurrent neural networks“, Fernandez et al. 2007 “Learning sequential tasks by incrementally adding higher orders”, Ring, 1993
This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data (e.g., character->word). Their approach does not require boundary information to segment the sequence in meaningful groups (like in Chung et al., 2016). Their model is organized as a set of layers that aim at capturing the information form different “level of abstraction”. The lowest level activate the upper one and decide when to update it based on a controller (or state cell, called c). A key feature of their model is that c is a discrete variable, allowing potentially fast inference time. However, this makes their model more challenging to learn, leading to the use of the straight-through estimator by Hinton, 2012. The experiment section is thorough and their model obtain competitive performance on several challenging tasks. The qualitative results show also that their model can capture natural boundaries. Overall this paper presents a strong and novel model with promising experimental results. On a minor note, I have few remarks/complaints about the writing and the related work: - In the introduction: “One of the key principles of learning in deep neural networks as well as in the human brain” : please provide evidence for the “human brain” part of this claim. “For modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances” I believe you re missing Mikolov et al. 2010 in the references. “in spite of the fact that hierarchical multiscale structures naturally exist in many temporal data”: missing reference to Lin et al., 1996 - in the related work: “A more recent model, the clockwork RNN (CW-RNN) (Koutnk et al., 2014) extends the hierarchicalRNN (El Hihi & Bengio, 1995)” : It extends the NARX model of Lin et al. 1996, not the El Hihi & Bengio, 1995. While the above models focus on online prediction problems, where a prediction needs to be made...”: I believe there is a lot of missing references, in particular to Socher’s work or older recursive networks. “The norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2012)”: this is not the first work using gradient clipping. I believe it was introduced in Mikolov et al., 2010. Missing references: “Recurrent neural network based language model.”, Mikolov et al. 2010 “Learning long-term dependencies in NARX recurrent neural networks”, Lin et al. 1996 “Sequence labelling in structured domains with hierarchical recurrent neural networks“, Fernandez et al. 2007 “Learning sequential tasks by incrementally adding higher orders”, Ring, 1993
This extension to RNNs is clearly motivated, and the details of the proposed method are sensible. The paper would have benefitted from more experiments such as those in Figure 5 teasing out the representations learned by this model.
This paper proposes a new multiscale recurrent neural network, where each layer has different time scale, and the scale is not fixed but variable and determined by a neural network. The method is elegantly formulated within a recurrent neural network framework, and shows the state-of-the-art performance on several benchmarks. The paper is well written. Question) Can you extend it to bidirectional RNN?
The paper proposes a modified RNN architecture with multiple layers, where higher layers are only passed lower layer states if a FLUSH operation is predicted, consisting of passing up the state and reseting the lower layer's state. In order to select one of three operations at each time step, the authors propose using the straight-through estimator with a slope-annealing trick during training. Empirical results and visualizations illustrate that the modified architecture performs well at boundary detection. Pros: - Paper is well-motivated, exceptionally well-composed - Provides promising initial results on learning hierarchical representations through visualizations and thorough experiments on language modeling and handwriting generation - The annealing trick with the straight-through estimator also seems potentially useful for other tasks containing discrete variables, and the trade-off in the flush operation is innovative. Cons: - In a couple cases the paper does not fully deliver. Empirical results on computational savings are not given, and hierarchy beyond a single level (where the data contains separators such as spaces and pen up/down) does not seem to be demonstrated. - It's unclear whether better downstream performance is due to use of hierarchical information or due to the architecture changes acting as regularization, something which could hopefully be addressed.
This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data (e.g., character->word). Their approach does not require boundary information to segment the sequence in meaningful groups (like in Chung et al., 2016). Their model is organized as a set of layers that aim at capturing the information form different “level of abstraction”. The lowest level activate the upper one and decide when to update it based on a controller (or state cell, called c). A key feature of their model is that c is a discrete variable, allowing potentially fast inference time. However, this makes their model more challenging to learn, leading to the use of the straight-through estimator by Hinton, 2012. The experiment section is thorough and their model obtain competitive performance on several challenging tasks. The qualitative results show also that their model can capture natural boundaries. Overall this paper presents a strong and novel model with promising experimental results. On a minor note, I have few remarks/complaints about the writing and the related work: - In the introduction: “One of the key principles of learning in deep neural networks as well as in the human brain” : please provide evidence for the “human brain” part of this claim. “For modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances” I believe you re missing Mikolov et al. 2010 in the references. “in spite of the fact that hierarchical multiscale structures naturally exist in many temporal data”: missing reference to Lin et al., 1996 - in the related work: “A more recent model, the clockwork RNN (CW-RNN) (Koutnk et al., 2014) extends the hierarchicalRNN (El Hihi & Bengio, 1995)” : It extends the NARX model of Lin et al. 1996, not the El Hihi & Bengio, 1995. While the above models focus on online prediction problems, where a prediction needs to be made...”: I believe there is a lot of missing references, in particular to Socher’s work or older recursive networks. “The norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2012)”: this is not the first work using gradient clipping. I believe it was introduced in Mikolov et al., 2010. Missing references: “Recurrent neural network based language model.”, Mikolov et al. 2010 “Learning long-term dependencies in NARX recurrent neural networks”, Lin et al. 1996 “Sequence labelling in structured domains with hierarchical recurrent neural networks“, Fernandez et al. 2007 “Learning sequential tasks by incrementally adding higher orders”, Ring, 1993
The following statement best summarizes the contribution: "This paper shows that model free RL methods can learn how to gather information about physical properties of objects, even when this information is not available to a passive observer, and use this information to make decisions." So this is not a paper about new theory or algorithms, but rather about solving the problem of acquiring knowledge about the physics of the world around us, which is important for many problems and helps explain human performance in many tasks. There are still some concerns about the depth-of-analysis of the paper, but on balance, it is seen as an unconventional but interesting paper. As per AnonReviewer6, the final version could still aim to better address "What should other researchers focus on if they are trying to build agents that can understand physics intuitively (building off this work)?" -- Area chair
The machine learning field has profited from hundreds of yearly publications that propose a new algorithm, which outperforms baseline algorithms and has a theoretical proof for some idealized form of the algorithm. This paper, however, is different. It does not propose a new algorithm. It incidentally uses an RL algorithm, but many other RL algorithms could have been used just as well. This paper is not about new models or algorithms. So what is this paper about? It is an initial research step toward understanding objects and intuitive reasoning in physical worlds. Why is this important? Our best AI agents currently fail on simple control tasks and simple games, such as Montezuma’s Revenge, because when they look at a screen that has a ladder, a key and a skull they don’t immediately know that keys open doors, that skulls are probably hazardous and best avoided, that ladders allow us to defy gravity, etc. The understanding of physics, relations and objects enables children to solve seemingly simple problems that our best existing AI agents do not come close to begin to solve. Endowing our agents with knowledge of objects would help enormously with planning, reasoning and exploration. Yet, this is far from trivial. First, what is an object? It turns out this is not an easy question to answer and has baffled psychologists and philosophers. To simplify matters, in this paper we restrict our attention to rigid bodies. Staring at a thing in one’s hands is not enough to understand what it is. Vision or open loop perception in general is not enough. Trying to pull the thing apart to see whether it is a single entity might help. (Children do indeed love to tear things like paper apart to understand them.) Touching the thing to see what happens also helps --- perhaps it lights up and starts beeping. Further interaction might reveal that it enables us to remotely talk to someone else, to tweet, etc. Much of the knowledge gained is the result of interaction, that is perception by action. This paper is also about designing tasks to understand how agents acquire intuitive reasoning and experimentation strategies in physical worlds. An abundant body of evidence in psychology --- see eg the works of Gerd Gigerenzer
We would like to highlight to all reviewers that the experiments have been updated substantially since the previous version of the paper. In particular, we now include the missing results for Fist Pixels as requested by Reviewer 3, as well as additional experiments comparing our models to random baseline policies in both environments. In the process of making these updates we have have made several minor changes to the environments themselves. These changes have made the tower environment harder to solve, but do not alter any conclusions drawn from the experiments.
This paper addresses the question of how to utilize physical interactions to answer questions about physical outcomes. This question falls into a popular stream in ML community -- understanding physics. The paper moved a step further and worked on experimental setups where there is no prior about the physical properties/rules and it uses a deep reinforcement learning (DRL) technique to address the problem. My overall opinion about this paper is: an interesting attempt and idea, yet without a clear contribution. The experimental setups are quite interesting. The goal is to figure out which blocks are heavier or which blocks are glued together -- only by pushing and pulling objects around without any prior. The paper also shows reasonable performances on each task with detailed scenarios. While these experiments and results are interesting, the contribution is unclear. My main question is: does this result bring us any new insight? While the scenarios are interesting and focused on physical experiments, this is not any more different (potentially easier) than learning from playing games (e.g. Atari). In other words, are the tasks really different from other typical popular DRL tasks? To this end, I would have been more excited if authors showed some more new insights or experiments on learned representations and etc. Currently, the paper only discusses the factual outcome. For example, it describes the experimental setup and how much performances an agent could achieve. The authors could probably dissect the learned representations further, or discuss how the experimental results are linked to the human behavior or physical properties/laws. I am very in-between for my overall rating. I think the paper could have a deeper analysis. I however recommend the acceptance because of its merit of the idea. The followings are some detailed questions (not directly impacting my overall rating): (1) Page 2 "we assume that the agent has no prior knowledge about the physical properties of objects, or the laws of physics, and hence must interact with the objects in order to learn to answer questions about these properties.": why does one "must" interact with objects in order to learn about the properties? Can't we also learn through observation? (2) Figure 1right is missing a Y-axis label. (3) Page 3: A relating to bandit is interesting, but the formal approach is all based on DRL. (4) Page 5 "which makes distinguishing between the two heaviest blocks very difficult": I am a bit confused why having a small mass gap makes the task harder (unless it's really close to 0). Shouldn't a machine be possible to distinguish even a pixel difference of speed? If not, isn't this just because of the network architecture? (5) Page 5 "Since the agents exhibit similar performance using pixels and features we conduct the remaining experiments in this section using feature observations, since these agents are substantially faster to train.": How about at least showing a correlation of performances at the instance level (rather than average performances)? Even so, I think this is a bit of big conclusion. (6) Throughout the papers, I felt that many conclusions (e.g. difficulty and etc) are based on a particularly chosen training distribution. For example, how does an agent really know when the instance is any more difficult? Doesn't this really depend on the empirically learned distribution of training samples (i.e. P(m_3 | m_1, m_2), where m_i indicates masses of object 1, 2, and 3)? In other words, does what's hard/easy matter much unless this is more thoroughly tested over various types of distributions? (7) Any baseline approach?
This paper investigates the question of gathering information (answering question) through direct interaction with the environment. In that sense, it is closely related to "active learning" in supervised learning, or to the fundamental problem of exploration-exploitation in RL. The authors consider a specific instance of this problem in a physics domain and learn information-seeking policies using recent deep RL methods. The paper is mostly empirical and explores the effect of changing the cost of information (via the discount factor) on the structure of the learned policies. It also shows that general-purpose deep policy gradient methods are sufficient powerful to learn such tasks. The proposed environment is, to my knowledge, novel as well the task formulation in section 2. (And it would be very valuable to the the community if the environment would be open-sourced) The expression "latent structure/dynamics" is used throughout the text and the connection with bandits is mentioned in section 4. It therefore seems that authors aspire for more generality with their approach but the paper doesn't quite fully ground the proposed approach formally in any existing framework nor does it provide a new one completely. For example: how does your approach formalize the concept of "questions" and "answers" ? What makes a question "difficult" ? How do you quantify "difficulty" ? How do you define the "cost of information"? What are its units (bits, scalar reward), its semantics ? Do you you have an MDP or a POMDP ? What kind of MDP do you consider ? How do you define your discounted MDP ? What is the state and action spaces ? Some important problem structure under the "interaction/labeling/reward" paragraph of section 2 would be worth expressing directly in your definition of the MDP: labeling actions can only occur during the "labeling phase" and that the transition and reward functions have a specific structure (positive/negative, lead to absorbing state). The notion of "phase" could perhaps be implemented by considering an augmented state space : $tilde s = (s, phase)$
This paper purports to investigate the ability of RL agents to perform ‘physics experiments’ in an environment, to infer physical properties about the objects in that environment. The problem is very well motivated; indeed, inferring the physical properties of objects is a crucial skill for intelligent agents, and there has been relatively little work in this direction, particularly in deep RL. The paper is also well-written. As there are no architectural or theoretical contributions of the paper (and none are claimed), the main novelty comes in the task application – using a recurrent A3C model for two tasks that simulate an agent interacting with an environment to infer physical properties of objects. More specifically, two tasks are considered – moving blocks to determine their mass, and poking towers such that they fall to determine the number of rigid bodies they are composed of. These of course represent a very limited cross-section of the prerequisite abilities for an agent to understand physics. This in itself is not a bad thing, but since there is no comparison of different (simpler) RL agents on the tasks, it is difficult to determine if the tasks selected are challenging. As mentioned in the pre-review question, the ‘Which is Heavier’ task seems quite easy due to the actuator set-up, and the fact that the model simply must learn to take the difference between successive block positions (which are directly encoded as features in most experiments). Thus, it is not particularly surprising that the RL agent can solve the proposed tasks. The main claim beyond solving two proposed tasks related to physics simulation is that “the agents learn different strategies for these tasks that balance the cost of gathering information against the cost of making mistakes”. The ‘cost of gathering information’ is implemented by multiplying the reward with a value of gamma  1. This is somewhat interesting behaviour, but is hardly surprising given the problem setup. One item the authors highlight is that their approach of learning about physical object properties through interaction is different from many previous approaches, which use visual cues. However, the authors also note that this in itself is not novel, and has been explored in other work (e.g. Agrawal et al. (2016)). I think it’s crucial for the authors to discuss these approaches in more detail (potentially along with removing some other, less relevant information from the related work section), and specifically highlight why the proposed tasks in this paper are interesting compared to, for example, learning to move objects towards certain end positions by poking them. To discern the level of contribution of the paper, one must ask the following questions: 1) how much do these two tasks contribute (above previous work) to the goal of having agents learn the properties of objects by interaction; and 2) how much do the results of the RL agent on these tasks contribute to our understanding of agents that interact with their environment to learn physical properties of objects? It is difficult to know exactly, but due to the concerns outlined above, I am not convinced that the answers to (1) or (2) are “to a significant extent”. In particular, for (1), since the proposed agent is able to essentially solve both tasks, it is not clear that the tasks can be used to benchmark more advanced agents (e.g. it can’t be used as a set of bAbI-like tasks). Another possible concern, as pointed out by Reviewer 3, is that the description of the model is extremely concise. It would be nice to have, for example, a diagram illustrating the inputs and outputs to the model at each time step, to ease replication. Overall, it is important to make progress towards agents that can learn to discover physical properties of their environment, and the paper contributes in this direction. However, the technical contributions of this paper are rather limited – thus, it is not clear to what extent the paper pushes forward research in this direction beyond previous work that is mentioned. It would be nice, for example, to have some discussion about the future of agents that learn physics from interaction (speculation on more difficult versions of the tasks in this paper), and how the proposed approach fits into that picture. --------------- EDIT: score updated, see comments below
This paper presents interesting experimental findings that state-of-the-art deep reinforcement learning methods enable agent learning of latent (physical) properties in its environment. The paper formulates the problem of an agent labeling environmental properties after interacting with the environment based on its actions, and applies the deep reinforcement learning model to evaluate whether such learning is possible. The approach jointly learns the convolutional layers for pixel-based perception and its later layers for learning actions based on reinforcement signals. We have a mixed opinion about this paper. The paper is written clearly and presents interesting experimental findings. It introduces and formulates a problem potentially important for many robotics applications. Simultaneously, the paper suffers from lacking algorithmic contributions and missing (some of) crucial experiments to confirm its true benefits. Pros: + This paper introduces a new problem of learning latent properties in the agent's environment. + The paper presents a framework to appropriately combine existing tools to address the formulated problem. + The paper tries reinforcement learning with image inputs and fist-like actuator actions. This will lead to its direct application to robots. Cons: - Lacking algorithmic contribution: this paper applies existing tools/methods to solve the problem rather than developing something new or extending them. The approach essentially is training LSTMs with convolutional layers using the previous Asynchronous Advantage Actor Critic. - In the Towers experiment, the results of probably the most important setting, "Fist Pixels", are missing. This setting receiving pixel inputs and using the Fist actuator in a continuous space is the setting closest to real-world robots, and thus is very important to confirm whether the proposed approach will be directly applicable to real-world robots. However, Figure 5 is missing the results with this setting. Is there any reason behind this? - The paper lacks its comparison to any baseline methods. Without explicit baselines, it is difficult to see what the agent is really learning and what aspect of the proposed approach is benefitting the task. For instance, in the Towers task, how would an agent randomly pushing/hitting the tower (using 'Fist') a number of times and then passively observing its consequence to produce a label perform compared to this approach? That is, how would an approach with a fixed action policy (but with everything else) perform compared to the full deep reinforcement learning version?
The following statement best summarizes the contribution: "This paper shows that model free RL methods can learn how to gather information about physical properties of objects, even when this information is not available to a passive observer, and use this information to make decisions." So this is not a paper about new theory or algorithms, but rather about solving the problem of acquiring knowledge about the physics of the world around us, which is important for many problems and helps explain human performance in many tasks. There are still some concerns about the depth-of-analysis of the paper, but on balance, it is seen as an unconventional but interesting paper. As per AnonReviewer6, the final version could still aim to better address "What should other researchers focus on if they are trying to build agents that can understand physics intuitively (building off this work)?" -- Area chair
The machine learning field has profited from hundreds of yearly publications that propose a new algorithm, which outperforms baseline algorithms and has a theoretical proof for some idealized form of the algorithm. This paper, however, is different. It does not propose a new algorithm. It incidentally uses an RL algorithm, but many other RL algorithms could have been used just as well. This paper is not about new models or algorithms. So what is this paper about? It is an initial research step toward understanding objects and intuitive reasoning in physical worlds. Why is this important? Our best AI agents currently fail on simple control tasks and simple games, such as Montezuma’s Revenge, because when they look at a screen that has a ladder, a key and a skull they don’t immediately know that keys open doors, that skulls are probably hazardous and best avoided, that ladders allow us to defy gravity, etc. The understanding of physics, relations and objects enables children to solve seemingly simple problems that our best existing AI agents do not come close to begin to solve. Endowing our agents with knowledge of objects would help enormously with planning, reasoning and exploration. Yet, this is far from trivial. First, what is an object? It turns out this is not an easy question to answer and has baffled psychologists and philosophers. To simplify matters, in this paper we restrict our attention to rigid bodies. Staring at a thing in one’s hands is not enough to understand what it is. Vision or open loop perception in general is not enough. Trying to pull the thing apart to see whether it is a single entity might help. (Children do indeed love to tear things like paper apart to understand them.) Touching the thing to see what happens also helps --- perhaps it lights up and starts beeping. Further interaction might reveal that it enables us to remotely talk to someone else, to tweet, etc. Much of the knowledge gained is the result of interaction, that is perception by action. This paper is also about designing tasks to understand how agents acquire intuitive reasoning and experimentation strategies in physical worlds. An abundant body of evidence in psychology --- see eg the works of Gerd Gigerenzer
We would like to highlight to all reviewers that the experiments have been updated substantially since the previous version of the paper. In particular, we now include the missing results for Fist Pixels as requested by Reviewer 3, as well as additional experiments comparing our models to random baseline policies in both environments. In the process of making these updates we have have made several minor changes to the environments themselves. These changes have made the tower environment harder to solve, but do not alter any conclusions drawn from the experiments.
This paper addresses the question of how to utilize physical interactions to answer questions about physical outcomes. This question falls into a popular stream in ML community -- understanding physics. The paper moved a step further and worked on experimental setups where there is no prior about the physical properties/rules and it uses a deep reinforcement learning (DRL) technique to address the problem. My overall opinion about this paper is: an interesting attempt and idea, yet without a clear contribution. The experimental setups are quite interesting. The goal is to figure out which blocks are heavier or which blocks are glued together -- only by pushing and pulling objects around without any prior. The paper also shows reasonable performances on each task with detailed scenarios. While these experiments and results are interesting, the contribution is unclear. My main question is: does this result bring us any new insight? While the scenarios are interesting and focused on physical experiments, this is not any more different (potentially easier) than learning from playing games (e.g. Atari). In other words, are the tasks really different from other typical popular DRL tasks? To this end, I would have been more excited if authors showed some more new insights or experiments on learned representations and etc. Currently, the paper only discusses the factual outcome. For example, it describes the experimental setup and how much performances an agent could achieve. The authors could probably dissect the learned representations further, or discuss how the experimental results are linked to the human behavior or physical properties/laws. I am very in-between for my overall rating. I think the paper could have a deeper analysis. I however recommend the acceptance because of its merit of the idea. The followings are some detailed questions (not directly impacting my overall rating): (1) Page 2 "we assume that the agent has no prior knowledge about the physical properties of objects, or the laws of physics, and hence must interact with the objects in order to learn to answer questions about these properties.": why does one "must" interact with objects in order to learn about the properties? Can't we also learn through observation? (2) Figure 1right is missing a Y-axis label. (3) Page 3: A relating to bandit is interesting, but the formal approach is all based on DRL. (4) Page 5 "which makes distinguishing between the two heaviest blocks very difficult": I am a bit confused why having a small mass gap makes the task harder (unless it's really close to 0). Shouldn't a machine be possible to distinguish even a pixel difference of speed? If not, isn't this just because of the network architecture? (5) Page 5 "Since the agents exhibit similar performance using pixels and features we conduct the remaining experiments in this section using feature observations, since these agents are substantially faster to train.": How about at least showing a correlation of performances at the instance level (rather than average performances)? Even so, I think this is a bit of big conclusion. (6) Throughout the papers, I felt that many conclusions (e.g. difficulty and etc) are based on a particularly chosen training distribution. For example, how does an agent really know when the instance is any more difficult? Doesn't this really depend on the empirically learned distribution of training samples (i.e. P(m_3 | m_1, m_2), where m_i indicates masses of object 1, 2, and 3)? In other words, does what's hard/easy matter much unless this is more thoroughly tested over various types of distributions? (7) Any baseline approach?
This paper investigates the question of gathering information (answering question) through direct interaction with the environment. In that sense, it is closely related to "active learning" in supervised learning, or to the fundamental problem of exploration-exploitation in RL. The authors consider a specific instance of this problem in a physics domain and learn information-seeking policies using recent deep RL methods. The paper is mostly empirical and explores the effect of changing the cost of information (via the discount factor) on the structure of the learned policies. It also shows that general-purpose deep policy gradient methods are sufficient powerful to learn such tasks. The proposed environment is, to my knowledge, novel as well the task formulation in section 2. (And it would be very valuable to the the community if the environment would be open-sourced) The expression "latent structure/dynamics" is used throughout the text and the connection with bandits is mentioned in section 4. It therefore seems that authors aspire for more generality with their approach but the paper doesn't quite fully ground the proposed approach formally in any existing framework nor does it provide a new one completely. For example: how does your approach formalize the concept of "questions" and "answers" ? What makes a question "difficult" ? How do you quantify "difficulty" ? How do you define the "cost of information"? What are its units (bits, scalar reward), its semantics ? Do you you have an MDP or a POMDP ? What kind of MDP do you consider ? How do you define your discounted MDP ? What is the state and action spaces ? Some important problem structure under the "interaction/labeling/reward" paragraph of section 2 would be worth expressing directly in your definition of the MDP: labeling actions can only occur during the "labeling phase" and that the transition and reward functions have a specific structure (positive/negative, lead to absorbing state). The notion of "phase" could perhaps be implemented by considering an augmented state space : $tilde s = (s, phase)$
This paper purports to investigate the ability of RL agents to perform ‘physics experiments’ in an environment, to infer physical properties about the objects in that environment. The problem is very well motivated; indeed, inferring the physical properties of objects is a crucial skill for intelligent agents, and there has been relatively little work in this direction, particularly in deep RL. The paper is also well-written. As there are no architectural or theoretical contributions of the paper (and none are claimed), the main novelty comes in the task application – using a recurrent A3C model for two tasks that simulate an agent interacting with an environment to infer physical properties of objects. More specifically, two tasks are considered – moving blocks to determine their mass, and poking towers such that they fall to determine the number of rigid bodies they are composed of. These of course represent a very limited cross-section of the prerequisite abilities for an agent to understand physics. This in itself is not a bad thing, but since there is no comparison of different (simpler) RL agents on the tasks, it is difficult to determine if the tasks selected are challenging. As mentioned in the pre-review question, the ‘Which is Heavier’ task seems quite easy due to the actuator set-up, and the fact that the model simply must learn to take the difference between successive block positions (which are directly encoded as features in most experiments). Thus, it is not particularly surprising that the RL agent can solve the proposed tasks. The main claim beyond solving two proposed tasks related to physics simulation is that “the agents learn different strategies for these tasks that balance the cost of gathering information against the cost of making mistakes”. The ‘cost of gathering information’ is implemented by multiplying the reward with a value of gamma  1. This is somewhat interesting behaviour, but is hardly surprising given the problem setup. One item the authors highlight is that their approach of learning about physical object properties through interaction is different from many previous approaches, which use visual cues. However, the authors also note that this in itself is not novel, and has been explored in other work (e.g. Agrawal et al. (2016)). I think it’s crucial for the authors to discuss these approaches in more detail (potentially along with removing some other, less relevant information from the related work section), and specifically highlight why the proposed tasks in this paper are interesting compared to, for example, learning to move objects towards certain end positions by poking them. To discern the level of contribution of the paper, one must ask the following questions: 1) how much do these two tasks contribute (above previous work) to the goal of having agents learn the properties of objects by interaction; and 2) how much do the results of the RL agent on these tasks contribute to our understanding of agents that interact with their environment to learn physical properties of objects? It is difficult to know exactly, but due to the concerns outlined above, I am not convinced that the answers to (1) or (2) are “to a significant extent”. In particular, for (1), since the proposed agent is able to essentially solve both tasks, it is not clear that the tasks can be used to benchmark more advanced agents (e.g. it can’t be used as a set of bAbI-like tasks). Another possible concern, as pointed out by Reviewer 3, is that the description of the model is extremely concise. It would be nice to have, for example, a diagram illustrating the inputs and outputs to the model at each time step, to ease replication. Overall, it is important to make progress towards agents that can learn to discover physical properties of their environment, and the paper contributes in this direction. However, the technical contributions of this paper are rather limited – thus, it is not clear to what extent the paper pushes forward research in this direction beyond previous work that is mentioned. It would be nice, for example, to have some discussion about the future of agents that learn physics from interaction (speculation on more difficult versions of the tasks in this paper), and how the proposed approach fits into that picture. --------------- EDIT: score updated, see comments below
This paper presents interesting experimental findings that state-of-the-art deep reinforcement learning methods enable agent learning of latent (physical) properties in its environment. The paper formulates the problem of an agent labeling environmental properties after interacting with the environment based on its actions, and applies the deep reinforcement learning model to evaluate whether such learning is possible. The approach jointly learns the convolutional layers for pixel-based perception and its later layers for learning actions based on reinforcement signals. We have a mixed opinion about this paper. The paper is written clearly and presents interesting experimental findings. It introduces and formulates a problem potentially important for many robotics applications. Simultaneously, the paper suffers from lacking algorithmic contributions and missing (some of) crucial experiments to confirm its true benefits. Pros: + This paper introduces a new problem of learning latent properties in the agent's environment. + The paper presents a framework to appropriately combine existing tools to address the formulated problem. + The paper tries reinforcement learning with image inputs and fist-like actuator actions. This will lead to its direct application to robots. Cons: - Lacking algorithmic contribution: this paper applies existing tools/methods to solve the problem rather than developing something new or extending them. The approach essentially is training LSTMs with convolutional layers using the previous Asynchronous Advantage Actor Critic. - In the Towers experiment, the results of probably the most important setting, "Fist Pixels", are missing. This setting receiving pixel inputs and using the Fist actuator in a continuous space is the setting closest to real-world robots, and thus is very important to confirm whether the proposed approach will be directly applicable to real-world robots. However, Figure 5 is missing the results with this setting. Is there any reason behind this? - The paper lacks its comparison to any baseline methods. Without explicit baselines, it is difficult to see what the agent is really learning and what aspect of the proposed approach is benefitting the task. For instance, in the Towers task, how would an agent randomly pushing/hitting the tower (using 'Fist') a number of times and then passively observing its consequence to produce a label perform compared to this approach? That is, how would an approach with a fixed action policy (but with everything else) perform compared to the full deep reinforcement learning version?
This paper addresses a question that is often overlooked in reinforcement learning or locomotion experiment. My biggest point of critique is that it's difficult to draw conclusions or reason beyond the results of the experiments. The authors only consider a single neural network architecture and a single reward function. For example, is the torque controller limited by the policy network? My suggestion is to vary the number of neurons or show that the same results hold for a different state representation (e.g. trained on pixel data). In the paper's current form, the term "DeepRL" seems arbitrary. On the positive side, the paper is well-structured and easy to read. The experiments are sound, clear and easy to interpret. It's definitely an interesting line of work and beyond the extension to 3D, I would argue that considering more realistic physical constraints (e.g. actuator constraints, communication delays etc. on real robots) could greatly improve the impact of this work.
After reading the paper and the reviews, I believe that the paper presents a solid contribution and a detailed empirical exploration of the choice of action space representations for continuous control trajectory tracking tasks, but has limited relevance to the ICLR audience in its present form. The conclusion that PD controllers with learned gains provide improved learning speed and sometimes better final results than the "default" joint torque representation is intriguing and has some implications for future work on trajectory tracking for simulated articulated rigid body characters, but it's unclear from the current results what conclusion there is to take away for general algorithm or model design. Since the only evaluation is on trajectory tracking, it's also not even clear (as pointed out by other reviewers) to what degree these results will generalize to other tasks. In fact, a contrarian view might be that PD tracking is specifically a good fit for trajectory tracking with a known reference trajectory, but might be a poor fit for accomplishing a particular task, where more open-loop behaviors might be more optimal. The passive compliance of MTUs is also shown to often not be beneficial, but it's again not clear whether this might in fact be an artifact of the trajectory tracking task. But perhaps more problematically, the primary conclusions of the paper are of limited relevance when it comes to design of algorithms or models (or at least, the authors don't discuss this), but are more relevant for practitioners interested in applying deep RL for learning controllers for articulated rigid body systems such as robots or virtual characters. As such, it's not clear what fraction of the ICLR audience will find the paper relevant to their interests. I would strongly encourage the authors to continue their research: there is a lot to like about the present paper, including an elegant reinforcement learning methodology, rigorous empirical results for the specific case of trajectory following, and some very nice videos and examples.
We would like to thank the reviewers again for their feedback and suggestions. We have revised the paper to better address the points raised in the reviews. To help track the changes, we have highlighted them in blue. On page 15 of the supplemental material, you will find a new set experiments comparing performance across different initializations of the network, different network architectures, and sensitivity to the amount of exploration noise applied during training. We hope these new additions will help to address some of the previously raised concerns. We look forward to receiving additional feedback from the reviewers. Thank you,
Thank you for the feedback and comments in the reviews + questions. Please find below a summary of our further reflections, which we will be incorporating into the paper. Additional experiments We are currently performing several additional experiments, which we will add to the next version of the paper: - Analysis of variance of performance for multiple runs of training (Anon Rev 2) - Sensitivity to exploration noise, i.e., covariance matrix, (Anon Rev 2) - Impact of network architecture (Anon Rev 1) Re: Relevance of paper to ICLR audience 1) While much is known about learning input (state) representations, much less is known about the extent to which output (action) representations matter for continuous control policies, and whether useful “output representations” can be learned. This paper doesn’t provide all the answers, but it shows that output representations do matter (for locomotion): they significantly impact the learning rate, robustness, overall performance, and policy query rate. 2) Many methods for continuous action spaces use physics-based simulations of locomotion as one of their key examples. However, the use of torques as the defacto unit of action is rather arbitrary; it ignores the notion that the passive properties of actuation mechanisms, e.g., muscles, can make a significant contribution to motion control, i.e., “intelligence by mechanics” (Blickhan et al., 2007).This can be thought of as a kind of partitioning of the computations between the control and the physical system. Our paper provides a first look at the impact of such “partitioning”. 3) A question that we would like to draw more attention to is: should musculo-tendon systems (as seen in nature) and other actuation details be considered as part of the control policy or part of the environment? I.e., can they be safely ignored because the control policy would “learn the useful aspects” anyhow? Re: generalization of results Here are our thoughts on this; we will update the paper to reflect these. 1) Admittedly, we do not provide a concrete answer for the generalization to arbitrary objective functions, and we will clearly state this in the paper. However physics-based locomotion is commonly used as an example in continuous-action RL work. While we do optimize for only one reward function (imitation), we do explore results for different characters, different motions, different query rates, and overall policy robustness. We believe that these are all critical dimensions for testing capabilities and generalization for locomotion control. The imitation objective can also be used as a motion prior and so it is likely to be useful more generally. It further allows us to produce more natural motions (subjectively speaking) than previously seen for similar work. We hope that the capability of learning more natural locomotion policies, even if this arrives in part via the objective function rather than a theoretical advancement, will be of broad interest to the ICLR community. 2) We believe that no single action parameterization will be the best for all problems. However, since objectives for motion control problems are often naturally expressed in terms of kinematic properties, higher-level actions such as target joint angles and velocities may be effective for a wide variety of motion control problems. We hope that our work will help open discussions around the choice of action parameterizations. Re: specificity of results to the use of a reference pose cost While the reward terms are mainly calculated according to joint positions and velocities, the real challenge for the control policy lies with: (a) learning to compensate for various state-dependent forces, such as gravity and ground-reaction forces, and (b) learning strategies such as foot-placement that are needed to maintain balance for all the locomotion gaits. The reference pose terms provides no information on how to achieve these “hidden” aspects of motion control that will ultimately determine the success of the locomotion policy. There is also only a weak correlation between the action space and the actual reference poses for any of the action spaces; the action space trajectories illustrated in Figure 11 are all quite different from the actual right hip angle trajectory (not shown, but it varies smoothly over time). Re: considering more realistic physical constraints (e.g. actuator constraints, communication delays etc. on real robots) could greatly improve the impact of this work. (Anon Rev 1) We enforce torque limits and joint limits, which are shared across all actuation models. We do not currently include activation time delays for any of the models.
The paper is straightforward, easy to read, and has clear results. Since all these parameterisations end up outputting torques, it seems like there shouldn't be much difference between them. There is a known function that convert from one representation to another (or at least to torques). Is it not possible that the only reason proportional control is a little better is that the tracking cost is a function of positions? Would we get the same result if there was no reference-pose cost, only a locomotion cost? Would we get the same result if the task was to spin a top? My guess is no. This work is interesting, but not likely to generalise to other scenarios, and in that sense is rather limited. The video is nice.
Paper studies deep reinforcement learning paradigm for controlling high dimensional characters. Experiments compare the effect different control parameterizations (torques, muscle-activations, PD control with target joint positions and target joint velocities) have on the performance of reinforcement learning and optimized control policies. Evaluated are different planer gate cycle trajectories. It is illustrated that more abstract parameterizations are in fact better and result in more robust and higher quality policies. > Significance & Originality: The explored parameterizations are relatively standard in humanoid control. The real novelty is systematic evaluation of the various parameterizations. I think this type of study is important and insightful. However, the findings are very specific to the problem and specific tested architecture. Its not clear that findings will transferable to other networks on other control problems/domains. As such for the ICLR community, this may have limited breadth and perhaps would have broader appeal in robotics / graphics community. > Clarity: The paper is well written and is pretty easy to understand for someone who has some background with constrained multi-body simulation and control. > Experiments: Experimental validation is lacking somewhat in my opinion. Given that this is a fundamentally experimental paper, I would have liked to see more analysis of sensitivity to various parameters and analysis of variance of performance when policy is optimized multiple times.
This paper addresses a question that is often overlooked in reinforcement learning or locomotion experiment. My biggest point of critique is that it's difficult to draw conclusions or reason beyond the results of the experiments. The authors only consider a single neural network architecture and a single reward function. For example, is the torque controller limited by the policy network? My suggestion is to vary the number of neurons or show that the same results hold for a different state representation (e.g. trained on pixel data). In the paper's current form, the term "DeepRL" seems arbitrary. On the positive side, the paper is well-structured and easy to read. The experiments are sound, clear and easy to interpret. It's definitely an interesting line of work and beyond the extension to 3D, I would argue that considering more realistic physical constraints (e.g. actuator constraints, communication delays etc. on real robots) could greatly improve the impact of this work.
This paper addresses a question that is often overlooked in reinforcement learning or locomotion experiment. My biggest point of critique is that it's difficult to draw conclusions or reason beyond the results of the experiments. The authors only consider a single neural network architecture and a single reward function. For example, is the torque controller limited by the policy network? My suggestion is to vary the number of neurons or show that the same results hold for a different state representation (e.g. trained on pixel data). In the paper's current form, the term "DeepRL" seems arbitrary. On the positive side, the paper is well-structured and easy to read. The experiments are sound, clear and easy to interpret. It's definitely an interesting line of work and beyond the extension to 3D, I would argue that considering more realistic physical constraints (e.g. actuator constraints, communication delays etc. on real robots) could greatly improve the impact of this work.
After reading the paper and the reviews, I believe that the paper presents a solid contribution and a detailed empirical exploration of the choice of action space representations for continuous control trajectory tracking tasks, but has limited relevance to the ICLR audience in its present form. The conclusion that PD controllers with learned gains provide improved learning speed and sometimes better final results than the "default" joint torque representation is intriguing and has some implications for future work on trajectory tracking for simulated articulated rigid body characters, but it's unclear from the current results what conclusion there is to take away for general algorithm or model design. Since the only evaluation is on trajectory tracking, it's also not even clear (as pointed out by other reviewers) to what degree these results will generalize to other tasks. In fact, a contrarian view might be that PD tracking is specifically a good fit for trajectory tracking with a known reference trajectory, but might be a poor fit for accomplishing a particular task, where more open-loop behaviors might be more optimal. The passive compliance of MTUs is also shown to often not be beneficial, but it's again not clear whether this might in fact be an artifact of the trajectory tracking task. But perhaps more problematically, the primary conclusions of the paper are of limited relevance when it comes to design of algorithms or models (or at least, the authors don't discuss this), but are more relevant for practitioners interested in applying deep RL for learning controllers for articulated rigid body systems such as robots or virtual characters. As such, it's not clear what fraction of the ICLR audience will find the paper relevant to their interests. I would strongly encourage the authors to continue their research: there is a lot to like about the present paper, including an elegant reinforcement learning methodology, rigorous empirical results for the specific case of trajectory following, and some very nice videos and examples.
We would like to thank the reviewers again for their feedback and suggestions. We have revised the paper to better address the points raised in the reviews. To help track the changes, we have highlighted them in blue. On page 15 of the supplemental material, you will find a new set experiments comparing performance across different initializations of the network, different network architectures, and sensitivity to the amount of exploration noise applied during training. We hope these new additions will help to address some of the previously raised concerns. We look forward to receiving additional feedback from the reviewers. Thank you,
Thank you for the feedback and comments in the reviews + questions. Please find below a summary of our further reflections, which we will be incorporating into the paper. Additional experiments We are currently performing several additional experiments, which we will add to the next version of the paper: - Analysis of variance of performance for multiple runs of training (Anon Rev 2) - Sensitivity to exploration noise, i.e., covariance matrix, (Anon Rev 2) - Impact of network architecture (Anon Rev 1) Re: Relevance of paper to ICLR audience 1) While much is known about learning input (state) representations, much less is known about the extent to which output (action) representations matter for continuous control policies, and whether useful “output representations” can be learned. This paper doesn’t provide all the answers, but it shows that output representations do matter (for locomotion): they significantly impact the learning rate, robustness, overall performance, and policy query rate. 2) Many methods for continuous action spaces use physics-based simulations of locomotion as one of their key examples. However, the use of torques as the defacto unit of action is rather arbitrary; it ignores the notion that the passive properties of actuation mechanisms, e.g., muscles, can make a significant contribution to motion control, i.e., “intelligence by mechanics” (Blickhan et al., 2007).This can be thought of as a kind of partitioning of the computations between the control and the physical system. Our paper provides a first look at the impact of such “partitioning”. 3) A question that we would like to draw more attention to is: should musculo-tendon systems (as seen in nature) and other actuation details be considered as part of the control policy or part of the environment? I.e., can they be safely ignored because the control policy would “learn the useful aspects” anyhow? Re: generalization of results Here are our thoughts on this; we will update the paper to reflect these. 1) Admittedly, we do not provide a concrete answer for the generalization to arbitrary objective functions, and we will clearly state this in the paper. However physics-based locomotion is commonly used as an example in continuous-action RL work. While we do optimize for only one reward function (imitation), we do explore results for different characters, different motions, different query rates, and overall policy robustness. We believe that these are all critical dimensions for testing capabilities and generalization for locomotion control. The imitation objective can also be used as a motion prior and so it is likely to be useful more generally. It further allows us to produce more natural motions (subjectively speaking) than previously seen for similar work. We hope that the capability of learning more natural locomotion policies, even if this arrives in part via the objective function rather than a theoretical advancement, will be of broad interest to the ICLR community. 2) We believe that no single action parameterization will be the best for all problems. However, since objectives for motion control problems are often naturally expressed in terms of kinematic properties, higher-level actions such as target joint angles and velocities may be effective for a wide variety of motion control problems. We hope that our work will help open discussions around the choice of action parameterizations. Re: specificity of results to the use of a reference pose cost While the reward terms are mainly calculated according to joint positions and velocities, the real challenge for the control policy lies with: (a) learning to compensate for various state-dependent forces, such as gravity and ground-reaction forces, and (b) learning strategies such as foot-placement that are needed to maintain balance for all the locomotion gaits. The reference pose terms provides no information on how to achieve these “hidden” aspects of motion control that will ultimately determine the success of the locomotion policy. There is also only a weak correlation between the action space and the actual reference poses for any of the action spaces; the action space trajectories illustrated in Figure 11 are all quite different from the actual right hip angle trajectory (not shown, but it varies smoothly over time). Re: considering more realistic physical constraints (e.g. actuator constraints, communication delays etc. on real robots) could greatly improve the impact of this work. (Anon Rev 1) We enforce torque limits and joint limits, which are shared across all actuation models. We do not currently include activation time delays for any of the models.
The paper is straightforward, easy to read, and has clear results. Since all these parameterisations end up outputting torques, it seems like there shouldn't be much difference between them. There is a known function that convert from one representation to another (or at least to torques). Is it not possible that the only reason proportional control is a little better is that the tracking cost is a function of positions? Would we get the same result if there was no reference-pose cost, only a locomotion cost? Would we get the same result if the task was to spin a top? My guess is no. This work is interesting, but not likely to generalise to other scenarios, and in that sense is rather limited. The video is nice.
Paper studies deep reinforcement learning paradigm for controlling high dimensional characters. Experiments compare the effect different control parameterizations (torques, muscle-activations, PD control with target joint positions and target joint velocities) have on the performance of reinforcement learning and optimized control policies. Evaluated are different planer gate cycle trajectories. It is illustrated that more abstract parameterizations are in fact better and result in more robust and higher quality policies. > Significance & Originality: The explored parameterizations are relatively standard in humanoid control. The real novelty is systematic evaluation of the various parameterizations. I think this type of study is important and insightful. However, the findings are very specific to the problem and specific tested architecture. Its not clear that findings will transferable to other networks on other control problems/domains. As such for the ICLR community, this may have limited breadth and perhaps would have broader appeal in robotics / graphics community. > Clarity: The paper is well written and is pretty easy to understand for someone who has some background with constrained multi-body simulation and control. > Experiments: Experimental validation is lacking somewhat in my opinion. Given that this is a fundamentally experimental paper, I would have liked to see more analysis of sensitivity to various parameters and analysis of variance of performance when policy is optimized multiple times.
This paper addresses a question that is often overlooked in reinforcement learning or locomotion experiment. My biggest point of critique is that it's difficult to draw conclusions or reason beyond the results of the experiments. The authors only consider a single neural network architecture and a single reward function. For example, is the torque controller limited by the policy network? My suggestion is to vary the number of neurons or show that the same results hold for a different state representation (e.g. trained on pixel data). In the paper's current form, the term "DeepRL" seems arbitrary. On the positive side, the paper is well-structured and easy to read. The experiments are sound, clear and easy to interpret. It's definitely an interesting line of work and beyond the extension to 3D, I would argue that considering more realistic physical constraints (e.g. actuator constraints, communication delays etc. on real robots) could greatly improve the impact of this work.
This paper proposed COCONET, which is a neural autoregressive model with convolution, to do music composition task. This paper also proposed to use blocked Gibbs sampling instead of the ancestral sampling of the original NADE model to generate better pieces of music. The experimental results showed that the NLL of COCONET is better than the other baselines and the human evaluation task by Amazon’s Mechanical Turk illustrated that the model can generate compelling music. In general, I think the paper is good. Using NADE based model with convolution operations on music generation tasks and using blocked Gibbs sampling contains some kind of novelty. However, the novelty of the paper is incremental, since the blocked Gibbs sampling for NADE model is already proposed by Yao et al., (2014) and the using NADE based model for music modeling has also been proposed by Boulanger-Lewandowski et al., (2012).
This paper applies an existing idea (Yao's block Gibbs sampling of NADE) to a music model. There is also prior art for applying NADE to music. The main novel and interesting result is that block Gibbs sampling (an approximation) actually improves performance, highlighting problems with NADE. This work is borderline for inclusion. The paper is mainly an application of existing ideas. The implications of the interesting results could perhaps have been explored further for a paper at a meeting on learning representations.
Thank you all for your reviews! We share your concern regarding quantitative comparison to other works, and regarding the generality of our method. We have looked into the datasets used in prior works, but found that their preprocessing severely degraded the musical structure. For example, the temporal granularity used in Boulanger-Lewandowski et al. is too coarse: in several pieces, discarding all notes that do not have their onsets on the eighth-note grid results in sparse notes with long stretches of silence between them. The pieces become unrecognizable, and (worse) non-musical. Downsampling or blurring does not work in symbolic music like it does in images: music is more similar to language, and using a coarse grid is analogous to removing words from a sentence and asking a model to learn this new distribution of broken sentences. This turns it into a different task, and makes qualitative judgement of samples not very meaningful. We also considered applying our method to image data as suggested. Unlike (symbolic) music which is discrete and intricately structured, the domain of images is smooth and forgiving of small errors. It is plausible that the NADE sampling approach generates fine images (indeed, that's what Yao et al. found), for reasons that don't carry over to our domain of interest, which is music. The development of a larger-scale, more diverse and higher-quality MIDI music dataset is a major component of our ongoing project. We nonetheless believe that the advances we show are of sufficient interest to justify publication at this time.
The paper tackles the task of music generation. They use an orderless NADE model for the task of "fill in the notes". Given a roll of T timesteps of pitches, they randomly mask out some pitches, and the model is trained to predict the missing notes. This follows how the orderless NADE model can be trained. During sampling, one normally follows an ancestral sampling procedure. For this, an ordering is defined over outputs, and one runs the model on the current input, samples one of the outputs according to the order, adds this output to the next input, and continues this procedure until all outputs have been sampled. The key point of the paper is that this is a bad sampling strategy. Instead, they suggest the strategy of Yao et al. 2014, which uses a blocked Gibbs sampling approach. The blocked Gibbs strategy instead masks N inputs randomly and independently, samples them, and repeats this procedure. The point of this strategy is the make sure the sampling chain mixes well, which will happen for large N. However, since the samples are independent, having a large N gives incoherent samples. Thus, the authors follow an annealed schedule for N, making it smaller over time, which will eventually reduce to ancestral sampling (giving global structure to the sample). They conduct a variety of experiments involving both normal metrics and human evaluations, and find that this blocked Gibbs sampling outperforms other sampling procedures. This is a well written paper - great job. My main problem with the paper is that having read Uria and Yao, I don't know how much I have learned from this work in the context of this being an ICLR submission. If this was submitted to some computational music / art conference, this paper would be a clear accept. However, for ICLR, I don't see enough novelty compared with previous works this builds upon. Orderless NADE is an established model. The blocked Gibbs sampling and annealing scheme are basically the exact same one used in Yao. Thus, the main novelty of this paper is its application to the music domain, and finding that Yao's method works better for sampling music. This is a good contribution, but more tailored to those working in the music domain. If the authors found that these results also hold for other domains like images (e.g. on CIFAR / tiny Imagenet) and text (e.g. document generation), then I would change my mind and accept this paper for ICLR. Even just trying musical domains other than Bach chorales would be useful. However, as it stands, the experiments are not convincing enough.
The paper presents a way to model the distribution of four-part Bach chorales using Convolutional Neural Networks. Furthermore it addresses the task of artificial music generation by sampling from the model using blocked Gibbs sampling and shows The CNN model for the distribution seems very appropriate for the data at hand. Also the analysis of the proposed sampling schemes with the analogy between Gibbs sampling and human music composition are very interesting. I am not too sure about the evaluation though. Since the reported likelihoods are not directly comparable to previous work, I have difficulties judging the quality of the quantitative results. For the human evaluation I would like to see the data for the direct comparisons between the models. E.g. How did NADE vs. Bach perform. Also I find the question: ‘what piece of music do you prefer’ a stronger test than the question ‘what piece is more musical to you’ because I don’t really know what ‘musical’ means to the AMT workers. Finally, while I think the Bach Chorales are interesting musical pieces that deserve to be subject of the analysis but I find it hard to judge how well this modelling approach will transfer to other types of music which might have a very different data distribution. Nevertheless, in conclusion, I believe this is an exciting model for an interesting task that produces non-trivial musical data.
This paper proposed COCONET, which is a neural autoregressive model with convolution, to do music composition task. This paper also proposed to use blocked Gibbs sampling instead of the ancestral sampling of the original NADE model to generate better pieces of music. The experimental results showed that the NLL of COCONET is better than the other baselines and the human evaluation task by Amazon’s Mechanical Turk illustrated that the model can generate compelling music. In general, I think the paper is good. Using NADE based model with convolution operations on music generation tasks and using blocked Gibbs sampling contains some kind of novelty. However, the novelty of the paper is incremental, since the blocked Gibbs sampling for NADE model is already proposed by Yao et al., (2014) and the using NADE based model for music modeling has also been proposed by Boulanger-Lewandowski et al., (2012).
This paper proposed COCONET, which is a neural autoregressive model with convolution, to do music composition task. This paper also proposed to use blocked Gibbs sampling instead of the ancestral sampling of the original NADE model to generate better pieces of music. The experimental results showed that the NLL of COCONET is better than the other baselines and the human evaluation task by Amazon’s Mechanical Turk illustrated that the model can generate compelling music. In general, I think the paper is good. Using NADE based model with convolution operations on music generation tasks and using blocked Gibbs sampling contains some kind of novelty. However, the novelty of the paper is incremental, since the blocked Gibbs sampling for NADE model is already proposed by Yao et al., (2014) and the using NADE based model for music modeling has also been proposed by Boulanger-Lewandowski et al., (2012).
This paper applies an existing idea (Yao's block Gibbs sampling of NADE) to a music model. There is also prior art for applying NADE to music. The main novel and interesting result is that block Gibbs sampling (an approximation) actually improves performance, highlighting problems with NADE. This work is borderline for inclusion. The paper is mainly an application of existing ideas. The implications of the interesting results could perhaps have been explored further for a paper at a meeting on learning representations.
Thank you all for your reviews! We share your concern regarding quantitative comparison to other works, and regarding the generality of our method. We have looked into the datasets used in prior works, but found that their preprocessing severely degraded the musical structure. For example, the temporal granularity used in Boulanger-Lewandowski et al. is too coarse: in several pieces, discarding all notes that do not have their onsets on the eighth-note grid results in sparse notes with long stretches of silence between them. The pieces become unrecognizable, and (worse) non-musical. Downsampling or blurring does not work in symbolic music like it does in images: music is more similar to language, and using a coarse grid is analogous to removing words from a sentence and asking a model to learn this new distribution of broken sentences. This turns it into a different task, and makes qualitative judgement of samples not very meaningful. We also considered applying our method to image data as suggested. Unlike (symbolic) music which is discrete and intricately structured, the domain of images is smooth and forgiving of small errors. It is plausible that the NADE sampling approach generates fine images (indeed, that's what Yao et al. found), for reasons that don't carry over to our domain of interest, which is music. The development of a larger-scale, more diverse and higher-quality MIDI music dataset is a major component of our ongoing project. We nonetheless believe that the advances we show are of sufficient interest to justify publication at this time.
The paper tackles the task of music generation. They use an orderless NADE model for the task of "fill in the notes". Given a roll of T timesteps of pitches, they randomly mask out some pitches, and the model is trained to predict the missing notes. This follows how the orderless NADE model can be trained. During sampling, one normally follows an ancestral sampling procedure. For this, an ordering is defined over outputs, and one runs the model on the current input, samples one of the outputs according to the order, adds this output to the next input, and continues this procedure until all outputs have been sampled. The key point of the paper is that this is a bad sampling strategy. Instead, they suggest the strategy of Yao et al. 2014, which uses a blocked Gibbs sampling approach. The blocked Gibbs strategy instead masks N inputs randomly and independently, samples them, and repeats this procedure. The point of this strategy is the make sure the sampling chain mixes well, which will happen for large N. However, since the samples are independent, having a large N gives incoherent samples. Thus, the authors follow an annealed schedule for N, making it smaller over time, which will eventually reduce to ancestral sampling (giving global structure to the sample). They conduct a variety of experiments involving both normal metrics and human evaluations, and find that this blocked Gibbs sampling outperforms other sampling procedures. This is a well written paper - great job. My main problem with the paper is that having read Uria and Yao, I don't know how much I have learned from this work in the context of this being an ICLR submission. If this was submitted to some computational music / art conference, this paper would be a clear accept. However, for ICLR, I don't see enough novelty compared with previous works this builds upon. Orderless NADE is an established model. The blocked Gibbs sampling and annealing scheme are basically the exact same one used in Yao. Thus, the main novelty of this paper is its application to the music domain, and finding that Yao's method works better for sampling music. This is a good contribution, but more tailored to those working in the music domain. If the authors found that these results also hold for other domains like images (e.g. on CIFAR / tiny Imagenet) and text (e.g. document generation), then I would change my mind and accept this paper for ICLR. Even just trying musical domains other than Bach chorales would be useful. However, as it stands, the experiments are not convincing enough.
The paper presents a way to model the distribution of four-part Bach chorales using Convolutional Neural Networks. Furthermore it addresses the task of artificial music generation by sampling from the model using blocked Gibbs sampling and shows The CNN model for the distribution seems very appropriate for the data at hand. Also the analysis of the proposed sampling schemes with the analogy between Gibbs sampling and human music composition are very interesting. I am not too sure about the evaluation though. Since the reported likelihoods are not directly comparable to previous work, I have difficulties judging the quality of the quantitative results. For the human evaluation I would like to see the data for the direct comparisons between the models. E.g. How did NADE vs. Bach perform. Also I find the question: ‘what piece of music do you prefer’ a stronger test than the question ‘what piece is more musical to you’ because I don’t really know what ‘musical’ means to the AMT workers. Finally, while I think the Bach Chorales are interesting musical pieces that deserve to be subject of the analysis but I find it hard to judge how well this modelling approach will transfer to other types of music which might have a very different data distribution. Nevertheless, in conclusion, I believe this is an exciting model for an interesting task that produces non-trivial musical data.
This paper proposed COCONET, which is a neural autoregressive model with convolution, to do music composition task. This paper also proposed to use blocked Gibbs sampling instead of the ancestral sampling of the original NADE model to generate better pieces of music. The experimental results showed that the NLL of COCONET is better than the other baselines and the human evaluation task by Amazon’s Mechanical Turk illustrated that the model can generate compelling music. In general, I think the paper is good. Using NADE based model with convolution operations on music generation tasks and using blocked Gibbs sampling contains some kind of novelty. However, the novelty of the paper is incremental, since the blocked Gibbs sampling for NADE model is already proposed by Yao et al., (2014) and the using NADE based model for music modeling has also been proposed by Boulanger-Lewandowski et al., (2012).
This submission proposes a letter-level decoder with a variation of the CTC approach they call ASG, where the blank symbol is dropped and replaced by letter repetition symbols, and where explicit normalization is dropped. Both the description of a letter-level model (though not novel), as well as the CTC-variant are interesting. The approach is evaluated on the LibriSpeech task. The authors claim that their approach is competitive. They compare their modelling variant ASG to CTC, but a comparison of the letter-level approach to available word-level results are missing. Compared to the results obtained in Panayotov et al. 2015, the performance obtained here seems only comparable to word-level GMM/HMM models, but worse than word-level hybrid DNN/HMM models, though Panayotov et al. also appled speaker adaptation, which was not done, as far as I can see. I suggest to add a comparison to Panyotov's results (in addition to mentioning Baidu's results on Librispeech, which are not comparable due to much larger amounts of training data), to allow readers to get a quantitative idea. As pointed out by the authors in the text, Baidu's GPU implementation for CTC is more aimed at larger vocabularies, therefore the comparison to GPU in Tables 1a-c do not seem to be helpful for this work, without further discussing the implementations. You are using quite a huge analysis window (nearly 2s). Even though other authors also use windows up to 0.5s to 1s (e.g. MRASTA features), some comments on how you arrive at such a large window, and what advantages you observe for it, would be interesting. The submission is well written, though more details on the experiences with using non-normalized (transition) scores and beam pruning would be desirable. Table 1 would be better readable if the units of the numbers shown in a/b/c would be shown within the tables, and not only in the caption. Prior (partial) publications of this work (your NIPS end-to-end workshop paper) should clearly be mentioned/referenced. What do you mean by transition "scalars"? I do not repeat further comments here, which were already given in the pre-review period. Minor comments: - Sec. 2.3, end of 2nd sentence: train properly the model -> train the model properly End of same paragraph: boostrap -> bootstrap (such errors should be avoided by performing an automatic spell check) - Sec. 2.3: Bayse -> Bayes - definition of logadd is wrong (see comment) - (applies also for your NIPS end-to-end workshop paper). - line before Eq. (3): all possible sequence of letters -> all possible sequences of letters (plural) - Sec. 2.4, first line: threholding -> thresholding (spell check..) - Figure 4: mention the corpus used here - dev?
Without revisions to this paper or a rebuttal from the authors, it is hard to accept this paper. The main contribution of the paper is removing the blank from CTC to create a somewhat different criterion, but this is not particularly novel (see, for example,
This paper describes an end-to-end system for speech recognition that uses a linear conditional random field framework. A convnet estimates node potentials, while transition scores are provided by trained scalar values. The convnet acoustic model computes scores for letters, not phones, which reduces the need for expert knowledge in training the system. At test time, scores from a word-level language model, the convnet node potentials, learned letter-to-letter transition scores, and a word insertion penalty are combined to find the best-scoring word hypothesis. The model may be trained from the raw audio waveform, power spectra, or MFCC features using conditional maximum likelihood estimation. Experiments on the Librispeech corpus show that the model achieves a 7.2% WER on the test-clean set from Librispeech using MFCC features, a 9.4% WER using power spectral features, and a 10.1% WER using the raw waveform. Pros + It is interesting to see that a convnet trained from scratch using conditional maximum likelihood can perform reasonably well in a speech recognition system for English that uses graphemic (letter-based) acoustic models instead of phonetic models. This is a promising research direction. Cons - The paper is missing a lot of context / prior work that deserves to be cited. In addition to the papers I already mentioned in various comments, the authors should also be aware of another 2016 Interspeech paper: Zhang et al., "Towards End-to-End Speech Recognition with Deep Convolutional Neural Networks",
There have been numerous works on learning from raw waveforms and training letter-based CTC networks for speech recognition, however, there are very few works on combining both of them with purely ConvNet as it is done in this paper. It is interesting to see results on a large scale corpus such as Librispeech that is used in this paper, though some baseline results from hybrid NN/HMM systems should be provided. To readers, it is unclear how this system is close to state-of-the-art only from Table 2. The key contribution of this paper may be the end-to-end sequence training criterion for their CTC variant (where the blank symbol is dropped), which may be viewed as sequence training of CTC as H. Sak, et al. "Learning acoustic frame labeling for speech recognition with recurrent neural networks", 2015. However, instead of generating the denominator lattices using a frame-level trained CTC model first, this paper directly compute the sequence-level loss by considering all the competing hypothesis in the normalizer. Therefore, the model is trained end-to-end. From this perspective, it is closely related to D. Povey's LF-MMI for sequence-training of HMMs. As another reviewer has pointed out, references and discussions on that should be provided. This approach should be more expensive than frame-level training of CTCs, however, from Table 1, the authors' implementation is much faster. Did the systems there use the same sampling rate? You said at the end of 2.2 that the step size for your model is 20ms. Is it also the same for Baidu's CTC system. Also, have you tried increasing the step size, e.g. to 30ms or 40ms, as people have found that it may work (equally) better, while significantly cut down the computational cost.
Dear authors, Here are some missing relevant citations. You should definitely cite the original paper that used CTC with characters. Graves et al., "Towards End-to-End Speech Recognition with Recurrent Neural Networks", in ICML 2014. You should probably also cite and have a related work section with attention-based models such as: Chan et al., "Listen, Attend and Spell: A Neural Network for Large Vocabulary Conversational Speech Recognition", in ICASSP 2016. Bahdanau et al., "End-to-End Attention-based Large Vocabulary Speech Recognition", in ICASSP 2016. both of which are highly relevant to end-to-end ASR. Question: Why did you use Librispeech as opposed to WSJ and/or SWBD. Most end-to-end ASR papers publish on WSJ, especially since there is an established benchmark for comparison (i.e., Graves et al., 2014, Bahdanau et al., 2016, Chan et al., 2016). SWBD also has much stronger benchmarks from the general speech community, and even for end-to-end ASR (i.e., see MSR's CTC paper by Zweig et., "Advances in All-Neural Speech Recognition", 2016). You should also definitely comment and compare to Zweig's paper, since they used a similar encoding mechanism. Question: Is "Letter Error Rate" (LER) the common terminology? From Alex Graves papers and others I see "Character Error Rate" (CER). What is the difference? Question: Very cool that you can combine wav+cnns+ctc->ASR, but still a little bit disappointed that handcrafted features perform better. Do you expect this to change with more data?
This submission proposes a letter-level decoder with a variation of the CTC approach they call ASG, where the blank symbol is dropped and replaced by letter repetition symbols, and where explicit normalization is dropped. Both the description of a letter-level model (though not novel), as well as the CTC-variant are interesting. The approach is evaluated on the LibriSpeech task. The authors claim that their approach is competitive. They compare their modelling variant ASG to CTC, but a comparison of the letter-level approach to available word-level results are missing. Compared to the results obtained in Panayotov et al. 2015, the performance obtained here seems only comparable to word-level GMM/HMM models, but worse than word-level hybrid DNN/HMM models, though Panayotov et al. also appled speaker adaptation, which was not done, as far as I can see. I suggest to add a comparison to Panyotov's results (in addition to mentioning Baidu's results on Librispeech, which are not comparable due to much larger amounts of training data), to allow readers to get a quantitative idea. As pointed out by the authors in the text, Baidu's GPU implementation for CTC is more aimed at larger vocabularies, therefore the comparison to GPU in Tables 1a-c do not seem to be helpful for this work, without further discussing the implementations. You are using quite a huge analysis window (nearly 2s). Even though other authors also use windows up to 0.5s to 1s (e.g. MRASTA features), some comments on how you arrive at such a large window, and what advantages you observe for it, would be interesting. The submission is well written, though more details on the experiences with using non-normalized (transition) scores and beam pruning would be desirable. Table 1 would be better readable if the units of the numbers shown in a/b/c would be shown within the tables, and not only in the caption. Prior (partial) publications of this work (your NIPS end-to-end workshop paper) should clearly be mentioned/referenced. What do you mean by transition "scalars"? I do not repeat further comments here, which were already given in the pre-review period. Minor comments: - Sec. 2.3, end of 2nd sentence: train properly the model -> train the model properly End of same paragraph: boostrap -> bootstrap (such errors should be avoided by performing an automatic spell check) - Sec. 2.3: Bayse -> Bayes - definition of logadd is wrong (see comment) - (applies also for your NIPS end-to-end workshop paper). - line before Eq. (3): all possible sequence of letters -> all possible sequences of letters (plural) - Sec. 2.4, first line: threholding -> thresholding (spell check..) - Figure 4: mention the corpus used here - dev?
A slightly more compressed version of this submission will be presented at the NIPS end-to-end workshop on Dec. 10, 2016. The NIPS submission seems to be a clear subset of this submission and should at least be mentioned in this paper.
When dropping the normalization of acoustic model scores, the range of scores obtained might vary and would have an effect on beam pruning and on its relation to the normalized LM scores. Did you analyse this?
Sec. 2.3: you use digits to label character repetitions. How do you handle numbers?
It seems that you use inconsistent notation - the variable 't' is used for different time scales: in Eq. (1) t represents strided time frames, whereas in x_t above it enumerates frames directly.
This submission proposes a letter-level decoder with a variation of the CTC approach they call ASG, where the blank symbol is dropped and replaced by letter repetition symbols, and where explicit normalization is dropped. Both the description of a letter-level model (though not novel), as well as the CTC-variant are interesting. The approach is evaluated on the LibriSpeech task. The authors claim that their approach is competitive. They compare their modelling variant ASG to CTC, but a comparison of the letter-level approach to available word-level results are missing. Compared to the results obtained in Panayotov et al. 2015, the performance obtained here seems only comparable to word-level GMM/HMM models, but worse than word-level hybrid DNN/HMM models, though Panayotov et al. also appled speaker adaptation, which was not done, as far as I can see. I suggest to add a comparison to Panyotov's results (in addition to mentioning Baidu's results on Librispeech, which are not comparable due to much larger amounts of training data), to allow readers to get a quantitative idea. As pointed out by the authors in the text, Baidu's GPU implementation for CTC is more aimed at larger vocabularies, therefore the comparison to GPU in Tables 1a-c do not seem to be helpful for this work, without further discussing the implementations. You are using quite a huge analysis window (nearly 2s). Even though other authors also use windows up to 0.5s to 1s (e.g. MRASTA features), some comments on how you arrive at such a large window, and what advantages you observe for it, would be interesting. The submission is well written, though more details on the experiences with using non-normalized (transition) scores and beam pruning would be desirable. Table 1 would be better readable if the units of the numbers shown in a/b/c would be shown within the tables, and not only in the caption. Prior (partial) publications of this work (your NIPS end-to-end workshop paper) should clearly be mentioned/referenced. What do you mean by transition "scalars"? I do not repeat further comments here, which were already given in the pre-review period. Minor comments: - Sec. 2.3, end of 2nd sentence: train properly the model -> train the model properly End of same paragraph: boostrap -> bootstrap (such errors should be avoided by performing an automatic spell check) - Sec. 2.3: Bayse -> Bayes - definition of logadd is wrong (see comment) - (applies also for your NIPS end-to-end workshop paper). - line before Eq. (3): all possible sequence of letters -> all possible sequences of letters (plural) - Sec. 2.4, first line: threholding -> thresholding (spell check..) - Figure 4: mention the corpus used here - dev?
Without revisions to this paper or a rebuttal from the authors, it is hard to accept this paper. The main contribution of the paper is removing the blank from CTC to create a somewhat different criterion, but this is not particularly novel (see, for example,
This paper describes an end-to-end system for speech recognition that uses a linear conditional random field framework. A convnet estimates node potentials, while transition scores are provided by trained scalar values. The convnet acoustic model computes scores for letters, not phones, which reduces the need for expert knowledge in training the system. At test time, scores from a word-level language model, the convnet node potentials, learned letter-to-letter transition scores, and a word insertion penalty are combined to find the best-scoring word hypothesis. The model may be trained from the raw audio waveform, power spectra, or MFCC features using conditional maximum likelihood estimation. Experiments on the Librispeech corpus show that the model achieves a 7.2% WER on the test-clean set from Librispeech using MFCC features, a 9.4% WER using power spectral features, and a 10.1% WER using the raw waveform. Pros + It is interesting to see that a convnet trained from scratch using conditional maximum likelihood can perform reasonably well in a speech recognition system for English that uses graphemic (letter-based) acoustic models instead of phonetic models. This is a promising research direction. Cons - The paper is missing a lot of context / prior work that deserves to be cited. In addition to the papers I already mentioned in various comments, the authors should also be aware of another 2016 Interspeech paper: Zhang et al., "Towards End-to-End Speech Recognition with Deep Convolutional Neural Networks",
There have been numerous works on learning from raw waveforms and training letter-based CTC networks for speech recognition, however, there are very few works on combining both of them with purely ConvNet as it is done in this paper. It is interesting to see results on a large scale corpus such as Librispeech that is used in this paper, though some baseline results from hybrid NN/HMM systems should be provided. To readers, it is unclear how this system is close to state-of-the-art only from Table 2. The key contribution of this paper may be the end-to-end sequence training criterion for their CTC variant (where the blank symbol is dropped), which may be viewed as sequence training of CTC as H. Sak, et al. "Learning acoustic frame labeling for speech recognition with recurrent neural networks", 2015. However, instead of generating the denominator lattices using a frame-level trained CTC model first, this paper directly compute the sequence-level loss by considering all the competing hypothesis in the normalizer. Therefore, the model is trained end-to-end. From this perspective, it is closely related to D. Povey's LF-MMI for sequence-training of HMMs. As another reviewer has pointed out, references and discussions on that should be provided. This approach should be more expensive than frame-level training of CTCs, however, from Table 1, the authors' implementation is much faster. Did the systems there use the same sampling rate? You said at the end of 2.2 that the step size for your model is 20ms. Is it also the same for Baidu's CTC system. Also, have you tried increasing the step size, e.g. to 30ms or 40ms, as people have found that it may work (equally) better, while significantly cut down the computational cost.
Dear authors, Here are some missing relevant citations. You should definitely cite the original paper that used CTC with characters. Graves et al., "Towards End-to-End Speech Recognition with Recurrent Neural Networks", in ICML 2014. You should probably also cite and have a related work section with attention-based models such as: Chan et al., "Listen, Attend and Spell: A Neural Network for Large Vocabulary Conversational Speech Recognition", in ICASSP 2016. Bahdanau et al., "End-to-End Attention-based Large Vocabulary Speech Recognition", in ICASSP 2016. both of which are highly relevant to end-to-end ASR. Question: Why did you use Librispeech as opposed to WSJ and/or SWBD. Most end-to-end ASR papers publish on WSJ, especially since there is an established benchmark for comparison (i.e., Graves et al., 2014, Bahdanau et al., 2016, Chan et al., 2016). SWBD also has much stronger benchmarks from the general speech community, and even for end-to-end ASR (i.e., see MSR's CTC paper by Zweig et., "Advances in All-Neural Speech Recognition", 2016). You should also definitely comment and compare to Zweig's paper, since they used a similar encoding mechanism. Question: Is "Letter Error Rate" (LER) the common terminology? From Alex Graves papers and others I see "Character Error Rate" (CER). What is the difference? Question: Very cool that you can combine wav+cnns+ctc->ASR, but still a little bit disappointed that handcrafted features perform better. Do you expect this to change with more data?
This submission proposes a letter-level decoder with a variation of the CTC approach they call ASG, where the blank symbol is dropped and replaced by letter repetition symbols, and where explicit normalization is dropped. Both the description of a letter-level model (though not novel), as well as the CTC-variant are interesting. The approach is evaluated on the LibriSpeech task. The authors claim that their approach is competitive. They compare their modelling variant ASG to CTC, but a comparison of the letter-level approach to available word-level results are missing. Compared to the results obtained in Panayotov et al. 2015, the performance obtained here seems only comparable to word-level GMM/HMM models, but worse than word-level hybrid DNN/HMM models, though Panayotov et al. also appled speaker adaptation, which was not done, as far as I can see. I suggest to add a comparison to Panyotov's results (in addition to mentioning Baidu's results on Librispeech, which are not comparable due to much larger amounts of training data), to allow readers to get a quantitative idea. As pointed out by the authors in the text, Baidu's GPU implementation for CTC is more aimed at larger vocabularies, therefore the comparison to GPU in Tables 1a-c do not seem to be helpful for this work, without further discussing the implementations. You are using quite a huge analysis window (nearly 2s). Even though other authors also use windows up to 0.5s to 1s (e.g. MRASTA features), some comments on how you arrive at such a large window, and what advantages you observe for it, would be interesting. The submission is well written, though more details on the experiences with using non-normalized (transition) scores and beam pruning would be desirable. Table 1 would be better readable if the units of the numbers shown in a/b/c would be shown within the tables, and not only in the caption. Prior (partial) publications of this work (your NIPS end-to-end workshop paper) should clearly be mentioned/referenced. What do you mean by transition "scalars"? I do not repeat further comments here, which were already given in the pre-review period. Minor comments: - Sec. 2.3, end of 2nd sentence: train properly the model -> train the model properly End of same paragraph: boostrap -> bootstrap (such errors should be avoided by performing an automatic spell check) - Sec. 2.3: Bayse -> Bayes - definition of logadd is wrong (see comment) - (applies also for your NIPS end-to-end workshop paper). - line before Eq. (3): all possible sequence of letters -> all possible sequences of letters (plural) - Sec. 2.4, first line: threholding -> thresholding (spell check..) - Figure 4: mention the corpus used here - dev?
A slightly more compressed version of this submission will be presented at the NIPS end-to-end workshop on Dec. 10, 2016. The NIPS submission seems to be a clear subset of this submission and should at least be mentioned in this paper.
When dropping the normalization of acoustic model scores, the range of scores obtained might vary and would have an effect on beam pruning and on its relation to the normalized LM scores. Did you analyse this?
Sec. 2.3: you use digits to label character repetitions. How do you handle numbers?
It seems that you use inconsistent notation - the variable 't' is used for different time scales: in Eq. (1) t represents strided time frames, whereas in x_t above it enumerates frames directly.
This paper provides a number of performance enhancements inspired by domain knowledge. Taken together, these produce a compelling system that has shown itself to be the best-in-class as per the related competition. Experts agree that the authors do a good job at justifying the majority of the design decisions. pros: - insights into the SOTA Doom player cons: - lack of pure technical novelty: the various elements have existed previously This paper comes down to a matter of taste in terms of appreciation of SOTA systems or technical novelty. With the code being released, I believe that this work will have impact as a benchmark, and as a guidebook as to how features can be combined for SOTA performance on FPS-style scenarios.
We thank the reviewers for their insightful comments! All reviewers agree that this paper makes a solid contribution with good experimental results. It is not uncommon to see application-oriented papers using a combination of multiple techniques to achieve strong performance. This category covers many seminar works, e.g., deep reinforcement learning for Atari games (applying deep models to traditional Q-learning), or even AlphaGo (supervised learning, policy gradient, value function, Monte-Carlo Tree Search, self-play). It may be a bit shortsighted to judge such strong performing papers with a single criterion. Confusion about the domain: Reviewer3 mentions that the paper "basically applies A3C to 3D spatial navigation tasks.", which is not true. In the deathmatch game of Doom, multiple players explore the maze and fight against each other to get a higher score, which is defined as #kills - #suicide. In this task, part of the goal is to learn anti-enemy tactics (e.g., dodging the rocket shot from the enemy, e.g., video:
The paper describes approaches taken to train learning agents for the 3D game Doom. The authors propose a number of performance enhancements (curriculum learning, attention (zoomed-in centered) frames, reward shaping, game variables, post-training rules) inspired by domain knowledge. The enhancements together lead to a clear win as demonstrated by the competition results. From Fig 4, the curriculum learning clearly helps with learning over increasingly difficult settings. A nice result is that there is no overfitting to the harder classes once they have learned (probably because the curriculum is health and speed). The authors conclude from Fig 5 that the adaptive curriculum is better and more stable that pure A3C; however, this is a bit of a stretch given that graph. They go on to say that Pure A3C doesn't learn at all in the harder map but then show no result/graph to back this claim. Tbl 5 shows a clear benefit of the post-training rules. If the goal is to solve problems like these (3D shooters), then this paper makes a significant contribution in that it shows which techniques are practical for solving the problem and ultimately improving performance in these kinds of tasks. Still, I am just not excited about this paper, mainly because it relies so heavily of many sources of domain knowledge, it is quite far from the pure reinforcement learning problem. The results are relatively unsurprising. Maybe they are novel for this problem, though. I'm not sure we can realistically draw any conclusions about Figure 6 in the paper's current form. I recommend the authors increase the resolution or run some actual metrics to determine the fuzziness/clarity of each row/image: something more concrete than an arrow of already low-resolution images. --- Added after rebuttal: I still do not see any high-res images for Figure 6 or any link to them, but I trust that the authors will add them if accepted.
This paper basically applies A3C to 3D spatial navigation tasks. - This is not the first time A3C has been applied to 3D navigation. In fact the original paper reported these experiments. Although the experimental results are great, I am not sure if this paper has any additional insights to warrant itself as a conference paper. It might make more sense as a workshop paper - Are the graphs in Fig 5 constructed using a single hyper-parameter sweep? I think the authors should report results with many random initializations to make the comparisons more robust - Overall the two main ideas in this paper -- A3C and curriculums -- are not really novel but the authors do make use of them in a real system.
This is a solid paper that applies A3C to Doom, enhancing it with a collection of tricks so as to win one of the VizDoom competitions. I think it is fair to expect the competition aspect to overshadow the more scientific approach of justifying every design decision in isolation, but in fact the authors do a decent job at the latter. Two of my concerns have remained unanswered (see AnonReviewer2, below). In addition, the citation list is rather thin, for example reward shaping has a rich literature, as do incrementally more difficult task setups, dating back at least to Mark Ring’s work in the 1990s. There has also been a lot of complementary work on other FPS games. I’m not asking that the authors do any direct comparisons, but to give the reader a sense of context in which to place this.
This paper provides a number of performance enhancements inspired by domain knowledge. Taken together, these produce a compelling system that has shown itself to be the best-in-class as per the related competition. Experts agree that the authors do a good job at justifying the majority of the design decisions. pros: - insights into the SOTA Doom player cons: - lack of pure technical novelty: the various elements have existed previously This paper comes down to a matter of taste in terms of appreciation of SOTA systems or technical novelty. With the code being released, I believe that this work will have impact as a benchmark, and as a guidebook as to how features can be combined for SOTA performance on FPS-style scenarios.
We thank the reviewers for their insightful comments! All reviewers agree that this paper makes a solid contribution with good experimental results. It is not uncommon to see application-oriented papers using a combination of multiple techniques to achieve strong performance. This category covers many seminar works, e.g., deep reinforcement learning for Atari games (applying deep models to traditional Q-learning), or even AlphaGo (supervised learning, policy gradient, value function, Monte-Carlo Tree Search, self-play). It may be a bit shortsighted to judge such strong performing papers with a single criterion. Confusion about the domain: Reviewer3 mentions that the paper "basically applies A3C to 3D spatial navigation tasks.", which is not true. In the deathmatch game of Doom, multiple players explore the maze and fight against each other to get a higher score, which is defined as #kills - #suicide. In this task, part of the goal is to learn anti-enemy tactics (e.g., dodging the rocket shot from the enemy, e.g., video:
The paper describes approaches taken to train learning agents for the 3D game Doom. The authors propose a number of performance enhancements (curriculum learning, attention (zoomed-in centered) frames, reward shaping, game variables, post-training rules) inspired by domain knowledge. The enhancements together lead to a clear win as demonstrated by the competition results. From Fig 4, the curriculum learning clearly helps with learning over increasingly difficult settings. A nice result is that there is no overfitting to the harder classes once they have learned (probably because the curriculum is health and speed). The authors conclude from Fig 5 that the adaptive curriculum is better and more stable that pure A3C; however, this is a bit of a stretch given that graph. They go on to say that Pure A3C doesn't learn at all in the harder map but then show no result/graph to back this claim. Tbl 5 shows a clear benefit of the post-training rules. If the goal is to solve problems like these (3D shooters), then this paper makes a significant contribution in that it shows which techniques are practical for solving the problem and ultimately improving performance in these kinds of tasks. Still, I am just not excited about this paper, mainly because it relies so heavily of many sources of domain knowledge, it is quite far from the pure reinforcement learning problem. The results are relatively unsurprising. Maybe they are novel for this problem, though. I'm not sure we can realistically draw any conclusions about Figure 6 in the paper's current form. I recommend the authors increase the resolution or run some actual metrics to determine the fuzziness/clarity of each row/image: something more concrete than an arrow of already low-resolution images. --- Added after rebuttal: I still do not see any high-res images for Figure 6 or any link to them, but I trust that the authors will add them if accepted.
This paper basically applies A3C to 3D spatial navigation tasks. - This is not the first time A3C has been applied to 3D navigation. In fact the original paper reported these experiments. Although the experimental results are great, I am not sure if this paper has any additional insights to warrant itself as a conference paper. It might make more sense as a workshop paper - Are the graphs in Fig 5 constructed using a single hyper-parameter sweep? I think the authors should report results with many random initializations to make the comparisons more robust - Overall the two main ideas in this paper -- A3C and curriculums -- are not really novel but the authors do make use of them in a real system.
This is a solid paper that applies A3C to Doom, enhancing it with a collection of tricks so as to win one of the VizDoom competitions. I think it is fair to expect the competition aspect to overshadow the more scientific approach of justifying every design decision in isolation, but in fact the authors do a decent job at the latter. Two of my concerns have remained unanswered (see AnonReviewer2, below). In addition, the citation list is rather thin, for example reward shaping has a rich literature, as do incrementally more difficult task setups, dating back at least to Mark Ring’s work in the 1990s. There has also been a lot of complementary work on other FPS games. I’m not asking that the authors do any direct comparisons, but to give the reader a sense of context in which to place this.
The paper combines semi-Markov models with state aggregation models to define a Semi-Aggregated Markov Decision Process (SAMDP). As per the reviews, the need for spatio-temporal representations is recognized as an important problem. Where the work currently falls short is the loose description of framework, which should be formalized for clarity and analysis, and in terms of its contributions and benefits with respect to the broader literature on hierarchical RL methods. The anonymous reviewers (and the additional public review/comment) provide a number of clear directions to pursue for the next iteration of this work; indeed, there is strong general enthusiasm for the greater goals (as shared by the relevant body of work in the research community), but the work is seen as requiring further clarity in formalization and presentation.
This work is interesting and one of the few papers out there to address the problem of autonomous skill acquisition in Deep RL. The importance of this problem cannot be overstated since Hierarchical RL is not only about using a meta-controller to appropriately use a specific skill from a repertoire of previously learned subgoals, but also about devising a set of subgoals through experience. Subgoals are inherently tied to the topology of the underlying problem and hence clustering approaches make sense. I would like to point out that a very similar attempt has been done in one of the past works in this area -
The paper starts by pointing out the need for methods that perform both state and temporal representation learning for RL and which allow gaining insight into what is being learned (perhaps in order to allow a human operator to intervene if necessary). This is a very important goal from a practical point of view, and it is great to see research in this direction. For this reason, I would like to encourage the authors to pursue this further. However, I am not at all convinced that the current incarnation of this work is the right answer. Part of the issues are more related to presentation, part may require rethinking. In order to get the "interpretability", the authors opt for some fairly specific ways of performing abstraction. For example, their skills always start In a single skill initiation state, and likewise end in one state. This seems unnecessarily restrictive, and it is not clear why this restriction is needed (other than convenience). Similarly, clustering is the basis for forming the higher level states, and there is a specific kind of clustering used here. Again, it is not clear why this has to be done via clustering as opposed to other methods. Ensuring temporal coherence in the particular form employed also seems restrictive. There is a reference to supplementary material where some of these choices are explained, but I could not find this in the posted version of the paper. The authors should either explain clearly why these specific choices are necessary, or (even better) try to think if they can be relaxed while still keeping interpretability. From a presentation point of view, the paper would benefit from formal definitions of AMDP and SAMDP, as well as from formal descriptions of the algorithms employed in constructing these representations (eg Bellman equations for the models, and update rules for the algorithms learning them). While intuitions are given, the math is not precisely stated. The overhead of constructing an SAMDP (computation time and space) should be clarified as well. The experiments are well carried out and it is nice to have both gridworld experiments, where visualization are easy to perform and understand, as well as Atari games (gridworld still have their place despite what other reviewers might say). The results are positive, but because the proposed approach has many moving parts which rely on specific choices, significance and general ease of use are unclear at this point. Perhaps having the complete supplementary would have helped in this respect. Small comment: The two lines after Eq 2 contain typos in the notation and a wrong sign in the equation.
The framework of semi-Markov decision processes (SDMPs) has been long used to model skill learning and temporal abstraction in reinforcement learning. This paper proposes a variant of such a model called a semi-aggregated MDP model. The formalism of SAMDP is not defined clearly enough to merit serious attention. The approach is quasi heuristic and explained through examples rather than clear definition. The work also lacks sufficient theoretical rigor. Simple experiments are proposed using 2D grid worlds to demonstrate skills. Grid worlds have served their purpose long enough in reinforcement learning, and it is time to retire them. More realistic domains are now routinely used and should be used in this paper as well.
The paper presents a method for visualization and analysis of policies from observed trajectories that the policies produce. The method infers higher level skills and clusters states. The result is a simplified, discrete higher-order state and action transition matrix. This model can be used for analysis, modeling, and interpretation. To construct semi-aggregated MDP the authors propose combining ideas for creating semi-MPDs and agregrated-MDPs. The method consists of choosing features, state clustering, skill inference, reward and skill length inference, and model selection. The method was demonstrated on a small grid-world problem, and DQN-trained agent for playing Atari games. The authors correctly identify that tools and means for interpretibility of RL methods are important for analysis, and deployment of such methods for real-world applications. This is particularly true in robotics and high-consequence systems. The end-result of the presented method is a high-level transition matrix. There is a big body of literature looking into hierarchical RL methods where lower level skills are combined with higher level policies. The presented method has the similar result, but the advantage of the presented method is that it comes up with a structure and analyzes already trained agent, which is very interesting. The paper would benefit from emphasizing this difference, and contrasting with the broader body of literature. To build the model, the authors propose combining the ideas from two existing ideas, semi-MPDs and agregrated-MDPs with using modified k-means for state clustering. It appears that the novelty of the presented method is limited. The paper would have been stronger if the authors explicitly stated the contributions over combining existing methods, and better highlighted the practical utility of the method. The evaluation section would be made stronger with more analytical results and precise evaluation, showing full strength of the method. The paper is difficult to read. To improve readability: - The Semi-Aggregated MDP section should include more precise description of the methods. The narrative that builds intuition is welcome. In addition to the existing narrative, algorithms and formulas where applicable should be included as well. - The paper should be self-contained. For example, more background on Occams Razor principle should be included. - Reduce the number of acronyms, in particular similarly sounding acronyms. Define acronyms before using. - Be more clear on the contributions, contrast with relevant literature, and the specific benefits of the presented method. - Fix typos, formatting mistakes etc., as they can be distracting for reading. The approach of reverse engineering the hierarchy, and learning high-level transition matrix is very interesting and promising. Perhaps the method can be used to outperform single network approach by using the model as an input to more specialized hierarchical trainers and learn complex behaviors more optimally then possible with one large network approach. Unfortunately, the paper falls short in the novelty, precision, and clarity.
The paper combines semi-Markov models with state aggregation models to define a Semi-Aggregated Markov Decision Process (SAMDP). As per the reviews, the need for spatio-temporal representations is recognized as an important problem. Where the work currently falls short is the loose description of framework, which should be formalized for clarity and analysis, and in terms of its contributions and benefits with respect to the broader literature on hierarchical RL methods. The anonymous reviewers (and the additional public review/comment) provide a number of clear directions to pursue for the next iteration of this work; indeed, there is strong general enthusiasm for the greater goals (as shared by the relevant body of work in the research community), but the work is seen as requiring further clarity in formalization and presentation.
This work is interesting and one of the few papers out there to address the problem of autonomous skill acquisition in Deep RL. The importance of this problem cannot be overstated since Hierarchical RL is not only about using a meta-controller to appropriately use a specific skill from a repertoire of previously learned subgoals, but also about devising a set of subgoals through experience. Subgoals are inherently tied to the topology of the underlying problem and hence clustering approaches make sense. I would like to point out that a very similar attempt has been done in one of the past works in this area -
The paper starts by pointing out the need for methods that perform both state and temporal representation learning for RL and which allow gaining insight into what is being learned (perhaps in order to allow a human operator to intervene if necessary). This is a very important goal from a practical point of view, and it is great to see research in this direction. For this reason, I would like to encourage the authors to pursue this further. However, I am not at all convinced that the current incarnation of this work is the right answer. Part of the issues are more related to presentation, part may require rethinking. In order to get the "interpretability", the authors opt for some fairly specific ways of performing abstraction. For example, their skills always start In a single skill initiation state, and likewise end in one state. This seems unnecessarily restrictive, and it is not clear why this restriction is needed (other than convenience). Similarly, clustering is the basis for forming the higher level states, and there is a specific kind of clustering used here. Again, it is not clear why this has to be done via clustering as opposed to other methods. Ensuring temporal coherence in the particular form employed also seems restrictive. There is a reference to supplementary material where some of these choices are explained, but I could not find this in the posted version of the paper. The authors should either explain clearly why these specific choices are necessary, or (even better) try to think if they can be relaxed while still keeping interpretability. From a presentation point of view, the paper would benefit from formal definitions of AMDP and SAMDP, as well as from formal descriptions of the algorithms employed in constructing these representations (eg Bellman equations for the models, and update rules for the algorithms learning them). While intuitions are given, the math is not precisely stated. The overhead of constructing an SAMDP (computation time and space) should be clarified as well. The experiments are well carried out and it is nice to have both gridworld experiments, where visualization are easy to perform and understand, as well as Atari games (gridworld still have their place despite what other reviewers might say). The results are positive, but because the proposed approach has many moving parts which rely on specific choices, significance and general ease of use are unclear at this point. Perhaps having the complete supplementary would have helped in this respect. Small comment: The two lines after Eq 2 contain typos in the notation and a wrong sign in the equation.
The framework of semi-Markov decision processes (SDMPs) has been long used to model skill learning and temporal abstraction in reinforcement learning. This paper proposes a variant of such a model called a semi-aggregated MDP model. The formalism of SAMDP is not defined clearly enough to merit serious attention. The approach is quasi heuristic and explained through examples rather than clear definition. The work also lacks sufficient theoretical rigor. Simple experiments are proposed using 2D grid worlds to demonstrate skills. Grid worlds have served their purpose long enough in reinforcement learning, and it is time to retire them. More realistic domains are now routinely used and should be used in this paper as well.
The paper presents a method for visualization and analysis of policies from observed trajectories that the policies produce. The method infers higher level skills and clusters states. The result is a simplified, discrete higher-order state and action transition matrix. This model can be used for analysis, modeling, and interpretation. To construct semi-aggregated MDP the authors propose combining ideas for creating semi-MPDs and agregrated-MDPs. The method consists of choosing features, state clustering, skill inference, reward and skill length inference, and model selection. The method was demonstrated on a small grid-world problem, and DQN-trained agent for playing Atari games. The authors correctly identify that tools and means for interpretibility of RL methods are important for analysis, and deployment of such methods for real-world applications. This is particularly true in robotics and high-consequence systems. The end-result of the presented method is a high-level transition matrix. There is a big body of literature looking into hierarchical RL methods where lower level skills are combined with higher level policies. The presented method has the similar result, but the advantage of the presented method is that it comes up with a structure and analyzes already trained agent, which is very interesting. The paper would benefit from emphasizing this difference, and contrasting with the broader body of literature. To build the model, the authors propose combining the ideas from two existing ideas, semi-MPDs and agregrated-MDPs with using modified k-means for state clustering. It appears that the novelty of the presented method is limited. The paper would have been stronger if the authors explicitly stated the contributions over combining existing methods, and better highlighted the practical utility of the method. The evaluation section would be made stronger with more analytical results and precise evaluation, showing full strength of the method. The paper is difficult to read. To improve readability: - The Semi-Aggregated MDP section should include more precise description of the methods. The narrative that builds intuition is welcome. In addition to the existing narrative, algorithms and formulas where applicable should be included as well. - The paper should be self-contained. For example, more background on Occams Razor principle should be included. - Reduce the number of acronyms, in particular similarly sounding acronyms. Define acronyms before using. - Be more clear on the contributions, contrast with relevant literature, and the specific benefits of the presented method. - Fix typos, formatting mistakes etc., as they can be distracting for reading. The approach of reverse engineering the hierarchy, and learning high-level transition matrix is very interesting and promising. Perhaps the method can be used to outperform single network approach by using the model as an input to more specialized hierarchical trainers and learn complex behaviors more optimally then possible with one large network approach. Unfortunately, the paper falls short in the novelty, precision, and clarity.
The authors have recently made several connections between deep learning and tensor algebra. While their earlier works dealt with supervised learning, the current work analyzes generative models through the lens of tensor algebra. The authors show propose a tensorial mixture model over local structures where the mixture components are expressed as tensor decompositions. They show that hierarchical tensor decomposition is exponentially more expressive compared to the shallow models. The paper makes original contributions in terms of establishing expressivity of deep generative models. The connections with tensor algebra could lead to further innovations, e.g. in training algorithms. However, the paper can be improved in two aspects: (1) It will be nice if the authors make a connection between the algebraic view presented here with the geometric view presented by:
This paper proposes a generative model for mixtures of basic local structures where the dependency between local structures is a tensor. They use tensor decomposition and the result of their earlier paper on expressive power of CNNs along with hierarchical Tucker to provide an inference mechanism. However, this is conditioned on the existence of decomposition. The authors do not discuss how applicable their method is for a general case, what is the subspace where this decomposition exists/is efficient/has low approximation error. Their answer to this question is that in deep learning era these theoretical analysis is not needed. While this claim is subjective, I need to emphasize that the paper does not clarify this claim and does not mention the restrictions. Hence, from theoretical perspective, the paper has flaws and the claims are not justified completely. Some claims cannot be justified with the current results in tensor literature as the authors also mentioned in the discussions. Therefore, they should have corrected their claims in the paper and made the clarifications that this approach is restricted to a clear subclass of tensors. If we ignore the theoretical aspect and only consider the paper from empirical perspective, the experiments the appear in the paper are not enough to accept the paper. MNIST and CIFAR-10 are very simple baselines and more extensive experiments are required. Also, the experiments for missing data are not covering real cases and are too synthetic. Also, the paper lacks the extension beyond images. Since the authors repeatedly mention that their approach goes beyond images, and since the theory part is not complete, those experiments are essential for acceptance of this paper.
This paper uses Tensors to build generative models. The main idea is to divide the input into regions represented with mixture models, and represent the joint distribution of the mixture components with a tensor. Then, by restricting themselves to tensors that have an efficient decomposition, they train convolutional arithmetic circuits to generate the probability of the input and class label, providing a generative model of the input and labels. This approach seems quite elegant. It is not completely clear to me how the authors choose the specific architecture for their model, and how these choices relate to the class of joint distributions that they can represent, but even if these choices are somewhat heuristic, the overall framework provides a nice way of controlling the generality of the distributions that are represented. The experiments are on simple, synthetic examples of missing data. This is somewhat of a limitation, and the paper would be more convincing if it could include experiments on a real-world problem that contained missing data. One issue here is that it must be known which elements of the input are missing, which somewhat limits applicability. Could experiments be run on problems relating to the Netflix challenge, which is the classic example of a prediction problem with missing data? In spite of these limitations, the experiments provide appropriate comparisons to prior work, and form a reasonable initial evaluation. I was a little confused about how the input of missing data is handled experimentally. From the introductory discussion my impression was that the generative model was built over region patches in the image. This led me to believe that they would marginalize over missing regions. However, when the missing data consists of IID randomly missing pixels, it seems that every region will be missing some information. Why is it appropriate to marginalize over missing pixels? Specifically, $x_i$ in Equation 6 represents a local region, and the ensuing discussion shows how to marginalize over missing regions. How is this done when only a subset of a region is missing? It also seems like the summation in the equation following Equation 6 could be quite large. What is the run time of this? The paper is also a bit schizophrenic about the extent to which the results are applicable beyond images. The motivation for the probabilistic model is mostly in terms of images. But in the experiments, the authors state that they do not use state-of-the-art inpainting algorithms because their method is not limited to images and they want to compare to methods that are restricted to images. This would be more convincing if there were experiments outside the image domain. It was also not clear to me how, if at all, the proposed network makes use of translation invariance. It is widely assumed that much of the success of CNNs comes from their encoding of translation invariance through weight sharing. Is such invariance built into the authors’ network? If not, why would we expect it to work well in challenging image domains? As a minor point, the paper is not carefully proofread. To just give a few examples from the first page or so: “significantly lesser” -> “significantly less” “the the” “provenly” -> provably
The paper provides an interesting use of generative models to address the classification with missing data problem. The tensorial mixture models proposed take into account the general problem of dependent samples. This is an nice extension of current mixture models where samples are usually considered as independent. Indeed the TMM model is reduced to the conventional latent variable models. As much as I love the ideas behind the paper, I feel pitiful about the sloppiness of the presentation (such as missing notations) and flaws in the technical derivations. Before going into the technical details, my high level concerns are as follows: (1) The joint density over all samples is modeled as a tensorial mixture generative model. The interpretation of the CP decomposition or HT decomposition on the prior density tensor is not clear. The authors have an interpretation of TMM as product of mixture models when samples are independent, however their interpretation seems flawed to me, and I will elaborate on this in the detailed technical comments below. (2) The authors employ convolution operators to compute an inner product. It is realizable by zero padding, but the invariance structure, which is the advantage of CNN compared to feed-forward neural network, will be lost. However, I am not sure how much this would affect the performance in practice. (3) The author could comment in the paper a little bit on the sample complexity of this method given the complexity of the model. Because I liked the ideas of the paper so much, and the ICLR paper submitted didn't present the technical details well due to sloppiness of notations, so I read the technical details in the arXiv version the authors pointed out. There are a few technical typos that I would like to point out (my reference to equations are to the ones in the arXiv paper). (1) The generative model as in figure (5) is flawed. P(x_i|d_i;theta_d_i) are vectors of length s, there the product of vectors is not well defined. It is obvious that the dimensions of the terms between two sides of the equation are not equal. In fact, this should be a tucker decomposition instead of multiplication. It should be P(X) = sum_d1,ldots,d_N P(d_1,ldots,d_N) (P(x_1|d_1;theta_d_1,P(x_2|d_2;theta_d_2,ldots,P(x_N|d_N;theta_d_N), which means a sum of multi-linear operation on tensor P(d_1,ldots,d_N), and each mode is projected onto P(x_i|d_i;theta_d_i. (2) I suspect the special case for diagonal Gaussian Mixture Models has some typos as I couldn't derive the third last equation on page 6. But it might be just I didn't understand this example. (3) The claim that TMM reduces to product of mixture model is not accurate. The first equation on page 7 is only right when "sum of product" operation is equal to "product of sum" operation. Similarly, in equation (6), the second equality doesn't hold unless in some special cases. However, this is not true. This might be just a typo, but it is good if the authors could fix this. I also suspect that if the authors correct this typo,the performance on MNIST might be improved. Overall, I like the ideas behind this paper very much. I suggest the authors fix the technical typos if the paper is accepted.
The authors have recently made several connections between deep learning and tensor algebra. While their earlier works dealt with supervised learning, the current work analyzes generative models through the lens of tensor algebra. The authors show propose a tensorial mixture model over local structures where the mixture components are expressed as tensor decompositions. They show that hierarchical tensor decomposition is exponentially more expressive compared to the shallow models. The paper makes original contributions in terms of establishing expressivity of deep generative models. The connections with tensor algebra could lead to further innovations, e.g. in training algorithms. However, the paper can be improved in two aspects: (1) It will be nice if the authors make a connection between the algebraic view presented here with the geometric view presented by:
This paper proposes a generative model for mixtures of basic local structures where the dependency between local structures is a tensor. They use tensor decomposition and the result of their earlier paper on expressive power of CNNs along with hierarchical Tucker to provide an inference mechanism. However, this is conditioned on the existence of decomposition. The authors do not discuss how applicable their method is for a general case, what is the subspace where this decomposition exists/is efficient/has low approximation error. Their answer to this question is that in deep learning era these theoretical analysis is not needed. While this claim is subjective, I need to emphasize that the paper does not clarify this claim and does not mention the restrictions. Hence, from theoretical perspective, the paper has flaws and the claims are not justified completely. Some claims cannot be justified with the current results in tensor literature as the authors also mentioned in the discussions. Therefore, they should have corrected their claims in the paper and made the clarifications that this approach is restricted to a clear subclass of tensors. If we ignore the theoretical aspect and only consider the paper from empirical perspective, the experiments the appear in the paper are not enough to accept the paper. MNIST and CIFAR-10 are very simple baselines and more extensive experiments are required. Also, the experiments for missing data are not covering real cases and are too synthetic. Also, the paper lacks the extension beyond images. Since the authors repeatedly mention that their approach goes beyond images, and since the theory part is not complete, those experiments are essential for acceptance of this paper.
This paper uses Tensors to build generative models. The main idea is to divide the input into regions represented with mixture models, and represent the joint distribution of the mixture components with a tensor. Then, by restricting themselves to tensors that have an efficient decomposition, they train convolutional arithmetic circuits to generate the probability of the input and class label, providing a generative model of the input and labels. This approach seems quite elegant. It is not completely clear to me how the authors choose the specific architecture for their model, and how these choices relate to the class of joint distributions that they can represent, but even if these choices are somewhat heuristic, the overall framework provides a nice way of controlling the generality of the distributions that are represented. The experiments are on simple, synthetic examples of missing data. This is somewhat of a limitation, and the paper would be more convincing if it could include experiments on a real-world problem that contained missing data. One issue here is that it must be known which elements of the input are missing, which somewhat limits applicability. Could experiments be run on problems relating to the Netflix challenge, which is the classic example of a prediction problem with missing data? In spite of these limitations, the experiments provide appropriate comparisons to prior work, and form a reasonable initial evaluation. I was a little confused about how the input of missing data is handled experimentally. From the introductory discussion my impression was that the generative model was built over region patches in the image. This led me to believe that they would marginalize over missing regions. However, when the missing data consists of IID randomly missing pixels, it seems that every region will be missing some information. Why is it appropriate to marginalize over missing pixels? Specifically, $x_i$ in Equation 6 represents a local region, and the ensuing discussion shows how to marginalize over missing regions. How is this done when only a subset of a region is missing? It also seems like the summation in the equation following Equation 6 could be quite large. What is the run time of this? The paper is also a bit schizophrenic about the extent to which the results are applicable beyond images. The motivation for the probabilistic model is mostly in terms of images. But in the experiments, the authors state that they do not use state-of-the-art inpainting algorithms because their method is not limited to images and they want to compare to methods that are restricted to images. This would be more convincing if there were experiments outside the image domain. It was also not clear to me how, if at all, the proposed network makes use of translation invariance. It is widely assumed that much of the success of CNNs comes from their encoding of translation invariance through weight sharing. Is such invariance built into the authors’ network? If not, why would we expect it to work well in challenging image domains? As a minor point, the paper is not carefully proofread. To just give a few examples from the first page or so: “significantly lesser” -> “significantly less” “the the” “provenly” -> provably
The paper provides an interesting use of generative models to address the classification with missing data problem. The tensorial mixture models proposed take into account the general problem of dependent samples. This is an nice extension of current mixture models where samples are usually considered as independent. Indeed the TMM model is reduced to the conventional latent variable models. As much as I love the ideas behind the paper, I feel pitiful about the sloppiness of the presentation (such as missing notations) and flaws in the technical derivations. Before going into the technical details, my high level concerns are as follows: (1) The joint density over all samples is modeled as a tensorial mixture generative model. The interpretation of the CP decomposition or HT decomposition on the prior density tensor is not clear. The authors have an interpretation of TMM as product of mixture models when samples are independent, however their interpretation seems flawed to me, and I will elaborate on this in the detailed technical comments below. (2) The authors employ convolution operators to compute an inner product. It is realizable by zero padding, but the invariance structure, which is the advantage of CNN compared to feed-forward neural network, will be lost. However, I am not sure how much this would affect the performance in practice. (3) The author could comment in the paper a little bit on the sample complexity of this method given the complexity of the model. Because I liked the ideas of the paper so much, and the ICLR paper submitted didn't present the technical details well due to sloppiness of notations, so I read the technical details in the arXiv version the authors pointed out. There are a few technical typos that I would like to point out (my reference to equations are to the ones in the arXiv paper). (1) The generative model as in figure (5) is flawed. P(x_i|d_i;theta_d_i) are vectors of length s, there the product of vectors is not well defined. It is obvious that the dimensions of the terms between two sides of the equation are not equal. In fact, this should be a tucker decomposition instead of multiplication. It should be P(X) = sum_d1,ldots,d_N P(d_1,ldots,d_N) (P(x_1|d_1;theta_d_1,P(x_2|d_2;theta_d_2,ldots,P(x_N|d_N;theta_d_N), which means a sum of multi-linear operation on tensor P(d_1,ldots,d_N), and each mode is projected onto P(x_i|d_i;theta_d_i. (2) I suspect the special case for diagonal Gaussian Mixture Models has some typos as I couldn't derive the third last equation on page 6. But it might be just I didn't understand this example. (3) The claim that TMM reduces to product of mixture model is not accurate. The first equation on page 7 is only right when "sum of product" operation is equal to "product of sum" operation. Similarly, in equation (6), the second equality doesn't hold unless in some special cases. However, this is not true. This might be just a typo, but it is good if the authors could fix this. I also suspect that if the authors correct this typo,the performance on MNIST might be improved. Overall, I like the ideas behind this paper very much. I suggest the authors fix the technical typos if the paper is accepted.
I find this paper not very compelling. The basic idea seems to be that we can put a fast neighbor searcher into a memory augmented net to make the memory lookups scalable. However, this was precisely the point of Rae et al. There are a number of standardized neighbor searchers; I don't understand why the authors choose to use their own (which they do not benchmark against the standards). Moreover, they test on a problem where there is no clear need for (vector based) fast-nn, because one can use hashing on the text. I also find the repeated distinction between "mips" and "nns" distracting; most libraries that can do one can do the other, or inputs can be modified to switch between the problems; indeed the authors do this when they convert to the "mcss" problem.
This paper was reviewed by three experts. While they find interesting ideas in the manuscript, all three point to deficiencies (unconvincing results, etc) and unanimously recommend rejection.
1. The hierarchical memory is fixed, not learned, and there is no hierarchical in the experimental section, only one layer for softmax layer. 2. It shows the 10-mips > 100-mips > 1000-mips, does it mean 1-mips is the best one we should adopt? 3. Approximated k-mips is worse than even original method. Why does it need exact k-mips? It seems the proposed method is not robust.
The paper proposes an algorithm for training memory networks which have very large memories. Training such models in traditional ways, by using soft-attention mechanism over all the memory slots is not only slow, it is also harder to train due to dispersion of gradients. The paper proposes to use the k-mips algorithm over the memories to choose a subset of the memory slots over which the attention is applied. Since the cost of exact k-mips is the same as doing full attention, the authors propose to use approximate k-mips, which while faster to compute, results in inferior performance. An artifact of using k-mips is that one cannot learn the memory slots. Hence they are pre-trained and kept fixed during entire training. The experimental section shows the efficacy of using k-mips using the SimpleQuestions dataset. The exact k-mips results in the same performance as the full attention. The approximate k-mips results in deterioration in performance. The paper is quite clearly written and easy to understand. I think the ideas proposed in the paper are not super convincing. I have a number of issues with this paper. 1. The k-mips algorithm forces the memories to be fixed. This to me is a rather limiting constraint, especially on problems/dataset which will require multiple hops of training to do compounded reasoning. As a results I'm not entirely sure about the usefulness of this technique. 2. Furthermore, the exact k-mips is the sample complexity as the full attention. The only way to achieve speedup is to use approx k-mips. That, as expected, results in a significant drop in performance. 3. The paper motivates the ideas by proposing solutions to eliminate heuristics used to prune the memories. However in Section 3.1 the authors themselves end up using multiple heuristics to make the training work. Agreed, that the used heuristics are not data dependent, but still, it feels like they are kicking the can down the road as far as heuristics are concerned. 4. The experimental results are not very convincing. First there is no speed comparison. Second, the authors do not compare with methods other than k-mips which do fast nearest neighbor search, such as, FLANN.
I find this paper not very compelling. The basic idea seems to be that we can put a fast neighbor searcher into a memory augmented net to make the memory lookups scalable. However, this was precisely the point of Rae et al. There are a number of standardized neighbor searchers; I don't understand why the authors choose to use their own (which they do not benchmark against the standards). Moreover, they test on a problem where there is no clear need for (vector based) fast-nn, because one can use hashing on the text. I also find the repeated distinction between "mips" and "nns" distracting; most libraries that can do one can do the other, or inputs can be modified to switch between the problems; indeed the authors do this when they convert to the "mcss" problem.
I find this paper not very compelling. The basic idea seems to be that we can put a fast neighbor searcher into a memory augmented net to make the memory lookups scalable. However, this was precisely the point of Rae et al. There are a number of standardized neighbor searchers; I don't understand why the authors choose to use their own (which they do not benchmark against the standards). Moreover, they test on a problem where there is no clear need for (vector based) fast-nn, because one can use hashing on the text. I also find the repeated distinction between "mips" and "nns" distracting; most libraries that can do one can do the other, or inputs can be modified to switch between the problems; indeed the authors do this when they convert to the "mcss" problem.
This paper was reviewed by three experts. While they find interesting ideas in the manuscript, all three point to deficiencies (unconvincing results, etc) and unanimously recommend rejection.
1. The hierarchical memory is fixed, not learned, and there is no hierarchical in the experimental section, only one layer for softmax layer. 2. It shows the 10-mips > 100-mips > 1000-mips, does it mean 1-mips is the best one we should adopt? 3. Approximated k-mips is worse than even original method. Why does it need exact k-mips? It seems the proposed method is not robust.
The paper proposes an algorithm for training memory networks which have very large memories. Training such models in traditional ways, by using soft-attention mechanism over all the memory slots is not only slow, it is also harder to train due to dispersion of gradients. The paper proposes to use the k-mips algorithm over the memories to choose a subset of the memory slots over which the attention is applied. Since the cost of exact k-mips is the same as doing full attention, the authors propose to use approximate k-mips, which while faster to compute, results in inferior performance. An artifact of using k-mips is that one cannot learn the memory slots. Hence they are pre-trained and kept fixed during entire training. The experimental section shows the efficacy of using k-mips using the SimpleQuestions dataset. The exact k-mips results in the same performance as the full attention. The approximate k-mips results in deterioration in performance. The paper is quite clearly written and easy to understand. I think the ideas proposed in the paper are not super convincing. I have a number of issues with this paper. 1. The k-mips algorithm forces the memories to be fixed. This to me is a rather limiting constraint, especially on problems/dataset which will require multiple hops of training to do compounded reasoning. As a results I'm not entirely sure about the usefulness of this technique. 2. Furthermore, the exact k-mips is the sample complexity as the full attention. The only way to achieve speedup is to use approx k-mips. That, as expected, results in a significant drop in performance. 3. The paper motivates the ideas by proposing solutions to eliminate heuristics used to prune the memories. However in Section 3.1 the authors themselves end up using multiple heuristics to make the training work. Agreed, that the used heuristics are not data dependent, but still, it feels like they are kicking the can down the road as far as heuristics are concerned. 4. The experimental results are not very convincing. First there is no speed comparison. Second, the authors do not compare with methods other than k-mips which do fast nearest neighbor search, such as, FLANN.
I find this paper not very compelling. The basic idea seems to be that we can put a fast neighbor searcher into a memory augmented net to make the memory lookups scalable. However, this was precisely the point of Rae et al. There are a number of standardized neighbor searchers; I don't understand why the authors choose to use their own (which they do not benchmark against the standards). Moreover, they test on a problem where there is no clear need for (vector based) fast-nn, because one can use hashing on the text. I also find the repeated distinction between "mips" and "nns" distracting; most libraries that can do one can do the other, or inputs can be modified to switch between the problems; indeed the authors do this when they convert to the "mcss" problem.
This paper proposes a model that is able to infer a program from input/output example pairs, focusing on a restricted domain-specific language that captures a fairly wide variety of string transformations, similar to that used by Flash Fill in Excel. The approach is to model successive “extensions” of a program tree conditioned on some embedding of the input/output pairs. Extension probabilities are computed as a function of leaf and production rule embeddings — one of the main contributions is the so-called “Recursive-Reverse-Recursive Neural Net” which computes a globally aware embedding of a leaf by doing something that looks like belief propagation on a tree (but training this operation in an end-to-end differentiable way). There are many strong points about this paper. In contrast with some of the related work in the deep learning community, I can imagine this being used in an actual application in the near future. The R3NN idea is a good one and the authors motivate it quite well. Moreover, the authors have explored many variants of this model to understand what works well and what does not. Finally, the exposition is clear (even if it is a long paper), which made this paper a pleasure to read. Some weaknesses of this paper: the results are still not super accurate, perhaps because the model has only been trained on small programs but is being asked to infer programs that should be much longer. And it’s unclear why the authors did not simply train on longer programs... It also seems that the number of I/O pairs is fixed? So if I had more I/O pairs, the model might not be able to use those additional pairs (and based on the experiments, more pairs can hurt...). Overall however, I would certainly like to see this paper accepted at ICLR. Other miscellaneous comments: * Too many e’s in the expansion probability expression — might be better just to write “Softmax”. * There is a comment about adding a bidirectional LSTM to process the global leaf representations before calculating scores, but no details are given on how this is done (as far as I can see). * The authors claim that using hyperbolic tangent activation functions is important — I’d be interested in some more discussion on this and why something like ReLU would not be good. * It’s unclear to me how batching was done in this setting since each program has a different tree topology. More discussion on this would be appreciated. Related to this, it would be good to add details on optimization algorithm (SGD? Adagrad? Adam?), learning rate schedules and how weights were initialized. At the moment, the results are not particularly reproducible. * In Figure 6 (unsolved benchmarks), it would be great to add the program sizes for these harder examples (i.e., did the approach fail because these benchmarks require long programs? Or was it some other reason?) * There is a missing related work by Piech et al (Learning Program Embeddings...) where the authors trained a recursive neural network (that matched abstract syntax trees for programs submitted to an online course) to predict program output (but did not synthesize programs).
There is a bit of a spread in the reviewer scores and unfortunately it wasn't possible to entice the reviewers to participate in a discussion. The area chair therefore discounts the late review of reviewer3, who seems to have had a misunderstanding that was successfully rebutted by the authors. The other reviewers are supportive of the paper.
Dear reviewers, do you have any reactions after the authors responded to your reviews?
The paper presents a method to synthesize string manipulation programs based on a set of input output pairs. The paper focuses on a restricted class of programs based on a simple context free grammar sufficient to solve string manipulation tasks from the FlashFill benchmark. A probabilistic generative model called Recursive-Reverse-Recursive Neural Network (R3NN) is presented that assigns a probability to each program's parse tree after a bottom-up and a top-down pass. Results are presented on a synthetic dataset and a Microsoft Excel benchmark called FlashFill. The problem of program synthesis is important with a lot of recent interest from the deep learning community. The approach taken in the paper based on parse trees and recursive neural networks seems interesting and promising. However, the model seems too complicated and unclear at several places (details below). On the negative side, the experiments are particularly weak, and the paper does not seem ready for publication based on its experimental results. I was positive about the paper until I realized that the method obtains an accuracy of 38% on FlashFill benchmark when presented with only 5 input-output examples but the performance degrades to 29% when 10 input-output examples are used. This was surprising to the authors too, and they came up with some hypothesis to explain this phenomenon. To me, this is a big problem indicating either a bug in the code or a severe shortcoming of the model. Any model useful for program synthesis needs to be applicable to many input-output examples because most complicated programs require many examples to disambiguate the details of the program. Given the shortcoming of the experiments, I am not convinced that the paper is ready for publication. Thus, I recommend weak reject. I encourage the authors to address the comments below and resubmit as the general idea seems promising. More comments: I am unclear about the model at several places: - How is the probability distribution normalized? Given the nature of bottom-up top-down evaluation of the potentials, should one enumerate over different completions of a program and the compare their exponentiated potentials? If so, does this restrict the applicability of the model to long programs as the enumeration of the completions gets prohibitively slow? - What if you only use 1 input-output pair for each program instead of 5? Do the results get better? - Section 5.1.2 is not clear to me. Can you elaborate by potentially including some examples? Does your input-output representation pre-supposes a fixed number of input-output examples across tasks (e.g. 5 or 10 for all of the tasks)? Regarding the experiments, - Could you present some baseline results on FlashFill benchmark based on previous work? - Is your method only applicable to short programs? (based on the choice of 13 for the number of instructions) - Does a program considered correct when it is identical to a test program, or is it considered correct when it succeeds on a set of held-out input-output pairs? - When using 100 or more program samples, do you report the accuracy of the best program out of 100 (i.e. recall) or do you first filter the programs based on training input-output pairs and then evaluate a program that is selected? Your paper is well beyond the recommended limit of 8 pages. please consider making it shorter.
This paper sets out to tackle the program synthesis problem: given a set of input/output pairs discover the program that generated them. The authors propose a bipartite model, with one component that is a generative model of tree-structured programs and the other component an input/output pair encoder for conditioning. They consider applying many variants of this basic model to a FlashFill DSL. The experiments explore a practical dataset and achieve fine numbers. The range of models considered, carefulness of the exposition, and basic experimental setup make this a valuable paper for an important area of research. I have a few questions, which I think would strengthen the paper, but think it's worth accepting as is. Questions/Comments: - The dataset is a good choice, because it is simple and easy to understand. What is the effect of the "rule based strategy" for computing well formed input strings? - Clarify what "backtracking search" is? I assume it is the same as trying to generate the latent function? - In general describing the accuracy as you increase the sample size could be summarize simply by reporting the log-probability of the latent function. Perhaps it's worth reporting that? Not sure if I missed something.
This paper proposes a model that is able to infer a program from input/output example pairs, focusing on a restricted domain-specific language that captures a fairly wide variety of string transformations, similar to that used by Flash Fill in Excel. The approach is to model successive “extensions” of a program tree conditioned on some embedding of the input/output pairs. Extension probabilities are computed as a function of leaf and production rule embeddings — one of the main contributions is the so-called “Recursive-Reverse-Recursive Neural Net” which computes a globally aware embedding of a leaf by doing something that looks like belief propagation on a tree (but training this operation in an end-to-end differentiable way). There are many strong points about this paper. In contrast with some of the related work in the deep learning community, I can imagine this being used in an actual application in the near future. The R3NN idea is a good one and the authors motivate it quite well. Moreover, the authors have explored many variants of this model to understand what works well and what does not. Finally, the exposition is clear (even if it is a long paper), which made this paper a pleasure to read. Some weaknesses of this paper: the results are still not super accurate, perhaps because the model has only been trained on small programs but is being asked to infer programs that should be much longer. And it’s unclear why the authors did not simply train on longer programs... It also seems that the number of I/O pairs is fixed? So if I had more I/O pairs, the model might not be able to use those additional pairs (and based on the experiments, more pairs can hurt...). Overall however, I would certainly like to see this paper accepted at ICLR. Other miscellaneous comments: * Too many e’s in the expansion probability expression — might be better just to write “Softmax”. * There is a comment about adding a bidirectional LSTM to process the global leaf representations before calculating scores, but no details are given on how this is done (as far as I can see). * The authors claim that using hyperbolic tangent activation functions is important — I’d be interested in some more discussion on this and why something like ReLU would not be good. * It’s unclear to me how batching was done in this setting since each program has a different tree topology. More discussion on this would be appreciated. Related to this, it would be good to add details on optimization algorithm (SGD? Adagrad? Adam?), learning rate schedules and how weights were initialized. At the moment, the results are not particularly reproducible. * In Figure 6 (unsolved benchmarks), it would be great to add the program sizes for these harder examples (i.e., did the approach fail because these benchmarks require long programs? Or was it some other reason?) * There is a missing related work by Piech et al (Learning Program Embeddings...) where the authors trained a recursive neural network (that matched abstract syntax trees for programs submitted to an online course) to predict program output (but did not synthesize programs).
Few remarks: The paper explains what the tree is and how examples are encoded, but its missing important explanation on: - how the I/O examples are encoded in the tree during training exactly. - it is not well explained how the prediction works during testing when the examples are given. It looks like the DSL is restricted to know all constant strings that will be used, which seems difficult in realistic scenarios.
This paper proposes a model that is able to infer a program from input/output example pairs, focusing on a restricted domain-specific language that captures a fairly wide variety of string transformations, similar to that used by Flash Fill in Excel. The approach is to model successive “extensions” of a program tree conditioned on some embedding of the input/output pairs. Extension probabilities are computed as a function of leaf and production rule embeddings — one of the main contributions is the so-called “Recursive-Reverse-Recursive Neural Net” which computes a globally aware embedding of a leaf by doing something that looks like belief propagation on a tree (but training this operation in an end-to-end differentiable way). There are many strong points about this paper. In contrast with some of the related work in the deep learning community, I can imagine this being used in an actual application in the near future. The R3NN idea is a good one and the authors motivate it quite well. Moreover, the authors have explored many variants of this model to understand what works well and what does not. Finally, the exposition is clear (even if it is a long paper), which made this paper a pleasure to read. Some weaknesses of this paper: the results are still not super accurate, perhaps because the model has only been trained on small programs but is being asked to infer programs that should be much longer. And it’s unclear why the authors did not simply train on longer programs... It also seems that the number of I/O pairs is fixed? So if I had more I/O pairs, the model might not be able to use those additional pairs (and based on the experiments, more pairs can hurt...). Overall however, I would certainly like to see this paper accepted at ICLR. Other miscellaneous comments: * Too many e’s in the expansion probability expression — might be better just to write “Softmax”. * There is a comment about adding a bidirectional LSTM to process the global leaf representations before calculating scores, but no details are given on how this is done (as far as I can see). * The authors claim that using hyperbolic tangent activation functions is important — I’d be interested in some more discussion on this and why something like ReLU would not be good. * It’s unclear to me how batching was done in this setting since each program has a different tree topology. More discussion on this would be appreciated. Related to this, it would be good to add details on optimization algorithm (SGD? Adagrad? Adam?), learning rate schedules and how weights were initialized. At the moment, the results are not particularly reproducible. * In Figure 6 (unsolved benchmarks), it would be great to add the program sizes for these harder examples (i.e., did the approach fail because these benchmarks require long programs? Or was it some other reason?) * There is a missing related work by Piech et al (Learning Program Embeddings...) where the authors trained a recursive neural network (that matched abstract syntax trees for programs submitted to an online course) to predict program output (but did not synthesize programs).
There is a bit of a spread in the reviewer scores and unfortunately it wasn't possible to entice the reviewers to participate in a discussion. The area chair therefore discounts the late review of reviewer3, who seems to have had a misunderstanding that was successfully rebutted by the authors. The other reviewers are supportive of the paper.
Dear reviewers, do you have any reactions after the authors responded to your reviews?
The paper presents a method to synthesize string manipulation programs based on a set of input output pairs. The paper focuses on a restricted class of programs based on a simple context free grammar sufficient to solve string manipulation tasks from the FlashFill benchmark. A probabilistic generative model called Recursive-Reverse-Recursive Neural Network (R3NN) is presented that assigns a probability to each program's parse tree after a bottom-up and a top-down pass. Results are presented on a synthetic dataset and a Microsoft Excel benchmark called FlashFill. The problem of program synthesis is important with a lot of recent interest from the deep learning community. The approach taken in the paper based on parse trees and recursive neural networks seems interesting and promising. However, the model seems too complicated and unclear at several places (details below). On the negative side, the experiments are particularly weak, and the paper does not seem ready for publication based on its experimental results. I was positive about the paper until I realized that the method obtains an accuracy of 38% on FlashFill benchmark when presented with only 5 input-output examples but the performance degrades to 29% when 10 input-output examples are used. This was surprising to the authors too, and they came up with some hypothesis to explain this phenomenon. To me, this is a big problem indicating either a bug in the code or a severe shortcoming of the model. Any model useful for program synthesis needs to be applicable to many input-output examples because most complicated programs require many examples to disambiguate the details of the program. Given the shortcoming of the experiments, I am not convinced that the paper is ready for publication. Thus, I recommend weak reject. I encourage the authors to address the comments below and resubmit as the general idea seems promising. More comments: I am unclear about the model at several places: - How is the probability distribution normalized? Given the nature of bottom-up top-down evaluation of the potentials, should one enumerate over different completions of a program and the compare their exponentiated potentials? If so, does this restrict the applicability of the model to long programs as the enumeration of the completions gets prohibitively slow? - What if you only use 1 input-output pair for each program instead of 5? Do the results get better? - Section 5.1.2 is not clear to me. Can you elaborate by potentially including some examples? Does your input-output representation pre-supposes a fixed number of input-output examples across tasks (e.g. 5 or 10 for all of the tasks)? Regarding the experiments, - Could you present some baseline results on FlashFill benchmark based on previous work? - Is your method only applicable to short programs? (based on the choice of 13 for the number of instructions) - Does a program considered correct when it is identical to a test program, or is it considered correct when it succeeds on a set of held-out input-output pairs? - When using 100 or more program samples, do you report the accuracy of the best program out of 100 (i.e. recall) or do you first filter the programs based on training input-output pairs and then evaluate a program that is selected? Your paper is well beyond the recommended limit of 8 pages. please consider making it shorter.
This paper sets out to tackle the program synthesis problem: given a set of input/output pairs discover the program that generated them. The authors propose a bipartite model, with one component that is a generative model of tree-structured programs and the other component an input/output pair encoder for conditioning. They consider applying many variants of this basic model to a FlashFill DSL. The experiments explore a practical dataset and achieve fine numbers. The range of models considered, carefulness of the exposition, and basic experimental setup make this a valuable paper for an important area of research. I have a few questions, which I think would strengthen the paper, but think it's worth accepting as is. Questions/Comments: - The dataset is a good choice, because it is simple and easy to understand. What is the effect of the "rule based strategy" for computing well formed input strings? - Clarify what "backtracking search" is? I assume it is the same as trying to generate the latent function? - In general describing the accuracy as you increase the sample size could be summarize simply by reporting the log-probability of the latent function. Perhaps it's worth reporting that? Not sure if I missed something.
This paper proposes a model that is able to infer a program from input/output example pairs, focusing on a restricted domain-specific language that captures a fairly wide variety of string transformations, similar to that used by Flash Fill in Excel. The approach is to model successive “extensions” of a program tree conditioned on some embedding of the input/output pairs. Extension probabilities are computed as a function of leaf and production rule embeddings — one of the main contributions is the so-called “Recursive-Reverse-Recursive Neural Net” which computes a globally aware embedding of a leaf by doing something that looks like belief propagation on a tree (but training this operation in an end-to-end differentiable way). There are many strong points about this paper. In contrast with some of the related work in the deep learning community, I can imagine this being used in an actual application in the near future. The R3NN idea is a good one and the authors motivate it quite well. Moreover, the authors have explored many variants of this model to understand what works well and what does not. Finally, the exposition is clear (even if it is a long paper), which made this paper a pleasure to read. Some weaknesses of this paper: the results are still not super accurate, perhaps because the model has only been trained on small programs but is being asked to infer programs that should be much longer. And it’s unclear why the authors did not simply train on longer programs... It also seems that the number of I/O pairs is fixed? So if I had more I/O pairs, the model might not be able to use those additional pairs (and based on the experiments, more pairs can hurt...). Overall however, I would certainly like to see this paper accepted at ICLR. Other miscellaneous comments: * Too many e’s in the expansion probability expression — might be better just to write “Softmax”. * There is a comment about adding a bidirectional LSTM to process the global leaf representations before calculating scores, but no details are given on how this is done (as far as I can see). * The authors claim that using hyperbolic tangent activation functions is important — I’d be interested in some more discussion on this and why something like ReLU would not be good. * It’s unclear to me how batching was done in this setting since each program has a different tree topology. More discussion on this would be appreciated. Related to this, it would be good to add details on optimization algorithm (SGD? Adagrad? Adam?), learning rate schedules and how weights were initialized. At the moment, the results are not particularly reproducible. * In Figure 6 (unsolved benchmarks), it would be great to add the program sizes for these harder examples (i.e., did the approach fail because these benchmarks require long programs? Or was it some other reason?) * There is a missing related work by Piech et al (Learning Program Embeddings...) where the authors trained a recursive neural network (that matched abstract syntax trees for programs submitted to an online course) to predict program output (but did not synthesize programs).
Few remarks: The paper explains what the tree is and how examples are encoded, but its missing important explanation on: - how the I/O examples are encoded in the tree during training exactly. - it is not well explained how the prediction works during testing when the examples are given. It looks like the DSL is restricted to know all constant strings that will be used, which seems difficult in realistic scenarios.
Altogether a very good paper, a nice read, and interesting. The work advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough. One caveat is that although the approach is intended to be general, no theoretical guarantees are provided about the learning performance. Privacy-preserving machine learning papers often analyze both the privacy (in the worst case, DP setting) and the learning performance (often under different assumptions). Since the learning performance might depend on the choice of architecture; future experimentation is encouraged, even using the same data sets, with different architectures. If this will not be added, then please justify the choice of architecture used, and/or clarify what can be generalized about the observed learning performance. Another caveat is that the reported epsilons are not those that can be privately released; the authors note that their technique for doing so would change the resulting epsilon. However this would need to be resolved in order to have a meaningful comparison to the epsilon-delta values reported in related work. Finally, as has been acknowledged in the paper, the present approach may not work on other natural data types. Experiments on other data sets is strongly encouraged. Also, please cite the data sets used. Other comments: Discussion of certain parts of the related work are thorough. However, please add some survey/discussion of the related work on differentially-private semi-supervised learning. For example, in the context of random forests, the following paper also proposed differentially-private semi-supervised learning via a teacher-learner approach (although not denoted as “teacher-learner”). The only time the private labeled data is used is when learning the “primary ensemble.” A "secondary ensemble" is then learned only from the unlabeled (non-private) data, with pseudo-labels generated by the primary ensemble. G. Jagannathan, C. Monteleoni, and K. Pillaipakkamnatt: A Semi-Supervised Learning Approach to Differential Privacy. Proc. 2013 IEEE International Conference on Data Mining Workshops, IEEE Workshop on Privacy Aspects of Data Mining (PADM), 2013. Section C. does a nice comparison of approaches. Please make sure the quantitative results here constitute an apples-to-apples comparison with the GAN results. The paper is extremely well-written, for the most part. Some places needing clarification include: - Last paragraph of 3.1. “all teachers....get the same training data....” This should be rephrased to make it clear that it is not the same w.r.t. all the teachers, but w.r.t. the same teacher on the neighboring database. - 4.1: The authors state: “The number n of teachers is limited by a trade-off between the classification task’s complexity and the available data.” However, since this tradeoff is not formalized, the statement is imprecise. In particular, if the analysis is done in the i.i.d. setting, the tradeoff would also likely depend on the relation of the target hypothesis to the data distribution. - Discussion of figure 3 was rather unclear in the text and caption and should be revised for clarity. In the text section, at first the explanation seems to imply that a larger gap is better (as is also indicated in the caption). However later it is stated that the gap stays under 20%. These sentences seem contradictory, which is likely not what was intended.
The paper presents a general teacher-student approach for differentially-private learning in which the student learns to predict a noise vote among a set of teachers. The noise allows the student to be differentially private, whilst maintaining good classification accuracies on MNIST and SVHN. The paper is well-written.
I think this paper has great impact. My question is what is the "auxiliary input" in Definition 2. Could you explain this term in theoretical view and what is that in your paper?
Thank you for providing an interesting paper. In the paper, the student model is trained by semi-supervised fashion as suggested in (Salimans et al., 2016). As far as I understand, teacher's ensembles are using for supervised learning and nonsensitive data is for unsupervised learning. So, my question is "Where is the generator ?". The aggregation of teacher network is treated as the generator in GAN framework?
Hi, I have few questions about the paper. 1- What attacker's goal did you consider in your paper? Is it recovering the training data, or checking whether a specific sample has been in the training data? 2- If the attacker's goal is to recover the training data, does the attacker want to recover the exact data or an approximation would be OK? 3- Talking about neural networks: - Do you think there is any attack method to recover an exact training data from the learning model? - Do you think there is any defense method to prevent an attacker from recovering even an approximate training data? 4- How can we quantify the strength of a learning model (specifically neural networks) without any defensive mechanism? 5- How can we quantify the strength of a learning model which has not been trained on exact training data? For example, some forms of adversarial training methods never train the model on the clean data; instead, at each epoch, the model is trained on different adversarial data derived from the real data. - How can the model "memorize" the training data, when 1) it has never seen the real data, 2) it has been trained on different data in different epochs? 6- How do you compare the performance of your method with adversarial training? Thanks.
This paper addresses the problem of achieving differential privacy in a very general scenario where a set of teachers is trained on disjoint subsets of sensitive data and the student performs prediction based on public data labeled by teachers through noisy voting. I found the approach altogether plausible and very clearly explained by the authors. Adding more discussion of the bound (and its tightness) from Theorem 1 itself would be appreciated. A simple idea of adding perturbation error to the counts, known from differentially-private literature, is nicely re-used by the authors and elegantly applied in a much broader (non-convex setting) and practical context than in a number of differentially-private and other related papers. The generality of the approach, clear improvement over predecessors, and clarity of the writing makes the method worth publishing.
This paper discusses how to guarantee privacy for training data. In the proposed approach multiple models trained with disjoint datasets are used as teachers'' model, which will train a student'' model to predict an output chosen by noisy voting among all of the teachers. The theoretical results are nice but also intuitive. Since teachers' result are provided via noisy voting, the student model may not duplicate the teacher's behavior. However, the probabilistic bound has quite a number of empirical parameters, which makes me difficult to decide whether the security is 100% guaranteed or not. The experiments on MNIST and SVHN are good. However, as the paper claims, the proposed approach may be mostly useful for sensitive data like medical histories, it will be nice to conduct one or two experiments on such applications.
Altogether a very good paper, a nice read, and interesting. The work advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough. One caveat is that although the approach is intended to be general, no theoretical guarantees are provided about the learning performance. Privacy-preserving machine learning papers often analyze both the privacy (in the worst case, DP setting) and the learning performance (often under different assumptions). Since the learning performance might depend on the choice of architecture; future experimentation is encouraged, even using the same data sets, with different architectures. If this will not be added, then please justify the choice of architecture used, and/or clarify what can be generalized about the observed learning performance. Another caveat is that the reported epsilons are not those that can be privately released; the authors note that their technique for doing so would change the resulting epsilon. However this would need to be resolved in order to have a meaningful comparison to the epsilon-delta values reported in related work. Finally, as has been acknowledged in the paper, the present approach may not work on other natural data types. Experiments on other data sets is strongly encouraged. Also, please cite the data sets used. Other comments: Discussion of certain parts of the related work are thorough. However, please add some survey/discussion of the related work on differentially-private semi-supervised learning. For example, in the context of random forests, the following paper also proposed differentially-private semi-supervised learning via a teacher-learner approach (although not denoted as “teacher-learner”). The only time the private labeled data is used is when learning the “primary ensemble.” A "secondary ensemble" is then learned only from the unlabeled (non-private) data, with pseudo-labels generated by the primary ensemble. G. Jagannathan, C. Monteleoni, and K. Pillaipakkamnatt: A Semi-Supervised Learning Approach to Differential Privacy. Proc. 2013 IEEE International Conference on Data Mining Workshops, IEEE Workshop on Privacy Aspects of Data Mining (PADM), 2013. Section C. does a nice comparison of approaches. Please make sure the quantitative results here constitute an apples-to-apples comparison with the GAN results. The paper is extremely well-written, for the most part. Some places needing clarification include: - Last paragraph of 3.1. “all teachers....get the same training data....” This should be rephrased to make it clear that it is not the same w.r.t. all the teachers, but w.r.t. the same teacher on the neighboring database. - 4.1: The authors state: “The number n of teachers is limited by a trade-off between the classification task’s complexity and the available data.” However, since this tradeoff is not formalized, the statement is imprecise. In particular, if the analysis is done in the i.i.d. setting, the tradeoff would also likely depend on the relation of the target hypothesis to the data distribution. - Discussion of figure 3 was rather unclear in the text and caption and should be revised for clarity. In the text section, at first the explanation seems to imply that a larger gap is better (as is also indicated in the caption). However later it is stated that the gap stays under 20%. These sentences seem contradictory, which is likely not what was intended.
Thanks for interesting and well-organized papers. I have a question about teacher-student model. Teachers are trained on sensitive data, and students are trained on non-sensitive data. I wonder how students work on the outputs of teachers. Sensitive and non-sensitive are different attributes, so I think there are no correlation between teachers and students. Please give me some more details. Thanks.
Altogether a very good paper, a nice read, and interesting. The work advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough. One caveat is that although the approach is intended to be general, no theoretical guarantees are provided about the learning performance. Privacy-preserving machine learning papers often analyze both the privacy (in the worst case, DP setting) and the learning performance (often under different assumptions). Since the learning performance might depend on the choice of architecture; future experimentation is encouraged, even using the same data sets, with different architectures. If this will not be added, then please justify the choice of architecture used, and/or clarify what can be generalized about the observed learning performance. Another caveat is that the reported epsilons are not those that can be privately released; the authors note that their technique for doing so would change the resulting epsilon. However this would need to be resolved in order to have a meaningful comparison to the epsilon-delta values reported in related work. Finally, as has been acknowledged in the paper, the present approach may not work on other natural data types. Experiments on other data sets is strongly encouraged. Also, please cite the data sets used. Other comments: Discussion of certain parts of the related work are thorough. However, please add some survey/discussion of the related work on differentially-private semi-supervised learning. For example, in the context of random forests, the following paper also proposed differentially-private semi-supervised learning via a teacher-learner approach (although not denoted as “teacher-learner”). The only time the private labeled data is used is when learning the “primary ensemble.” A "secondary ensemble" is then learned only from the unlabeled (non-private) data, with pseudo-labels generated by the primary ensemble. G. Jagannathan, C. Monteleoni, and K. Pillaipakkamnatt: A Semi-Supervised Learning Approach to Differential Privacy. Proc. 2013 IEEE International Conference on Data Mining Workshops, IEEE Workshop on Privacy Aspects of Data Mining (PADM), 2013. Section C. does a nice comparison of approaches. Please make sure the quantitative results here constitute an apples-to-apples comparison with the GAN results. The paper is extremely well-written, for the most part. Some places needing clarification include: - Last paragraph of 3.1. “all teachers....get the same training data....” This should be rephrased to make it clear that it is not the same w.r.t. all the teachers, but w.r.t. the same teacher on the neighboring database. - 4.1: The authors state: “The number n of teachers is limited by a trade-off between the classification task’s complexity and the available data.” However, since this tradeoff is not formalized, the statement is imprecise. In particular, if the analysis is done in the i.i.d. setting, the tradeoff would also likely depend on the relation of the target hypothesis to the data distribution. - Discussion of figure 3 was rather unclear in the text and caption and should be revised for clarity. In the text section, at first the explanation seems to imply that a larger gap is better (as is also indicated in the caption). However later it is stated that the gap stays under 20%. These sentences seem contradictory, which is likely not what was intended.
The paper presents a general teacher-student approach for differentially-private learning in which the student learns to predict a noise vote among a set of teachers. The noise allows the student to be differentially private, whilst maintaining good classification accuracies on MNIST and SVHN. The paper is well-written.
I think this paper has great impact. My question is what is the "auxiliary input" in Definition 2. Could you explain this term in theoretical view and what is that in your paper?
Thank you for providing an interesting paper. In the paper, the student model is trained by semi-supervised fashion as suggested in (Salimans et al., 2016). As far as I understand, teacher's ensembles are using for supervised learning and nonsensitive data is for unsupervised learning. So, my question is "Where is the generator ?". The aggregation of teacher network is treated as the generator in GAN framework?
Hi, I have few questions about the paper. 1- What attacker's goal did you consider in your paper? Is it recovering the training data, or checking whether a specific sample has been in the training data? 2- If the attacker's goal is to recover the training data, does the attacker want to recover the exact data or an approximation would be OK? 3- Talking about neural networks: - Do you think there is any attack method to recover an exact training data from the learning model? - Do you think there is any defense method to prevent an attacker from recovering even an approximate training data? 4- How can we quantify the strength of a learning model (specifically neural networks) without any defensive mechanism? 5- How can we quantify the strength of a learning model which has not been trained on exact training data? For example, some forms of adversarial training methods never train the model on the clean data; instead, at each epoch, the model is trained on different adversarial data derived from the real data. - How can the model "memorize" the training data, when 1) it has never seen the real data, 2) it has been trained on different data in different epochs? 6- How do you compare the performance of your method with adversarial training? Thanks.
This paper addresses the problem of achieving differential privacy in a very general scenario where a set of teachers is trained on disjoint subsets of sensitive data and the student performs prediction based on public data labeled by teachers through noisy voting. I found the approach altogether plausible and very clearly explained by the authors. Adding more discussion of the bound (and its tightness) from Theorem 1 itself would be appreciated. A simple idea of adding perturbation error to the counts, known from differentially-private literature, is nicely re-used by the authors and elegantly applied in a much broader (non-convex setting) and practical context than in a number of differentially-private and other related papers. The generality of the approach, clear improvement over predecessors, and clarity of the writing makes the method worth publishing.
This paper discusses how to guarantee privacy for training data. In the proposed approach multiple models trained with disjoint datasets are used as teachers'' model, which will train a student'' model to predict an output chosen by noisy voting among all of the teachers. The theoretical results are nice but also intuitive. Since teachers' result are provided via noisy voting, the student model may not duplicate the teacher's behavior. However, the probabilistic bound has quite a number of empirical parameters, which makes me difficult to decide whether the security is 100% guaranteed or not. The experiments on MNIST and SVHN are good. However, as the paper claims, the proposed approach may be mostly useful for sensitive data like medical histories, it will be nice to conduct one or two experiments on such applications.
Altogether a very good paper, a nice read, and interesting. The work advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough. One caveat is that although the approach is intended to be general, no theoretical guarantees are provided about the learning performance. Privacy-preserving machine learning papers often analyze both the privacy (in the worst case, DP setting) and the learning performance (often under different assumptions). Since the learning performance might depend on the choice of architecture; future experimentation is encouraged, even using the same data sets, with different architectures. If this will not be added, then please justify the choice of architecture used, and/or clarify what can be generalized about the observed learning performance. Another caveat is that the reported epsilons are not those that can be privately released; the authors note that their technique for doing so would change the resulting epsilon. However this would need to be resolved in order to have a meaningful comparison to the epsilon-delta values reported in related work. Finally, as has been acknowledged in the paper, the present approach may not work on other natural data types. Experiments on other data sets is strongly encouraged. Also, please cite the data sets used. Other comments: Discussion of certain parts of the related work are thorough. However, please add some survey/discussion of the related work on differentially-private semi-supervised learning. For example, in the context of random forests, the following paper also proposed differentially-private semi-supervised learning via a teacher-learner approach (although not denoted as “teacher-learner”). The only time the private labeled data is used is when learning the “primary ensemble.” A "secondary ensemble" is then learned only from the unlabeled (non-private) data, with pseudo-labels generated by the primary ensemble. G. Jagannathan, C. Monteleoni, and K. Pillaipakkamnatt: A Semi-Supervised Learning Approach to Differential Privacy. Proc. 2013 IEEE International Conference on Data Mining Workshops, IEEE Workshop on Privacy Aspects of Data Mining (PADM), 2013. Section C. does a nice comparison of approaches. Please make sure the quantitative results here constitute an apples-to-apples comparison with the GAN results. The paper is extremely well-written, for the most part. Some places needing clarification include: - Last paragraph of 3.1. “all teachers....get the same training data....” This should be rephrased to make it clear that it is not the same w.r.t. all the teachers, but w.r.t. the same teacher on the neighboring database. - 4.1: The authors state: “The number n of teachers is limited by a trade-off between the classification task’s complexity and the available data.” However, since this tradeoff is not formalized, the statement is imprecise. In particular, if the analysis is done in the i.i.d. setting, the tradeoff would also likely depend on the relation of the target hypothesis to the data distribution. - Discussion of figure 3 was rather unclear in the text and caption and should be revised for clarity. In the text section, at first the explanation seems to imply that a larger gap is better (as is also indicated in the caption). However later it is stated that the gap stays under 20%. These sentences seem contradictory, which is likely not what was intended.
Thanks for interesting and well-organized papers. I have a question about teacher-student model. Teachers are trained on sensitive data, and students are trained on non-sensitive data. I wonder how students work on the outputs of teachers. Sensitive and non-sensitive are different attributes, so I think there are no correlation between teachers and students. Please give me some more details. Thanks.
This paper addresses the problem of decoding barcode-like markers depicted in an image. The main insight is to train a CNN from generated data produced from a GAN. The GAN is trained using unlabeled images, and leverages a "3D model" that undergoes learnt image transformations (e.g., blur, lighting, background). The parameters for the image transformations are trained such that it confuses a GAN discriminator. A CNN is trained using images generated from the GAN and compared with hand-crafted features and from training with real images. The proposed method out-performs both baselines on decoding the barcode markers. The proposed GAN architecture could potentially be interesting. However, I won’t champion the paper as the evaluation could be improved. A critical missing baseline is a comparison against a generic GAN. Without this it’s hard to judge the benefit of the more structured GAN. Also, it would be worth seeing the result when one combines generated and real images for the final task. A couple of references that are relevant to this work (for object detection using rendered views of 3D shapes): [A] Xingchao Peng, Baochen Sun, Karim Ali, Kate Saenko, Learning Deep Object Detectors from 3D Models; ICCV, 2015. [B] Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views. Francisco Massa, Bryan C. Russell, Mathieu Aubry. CVPR 2016. The problem domain (decoding barcode markers on bees) is limited. It would be great to see this applied to another problem domain, e.g., object detection from 3D models as shown in paper reference [A], where direct comparison against prior work could be performed. I found the writing to be somewhat vague throughout. For instance, on first reading of the introduction it is not clear what exactly is the contribution of the paper. Minor comments: Fig 3 - Are these really renders from a 3D model? The images look like 2D images, perhaps spatially warped via a homography. Page 3: "chapter" => "section". In Table 2, what is the loss used for the DCNN? Fig 9 (a) - The last four images look like they have strange artifacts. Can you explain these?
This paper was fairly well received by the reviewers in terms of the underling idea but the fact that a very specialized problem was the focus of the paper held back reviewers from giving stronger ratings. The question of what sorts of baselines would be reasonable was discussed extensively as the reviewers felt that other credible baselines should be included. The authors argue certain baselines are not appropriate but they were not able to clearly sway the reviewers to a more positive rating based on their response to this issue. We recommend a workshop invitation for this paper.
We uploaded a new version of the paper based on the peer review feedback. * Clarified that an unconstrained GAN is not a suitable baseline. * Added additional references pointed out by reviewer 2. * Extracted a related work section to improve the overall clarity and also stated our contribution explicitly. * Various modifications to improve clarity.
Thank you very much for your reviews. Your feedback helped to improve the manuscript significantly, and we are preparing a revised version of the manuscript with changes outlined either below or in our responses to each reviewer. Multiple valid points of criticism were raised during the review process and have already been worked into the current version of the document. For example, we included hand-designed augmentations for comparison with the learned ones. However, in two of the three reviews there seems to be a major misunderstanding that we would like to clarify here. Since this relates to the central finding of our paper, we would like to provide a detailed response to this point. We hope that, in the light of this fact, the reviewer’s rating of our contribution’s importance and novelty will be reconsidered. > Reviewer 2: “A critical missing baseline is a comparison against a generic GAN. > Without this it’s hard to judge the benefit of the more structured GAN. Also, > it would be worth seeing the result when one combines generated and real images > for the final task.” > Reviewer 3: “ [...] the proposed method is more model driven that previous GAN > models. But does it pay off? how would a traditional GAN approach perform? [...] > The answers of the authors only partially addresses the point. The key proposal > of the submission seems parameterised modules that can be trained to match the > real data distribution. but it remains unclear why not a more generic > parameterisation can also do the job. E.g. a neural network - as done in regular > GANs. The benefit of introducing a stronger model is unclear.” The main point of critique here is that a comparison with a generic GAN (Goodfellow et al. 2014) is missing. This comment implies that both methods (GAN and RenderGAN) share the same task domain, which is incorrect. The task we address is generating _labeled_ data. We emphasize that we do not refer to the binary class label but rather to higher dimensional labels. In our example scenario, this corresponds to images of bee markers and their respective bit configuration (its ID) and rotations in 3D space. A generic GAN cannot generate labels, it learns to generate realistic images _without_ labels. Ultimately, we want to train a convnet (‘decoder network’) in a supervised setting to map an image to its respective labels. Thus, we need labeled samples and hence, a conventional GAN cannot be used as a baseline! Stated formally: the RenderGAN samples from the joint distribution p(l, x) of labels l and data x whereas a conventional GAN can only sample from the data distribution p(x). There are two alternative approaches to our RenderGAN, one being a conventional 3-dimensional rendering pipeline that can be used to generate images of bee markers with known ID and spatial orientation. Secondly, one could train the decoder network with manually labeled data. Both approaches have been implemented and tested against the RenderGAN and do not perform satisfyingly. To improve both alternatives’ performance one would need to either tune the rendering pipeline to match the details of the real world imaging process, or label more data manually. Both measures are time-consuming and do not generalize well when changing parts of the imaging process (lighting, cameras, compression, etc.) or the marker design. Our approach is to extend a generic GAN by adding several network modules, the first being the network equivalent of a simple 3D model. Secondly, we learn a number of parameterised augmentation functions. We would like to point out that this approach was _not_ chosen to improve the generative capabilities of the network but to constrain it in such a way that the image produced by the GAN is correct with respect to the labels fed into the network. In our use case, each image produced by the GAN has to preserve the given bit pattern and rotation in space provided by the 3D model for the labels to remain valid. This point was already addressed in our paper and the pre-review questions: > Paper Introduction: “[...] We constrain the augmentation of the images such that > the high-level information represented by the 3D model is preserved. The > RenderGAN framework allows us to generate images of which the labels are known > from the 3D model, and that also look strikingly real due to the GAN framework. > The training procedure of the RenderGAN framework does not require any labels. > We can generate high-quality, labeled data with a simple 3D model and a large > amount of unlabeled data.” > Our reply
This paper addresses the problem of decoding barcode-like markers depicted in an image. The main insight is to train a CNN from generated data produced from a GAN. The GAN is trained using unlabeled images, and leverages a "3D model" that undergoes learnt image transformations (e.g., blur, lighting, background). The parameters for the image transformations are trained such that it confuses a GAN discriminator. A CNN is trained using images generated from the GAN and compared with hand-crafted features and from training with real images. The proposed method out-performs both baselines on decoding the barcode markers. The proposed GAN architecture could potentially be interesting. However, I won’t champion the paper as the evaluation could be improved. A critical missing baseline is a comparison against a generic GAN. Without this it’s hard to judge the benefit of the more structured GAN. Also, it would be worth seeing the result when one combines generated and real images for the final task. A couple of references that are relevant to this work (for object detection using rendered views of 3D shapes): [A] Xingchao Peng, Baochen Sun, Karim Ali, Kate Saenko, Learning Deep Object Detectors from 3D Models; ICCV, 2015. [B] Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views. Francisco Massa, Bryan C. Russell, Mathieu Aubry. CVPR 2016. The problem domain (decoding barcode markers on bees) is limited. It would be great to see this applied to another problem domain, e.g., object detection from 3D models as shown in paper reference [A], where direct comparison against prior work could be performed. I found the writing to be somewhat vague throughout. For instance, on first reading of the introduction it is not clear what exactly is the contribution of the paper. Minor comments: Fig 3 - Are these really renders from a 3D model? The images look like 2D images, perhaps spatially warped via a homography. Page 3: "chapter" => "section". In Table 2, what is the loss used for the DCNN? Fig 9 (a) - The last four images look like they have strange artifacts. Can you explain these?
The paper proposes an approach to generating synthetic training data for deep networks, based on rendering 3D models and learning additional transformations with adversarial training. The approach is applied to generating barcode-like markers used for honeybee identification. The authors demonstrate that a classifier trained on synthetic data generated with the proposed approach outperforms both training on (limited) real data and training on data with hand-designed augmentations. The topic of the paper — using machine learning (in particular, adversarial training) for generating realistic synthetic training data — is very interesting and important. The proposed method looks reasonable, and the paper is written well. The downside is that experiments are limited to a fairly simple and not-widely-known domain of honeybee marker classification. While I am sure this is an important task by itself, in order to demonstrate general applicability of the method and to allow comparison with existing techniques, experiments on some standard and/or realistic datasets would be very helpful. Overall, I recommend acceptance, but encourage the authors to perform experiments on more datasets. I appreciate that the authors added a baseline with manually designed transformations. This strengthens the paper. As Reviewer3 points out, it would be interesting to analyze if restricting GAN to a fixed set of transformations is necessary here, and which transformations are most important. Perhaps this would provide some guidelines for designing sets of transformations for more complicated scenarios. The authors should tone down their claims such as “Our method is an improvement over previous work ...> Whereas previous work relied on real data for training using pre-trained models or mixing real and generated data, we were able to train a DCNN from scratch with generated data that performed well when tested on real data. “. This is not a fair comparison: the domain studied by authors in this work is much simpler than what was studied in these previous works, so this comparison is not appropriate.
The submission proposes an interesting way to match synthetic data to real data in a GAN type architecture. The main novelty are parametric modules that emulate different transformations and artefact that allow to match the natural appearance. several points were raised during the discussion: 1. the proposed method is more model driven that previous GAN models. But does it pay off? how would a traditional GAN approach perform? The mentioned effects like blur, lighting and background could also potentially be modelled by upsamling network that directly predicts the image. I would assume that blur and lighting can be modelled by convolutions. transformations to some extend by convolutions - or spatial transformer networks. The answers of the authors only partially addresses the point. The key proposal of the submission seems parameterised modules that can be trained to match the real data distribution. but it remains unclear why not a more generic parameterisation can also do the job. E.g. a neural network - as done in regular GANs. The benefit of introducing a stronger model is unclear. Using a render engine to generate the initial sample appearance if of limited novelty. 2. how does it compare to traditional data augmentation techniques, e.g. noise, dropout, transformations. you are linking to keras code - where data augmentation is readily available and could be tested (ImageDataGenerator) The authors reply that plenty of such augmentation was used and more details are going to be provided in the appendix. it would have been appreciated if such information was directly included in the revision - so that the procedure could be directly checked. right now - this remains a point of uncertainty. 3. How do the different stages (phis) effect performance? which are the most important ones? The authors do evaluate the effect of hand tuning the transformation stages vs. learning them. it would be great to also include results of including/excluding stages completely - and also reporting how much the initial jittering of the data helps. While there is an interesting idea of (limited) novelty to the paper, there are some concerns about evalations and comparisons as outlined above. In addition, only success on a single dataset/task is shown. Yet the task is interesting and seems challenging. Overall, this remains makes only a weak recommendation for acceptance.
We updated our paper based on the feedback from the pre-review questions. We included handmade augmentation in the evaluation. We also retrained the DCNN on the real data. Thanks for the feedback.
This paper addresses the problem of decoding barcode-like markers depicted in an image. The main insight is to train a CNN from generated data produced from a GAN. The GAN is trained using unlabeled images, and leverages a "3D model" that undergoes learnt image transformations (e.g., blur, lighting, background). The parameters for the image transformations are trained such that it confuses a GAN discriminator. A CNN is trained using images generated from the GAN and compared with hand-crafted features and from training with real images. The proposed method out-performs both baselines on decoding the barcode markers. The proposed GAN architecture could potentially be interesting. However, I won’t champion the paper as the evaluation could be improved. A critical missing baseline is a comparison against a generic GAN. Without this it’s hard to judge the benefit of the more structured GAN. Also, it would be worth seeing the result when one combines generated and real images for the final task. A couple of references that are relevant to this work (for object detection using rendered views of 3D shapes): [A] Xingchao Peng, Baochen Sun, Karim Ali, Kate Saenko, Learning Deep Object Detectors from 3D Models; ICCV, 2015. [B] Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views. Francisco Massa, Bryan C. Russell, Mathieu Aubry. CVPR 2016. The problem domain (decoding barcode markers on bees) is limited. It would be great to see this applied to another problem domain, e.g., object detection from 3D models as shown in paper reference [A], where direct comparison against prior work could be performed. I found the writing to be somewhat vague throughout. For instance, on first reading of the introduction it is not clear what exactly is the contribution of the paper. Minor comments: Fig 3 - Are these really renders from a 3D model? The images look like 2D images, perhaps spatially warped via a homography. Page 3: "chapter" => "section". In Table 2, what is the loss used for the DCNN? Fig 9 (a) - The last four images look like they have strange artifacts. Can you explain these?
This paper was fairly well received by the reviewers in terms of the underling idea but the fact that a very specialized problem was the focus of the paper held back reviewers from giving stronger ratings. The question of what sorts of baselines would be reasonable was discussed extensively as the reviewers felt that other credible baselines should be included. The authors argue certain baselines are not appropriate but they were not able to clearly sway the reviewers to a more positive rating based on their response to this issue. We recommend a workshop invitation for this paper.
We uploaded a new version of the paper based on the peer review feedback. * Clarified that an unconstrained GAN is not a suitable baseline. * Added additional references pointed out by reviewer 2. * Extracted a related work section to improve the overall clarity and also stated our contribution explicitly. * Various modifications to improve clarity.
Thank you very much for your reviews. Your feedback helped to improve the manuscript significantly, and we are preparing a revised version of the manuscript with changes outlined either below or in our responses to each reviewer. Multiple valid points of criticism were raised during the review process and have already been worked into the current version of the document. For example, we included hand-designed augmentations for comparison with the learned ones. However, in two of the three reviews there seems to be a major misunderstanding that we would like to clarify here. Since this relates to the central finding of our paper, we would like to provide a detailed response to this point. We hope that, in the light of this fact, the reviewer’s rating of our contribution’s importance and novelty will be reconsidered. > Reviewer 2: “A critical missing baseline is a comparison against a generic GAN. > Without this it’s hard to judge the benefit of the more structured GAN. Also, > it would be worth seeing the result when one combines generated and real images > for the final task.” > Reviewer 3: “ [...] the proposed method is more model driven that previous GAN > models. But does it pay off? how would a traditional GAN approach perform? [...] > The answers of the authors only partially addresses the point. The key proposal > of the submission seems parameterised modules that can be trained to match the > real data distribution. but it remains unclear why not a more generic > parameterisation can also do the job. E.g. a neural network - as done in regular > GANs. The benefit of introducing a stronger model is unclear.” The main point of critique here is that a comparison with a generic GAN (Goodfellow et al. 2014) is missing. This comment implies that both methods (GAN and RenderGAN) share the same task domain, which is incorrect. The task we address is generating _labeled_ data. We emphasize that we do not refer to the binary class label but rather to higher dimensional labels. In our example scenario, this corresponds to images of bee markers and their respective bit configuration (its ID) and rotations in 3D space. A generic GAN cannot generate labels, it learns to generate realistic images _without_ labels. Ultimately, we want to train a convnet (‘decoder network’) in a supervised setting to map an image to its respective labels. Thus, we need labeled samples and hence, a conventional GAN cannot be used as a baseline! Stated formally: the RenderGAN samples from the joint distribution p(l, x) of labels l and data x whereas a conventional GAN can only sample from the data distribution p(x). There are two alternative approaches to our RenderGAN, one being a conventional 3-dimensional rendering pipeline that can be used to generate images of bee markers with known ID and spatial orientation. Secondly, one could train the decoder network with manually labeled data. Both approaches have been implemented and tested against the RenderGAN and do not perform satisfyingly. To improve both alternatives’ performance one would need to either tune the rendering pipeline to match the details of the real world imaging process, or label more data manually. Both measures are time-consuming and do not generalize well when changing parts of the imaging process (lighting, cameras, compression, etc.) or the marker design. Our approach is to extend a generic GAN by adding several network modules, the first being the network equivalent of a simple 3D model. Secondly, we learn a number of parameterised augmentation functions. We would like to point out that this approach was _not_ chosen to improve the generative capabilities of the network but to constrain it in such a way that the image produced by the GAN is correct with respect to the labels fed into the network. In our use case, each image produced by the GAN has to preserve the given bit pattern and rotation in space provided by the 3D model for the labels to remain valid. This point was already addressed in our paper and the pre-review questions: > Paper Introduction: “[...] We constrain the augmentation of the images such that > the high-level information represented by the 3D model is preserved. The > RenderGAN framework allows us to generate images of which the labels are known > from the 3D model, and that also look strikingly real due to the GAN framework. > The training procedure of the RenderGAN framework does not require any labels. > We can generate high-quality, labeled data with a simple 3D model and a large > amount of unlabeled data.” > Our reply
This paper addresses the problem of decoding barcode-like markers depicted in an image. The main insight is to train a CNN from generated data produced from a GAN. The GAN is trained using unlabeled images, and leverages a "3D model" that undergoes learnt image transformations (e.g., blur, lighting, background). The parameters for the image transformations are trained such that it confuses a GAN discriminator. A CNN is trained using images generated from the GAN and compared with hand-crafted features and from training with real images. The proposed method out-performs both baselines on decoding the barcode markers. The proposed GAN architecture could potentially be interesting. However, I won’t champion the paper as the evaluation could be improved. A critical missing baseline is a comparison against a generic GAN. Without this it’s hard to judge the benefit of the more structured GAN. Also, it would be worth seeing the result when one combines generated and real images for the final task. A couple of references that are relevant to this work (for object detection using rendered views of 3D shapes): [A] Xingchao Peng, Baochen Sun, Karim Ali, Kate Saenko, Learning Deep Object Detectors from 3D Models; ICCV, 2015. [B] Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views. Francisco Massa, Bryan C. Russell, Mathieu Aubry. CVPR 2016. The problem domain (decoding barcode markers on bees) is limited. It would be great to see this applied to another problem domain, e.g., object detection from 3D models as shown in paper reference [A], where direct comparison against prior work could be performed. I found the writing to be somewhat vague throughout. For instance, on first reading of the introduction it is not clear what exactly is the contribution of the paper. Minor comments: Fig 3 - Are these really renders from a 3D model? The images look like 2D images, perhaps spatially warped via a homography. Page 3: "chapter" => "section". In Table 2, what is the loss used for the DCNN? Fig 9 (a) - The last four images look like they have strange artifacts. Can you explain these?
The paper proposes an approach to generating synthetic training data for deep networks, based on rendering 3D models and learning additional transformations with adversarial training. The approach is applied to generating barcode-like markers used for honeybee identification. The authors demonstrate that a classifier trained on synthetic data generated with the proposed approach outperforms both training on (limited) real data and training on data with hand-designed augmentations. The topic of the paper — using machine learning (in particular, adversarial training) for generating realistic synthetic training data — is very interesting and important. The proposed method looks reasonable, and the paper is written well. The downside is that experiments are limited to a fairly simple and not-widely-known domain of honeybee marker classification. While I am sure this is an important task by itself, in order to demonstrate general applicability of the method and to allow comparison with existing techniques, experiments on some standard and/or realistic datasets would be very helpful. Overall, I recommend acceptance, but encourage the authors to perform experiments on more datasets. I appreciate that the authors added a baseline with manually designed transformations. This strengthens the paper. As Reviewer3 points out, it would be interesting to analyze if restricting GAN to a fixed set of transformations is necessary here, and which transformations are most important. Perhaps this would provide some guidelines for designing sets of transformations for more complicated scenarios. The authors should tone down their claims such as “Our method is an improvement over previous work ...> Whereas previous work relied on real data for training using pre-trained models or mixing real and generated data, we were able to train a DCNN from scratch with generated data that performed well when tested on real data. “. This is not a fair comparison: the domain studied by authors in this work is much simpler than what was studied in these previous works, so this comparison is not appropriate.
The submission proposes an interesting way to match synthetic data to real data in a GAN type architecture. The main novelty are parametric modules that emulate different transformations and artefact that allow to match the natural appearance. several points were raised during the discussion: 1. the proposed method is more model driven that previous GAN models. But does it pay off? how would a traditional GAN approach perform? The mentioned effects like blur, lighting and background could also potentially be modelled by upsamling network that directly predicts the image. I would assume that blur and lighting can be modelled by convolutions. transformations to some extend by convolutions - or spatial transformer networks. The answers of the authors only partially addresses the point. The key proposal of the submission seems parameterised modules that can be trained to match the real data distribution. but it remains unclear why not a more generic parameterisation can also do the job. E.g. a neural network - as done in regular GANs. The benefit of introducing a stronger model is unclear. Using a render engine to generate the initial sample appearance if of limited novelty. 2. how does it compare to traditional data augmentation techniques, e.g. noise, dropout, transformations. you are linking to keras code - where data augmentation is readily available and could be tested (ImageDataGenerator) The authors reply that plenty of such augmentation was used and more details are going to be provided in the appendix. it would have been appreciated if such information was directly included in the revision - so that the procedure could be directly checked. right now - this remains a point of uncertainty. 3. How do the different stages (phis) effect performance? which are the most important ones? The authors do evaluate the effect of hand tuning the transformation stages vs. learning them. it would be great to also include results of including/excluding stages completely - and also reporting how much the initial jittering of the data helps. While there is an interesting idea of (limited) novelty to the paper, there are some concerns about evalations and comparisons as outlined above. In addition, only success on a single dataset/task is shown. Yet the task is interesting and seems challenging. Overall, this remains makes only a weak recommendation for acceptance.
We updated our paper based on the feedback from the pre-review questions. We included handmade augmentation in the evaluation. We also retrained the DCNN on the real data. Thanks for the feedback.
This paper comes up with a novel approach to searching the space of architectures for deep neural networks using reinforcement learning. The idea is straightforward and sensible: use a reinforcement learning strategy to iteratively grow a deep net graph (the space of actions is e.g. adding different layer types) via Q-learning. The reviewers agree that the idea is interesting, novel and promising but are underwhelmed with the execution of the experiments and the empirical results. The idea behind the paper and the formulation of the problem are quite similar to a concurrent submission (
We have added the results of the stability experiment (as suggested by AnonReviewer1) into the Appendix section D.1. We have also included a citation to the CNF paper and results from the latest ResNet paper.
This paper introduces a reinforcement learning framework for designing a neural network architecture. For each time-step, the agent picks a new layer type with corresponding layer parameters (e.g., #filters). In order to reduce the size of state-action space, they used a small set of design choices. Strengths: - A novel approach for automatic design of neural network architectures. - Shows quite promising results on several datasets (MNIST, CIFAR-10). Weakness: - Limited architecture design choices due to many prior assumptions (e.g., a set of possible number of convolution filters, at most 2 fully-connected layers, maximum depth, hard-coded dropout, etc.) - The method is demonstrated in tabular Q-learning setting, but it is unclear whether the proposed method would work in a large state-action space. Overall, this is an interesting and novel approach for neural network architecture design, and it seems to be worth publication despite some weaknesses.
The paper looks solid and the idea is natural. Results seem promising as well. I am mostly concerned about the computational cost of the method. 8-10 days on 10 GPUs for relatively tiny datasets is quite prohibitive for most applications I would ever encounter. I think the main question is how this approach scales to larger images and also when applied to more exotic and possibly tiny datasets. Can you run an experiment on Caltech-101 for instance? I would be very curious to see if your approach is suitable for the low-data regime and areas where we all do not know right away how a suitable architecture looks like. For Cifar-10/100, MNIST and SVHN, everyone knows very well what a reasonable model initialization looks like. If you show proof that you can discover a competitive architecture for something like Caltech-101, I would recommend the paper for publication. Minor: - ResNets should be mentioned in Table
Authors learn deep architectures on a few small vision problems using Q-learning and obtain solid results, SOTA results when limiting to certain types of layers and competitive against everything else. It would be good to know how well this performs when allowing more complex structures. Paper would be much more convincing on a real-size task such as ImageNet.
This paper comes up with a novel approach to searching the space of architectures for deep neural networks using reinforcement learning. The idea is straightforward and sensible: use a reinforcement learning strategy to iteratively grow a deep net graph (the space of actions is e.g. adding different layer types) via Q-learning. The reviewers agree that the idea is interesting, novel and promising but are underwhelmed with the execution of the experiments and the empirical results. The idea behind the paper and the formulation of the problem are quite similar to a concurrent submission (
We have added the results of the stability experiment (as suggested by AnonReviewer1) into the Appendix section D.1. We have also included a citation to the CNF paper and results from the latest ResNet paper.
This paper introduces a reinforcement learning framework for designing a neural network architecture. For each time-step, the agent picks a new layer type with corresponding layer parameters (e.g., #filters). In order to reduce the size of state-action space, they used a small set of design choices. Strengths: - A novel approach for automatic design of neural network architectures. - Shows quite promising results on several datasets (MNIST, CIFAR-10). Weakness: - Limited architecture design choices due to many prior assumptions (e.g., a set of possible number of convolution filters, at most 2 fully-connected layers, maximum depth, hard-coded dropout, etc.) - The method is demonstrated in tabular Q-learning setting, but it is unclear whether the proposed method would work in a large state-action space. Overall, this is an interesting and novel approach for neural network architecture design, and it seems to be worth publication despite some weaknesses.
The paper looks solid and the idea is natural. Results seem promising as well. I am mostly concerned about the computational cost of the method. 8-10 days on 10 GPUs for relatively tiny datasets is quite prohibitive for most applications I would ever encounter. I think the main question is how this approach scales to larger images and also when applied to more exotic and possibly tiny datasets. Can you run an experiment on Caltech-101 for instance? I would be very curious to see if your approach is suitable for the low-data regime and areas where we all do not know right away how a suitable architecture looks like. For Cifar-10/100, MNIST and SVHN, everyone knows very well what a reasonable model initialization looks like. If you show proof that you can discover a competitive architecture for something like Caltech-101, I would recommend the paper for publication. Minor: - ResNets should be mentioned in Table
Authors learn deep architectures on a few small vision problems using Q-learning and obtain solid results, SOTA results when limiting to certain types of layers and competitive against everything else. It would be good to know how well this performs when allowing more complex structures. Paper would be much more convincing on a real-size task such as ImageNet.
The authors proposed to use leaky rectified linear units replacing binary units in Gaussian RBM. A sampling method was presented to train the leaky-ReLU RBM. In the experimental section, AIS estimated likelihood on Cifar10 and SVHN were reported. It's interesting for trying different nonlinear hidden units for RBM. However, there are some concerns for the current work. 1. The author did not explain why the proposed sampling method (Alg. 2) is correct. And the additional computation cost (the inner loop and the projection) should be discussed. 2. The results (both the resulting likelihood and the generative samples) of Gaussian RBM are much worse than what we have experienced. It seems that the Gaussian RBM were not trained properly. 3. The representation learned from a good generative model often helps the classification task when there are fewer label samples. Gaussian RBM works well for texture synthesis tasks in which mixing is an important issue. The authors are encouraged to do more experiments in these two direction.
This paper identifies a joint distribution for an RBM variant based on leaky-ReLU activations. It also proposes using a sequence of distributions, both as an annealing-based training method, or to estimate log(Z) with AIS. This paper was borderline. While there is an interesting idea, the reviewers weren't generally as excited by the work as for other papers. One limitation is that unit-variance Gaussian RBMs aren't a strong baseline for comparison, although that is the focus of the main body of the paper. An update to the paper has results for binary visibles in an appendix, although I'm not sure exactly what was done, if the results are comparable, or if there is a large cost of projection here.
We thank all reviewers for their careful reading of the paper and constructive feedback. We have extensively revised the paper to address these requests for clarifications and experiments. Here, we explain some of these and point to related updates in our revision. Please check the individual replies below the reviews.
The authors proposed to use leaky rectified linear units replacing binary units in Gaussian RBM. A sampling method was presented to train the leaky-ReLU RBM. In the experimental section, AIS estimated likelihood on Cifar10 and SVHN were reported. It's interesting for trying different nonlinear hidden units for RBM. However, there are some concerns for the current work. 1. The author did not explain why the proposed sampling method (Alg. 2) is correct. And the additional computation cost (the inner loop and the projection) should be discussed. 2. The results (both the resulting likelihood and the generative samples) of Gaussian RBM are much worse than what we have experienced. It seems that the Gaussian RBM were not trained properly. 3. The representation learned from a good generative model often helps the classification task when there are fewer label samples. Gaussian RBM works well for texture synthesis tasks in which mixing is an important issue. The authors are encouraged to do more experiments in these two direction.
Based on previous work such as the stepped sigmoid units and ReLU hidden units for discriminatively trained supervised models, a Leaky-ReLU model is proposed for generative learning. Pro: what is interesting is that unlike the traditional way of first defining an energy function and then deriving the conditional distributions, this paper propose the forms of the conditional first and then derive the energy function. However this general formulation is not novel to this paper, but was generalized to exponential family GLMs earlier. Con: Because of the focus on specifying the conditionals, the joint pdf and the marginal p(v) becomes complicated and hard to compute. On the experiments, it would been nice to see a RBM with binary visbles and leaky ReLu for hiddens. This would demonstrate the superiority of the leaky ReLU hidden units. In addition, there are more results on binary MNIST modeling with which the authors can compare the results to. While the authors is correct that the annealing distribution is no longer Gaussian, perhaps CD-25 or (Faast) PCD experiments can be run to compare agains the baseline RBM trained using (Fast) PCD. This paper is interesting as it combines new hidden function with the easiness of annealed AIS sampling, However, the baseline comparisons to Stepped Sigmoid Units (Nair &Hinton) or other models like the spike-and-slab RBMs (and others) are missing, without those comparisons, it is hard to tell whether leaky ReLU RBMs are better even in continuous visible domain.
The authors propose a novel energy-function for RBMs, using the leaky relu max(cx, x) activation function for the hidden-units. Analogous to ReLU units in feed-forward networks, these leaky relu RBMs split the input space into a combinatorial number of regions, where each region defines p(v) as a truncated Gaussian. A further contribution of the paper is in proposing a novel sampling scheme for the leaky RBM: one can run a much shorter Markov chain by initializing it from a sample of the leaky RBM with c=1 (which yields a standard multi-variate normal over the visibles) and then slowly annealing c. In low-dimension a similar scheme is shown to outperform AIS for estimating the partition function. Experiments are performed on both CIFAR-10 and SVHN. This is an interesting paper which I believe would be of interest to the ICLR community. The theoretical contributions are strong: the authors not only introduce a proper energy formulation of ReLU RBMs, but also a novel sampling mechanism and an improvement on AIS for estimating their partition function. Unfortunately, the experimental results are somewhat limited. The PCD baseline is notably absent. Including (bernoulli visible, leaky-relu hidden) would have allowed the authors to evaluate likelihoods on standard binary RBM datasets. As it stands, performance on CIFAR-10 and SVHN, while improved with leaky-relu, is a far cry from more recent generative models (VAE-based, or auto-regressive models). While this comparison may be unfair, it will certainly limit the wider appeal of the paper to the community. Furthermore, there is the issue of the costly projection method which is required to guarantee that the energy-function remain bounded (covariance matrix over each region be PSD). Again, while it may be fair to leave that for future work given the other contributions, this will further limit the appeal of the paper. PROS: Introduces an energy function having the leaky-relu as an activation function Introduces a novel sampling procedure based on annealing the leakiness parameter Similar sampling scheme shown to outperform AIS CONS: Results are somewhat out of date Missing experiments on binary datasets (more comparable to prior RBM work) Missing PCD baseline Cost of projection method
This paper proposed a new variant of RBM, which has a nonlinearity of leaky ReLU, in contrast to the sigmoid function nonlinearity in RBM. By gradually annealing the leakiness coefficient (corresponding to from Gaussian to non-Gaussian model), the authors can sample from their model with a higher mixing rate. With the same idea annealing leakiness, they show they can estimate the partition function of the new model more accurately. Main comments: The proposed model can only account for real-valued data. However, RBM is primarily used to model binary data, real-valued RBM (Gaussian-RBM) is not a well-recognized model for real-valued data. So, to demonstrate the superiority of the model, the author should also include the comparison with binary data. And it is also not enough to only compare two datasets for a newly proposed model. The claim that the marginal distribution of visible variables is truncated Gaussian is incorrect. For a truncated normal, the values of variables are constrained to be within some region, e.g. requiring variable v from the region a1
The authors proposed to use leaky rectified linear units replacing binary units in Gaussian RBM. A sampling method was presented to train the leaky-ReLU RBM. In the experimental section, AIS estimated likelihood on Cifar10 and SVHN were reported. It's interesting for trying different nonlinear hidden units for RBM. However, there are some concerns for the current work. 1. The author did not explain why the proposed sampling method (Alg. 2) is correct. And the additional computation cost (the inner loop and the projection) should be discussed. 2. The results (both the resulting likelihood and the generative samples) of Gaussian RBM are much worse than what we have experienced. It seems that the Gaussian RBM were not trained properly. 3. The representation learned from a good generative model often helps the classification task when there are fewer label samples. Gaussian RBM works well for texture synthesis tasks in which mixing is an important issue. The authors are encouraged to do more experiments in these two direction.
This paper identifies a joint distribution for an RBM variant based on leaky-ReLU activations. It also proposes using a sequence of distributions, both as an annealing-based training method, or to estimate log(Z) with AIS. This paper was borderline. While there is an interesting idea, the reviewers weren't generally as excited by the work as for other papers. One limitation is that unit-variance Gaussian RBMs aren't a strong baseline for comparison, although that is the focus of the main body of the paper. An update to the paper has results for binary visibles in an appendix, although I'm not sure exactly what was done, if the results are comparable, or if there is a large cost of projection here.
We thank all reviewers for their careful reading of the paper and constructive feedback. We have extensively revised the paper to address these requests for clarifications and experiments. Here, we explain some of these and point to related updates in our revision. Please check the individual replies below the reviews.
The authors proposed to use leaky rectified linear units replacing binary units in Gaussian RBM. A sampling method was presented to train the leaky-ReLU RBM. In the experimental section, AIS estimated likelihood on Cifar10 and SVHN were reported. It's interesting for trying different nonlinear hidden units for RBM. However, there are some concerns for the current work. 1. The author did not explain why the proposed sampling method (Alg. 2) is correct. And the additional computation cost (the inner loop and the projection) should be discussed. 2. The results (both the resulting likelihood and the generative samples) of Gaussian RBM are much worse than what we have experienced. It seems that the Gaussian RBM were not trained properly. 3. The representation learned from a good generative model often helps the classification task when there are fewer label samples. Gaussian RBM works well for texture synthesis tasks in which mixing is an important issue. The authors are encouraged to do more experiments in these two direction.
Based on previous work such as the stepped sigmoid units and ReLU hidden units for discriminatively trained supervised models, a Leaky-ReLU model is proposed for generative learning. Pro: what is interesting is that unlike the traditional way of first defining an energy function and then deriving the conditional distributions, this paper propose the forms of the conditional first and then derive the energy function. However this general formulation is not novel to this paper, but was generalized to exponential family GLMs earlier. Con: Because of the focus on specifying the conditionals, the joint pdf and the marginal p(v) becomes complicated and hard to compute. On the experiments, it would been nice to see a RBM with binary visbles and leaky ReLu for hiddens. This would demonstrate the superiority of the leaky ReLU hidden units. In addition, there are more results on binary MNIST modeling with which the authors can compare the results to. While the authors is correct that the annealing distribution is no longer Gaussian, perhaps CD-25 or (Faast) PCD experiments can be run to compare agains the baseline RBM trained using (Fast) PCD. This paper is interesting as it combines new hidden function with the easiness of annealed AIS sampling, However, the baseline comparisons to Stepped Sigmoid Units (Nair &Hinton) or other models like the spike-and-slab RBMs (and others) are missing, without those comparisons, it is hard to tell whether leaky ReLU RBMs are better even in continuous visible domain.
The authors propose a novel energy-function for RBMs, using the leaky relu max(cx, x) activation function for the hidden-units. Analogous to ReLU units in feed-forward networks, these leaky relu RBMs split the input space into a combinatorial number of regions, where each region defines p(v) as a truncated Gaussian. A further contribution of the paper is in proposing a novel sampling scheme for the leaky RBM: one can run a much shorter Markov chain by initializing it from a sample of the leaky RBM with c=1 (which yields a standard multi-variate normal over the visibles) and then slowly annealing c. In low-dimension a similar scheme is shown to outperform AIS for estimating the partition function. Experiments are performed on both CIFAR-10 and SVHN. This is an interesting paper which I believe would be of interest to the ICLR community. The theoretical contributions are strong: the authors not only introduce a proper energy formulation of ReLU RBMs, but also a novel sampling mechanism and an improvement on AIS for estimating their partition function. Unfortunately, the experimental results are somewhat limited. The PCD baseline is notably absent. Including (bernoulli visible, leaky-relu hidden) would have allowed the authors to evaluate likelihoods on standard binary RBM datasets. As it stands, performance on CIFAR-10 and SVHN, while improved with leaky-relu, is a far cry from more recent generative models (VAE-based, or auto-regressive models). While this comparison may be unfair, it will certainly limit the wider appeal of the paper to the community. Furthermore, there is the issue of the costly projection method which is required to guarantee that the energy-function remain bounded (covariance matrix over each region be PSD). Again, while it may be fair to leave that for future work given the other contributions, this will further limit the appeal of the paper. PROS: Introduces an energy function having the leaky-relu as an activation function Introduces a novel sampling procedure based on annealing the leakiness parameter Similar sampling scheme shown to outperform AIS CONS: Results are somewhat out of date Missing experiments on binary datasets (more comparable to prior RBM work) Missing PCD baseline Cost of projection method
This paper proposed a new variant of RBM, which has a nonlinearity of leaky ReLU, in contrast to the sigmoid function nonlinearity in RBM. By gradually annealing the leakiness coefficient (corresponding to from Gaussian to non-Gaussian model), the authors can sample from their model with a higher mixing rate. With the same idea annealing leakiness, they show they can estimate the partition function of the new model more accurately. Main comments: The proposed model can only account for real-valued data. However, RBM is primarily used to model binary data, real-valued RBM (Gaussian-RBM) is not a well-recognized model for real-valued data. So, to demonstrate the superiority of the model, the author should also include the comparison with binary data. And it is also not enough to only compare two datasets for a newly proposed model. The claim that the marginal distribution of visible variables is truncated Gaussian is incorrect. For a truncated normal, the values of variables are constrained to be within some region, e.g. requiring variable v from the region a1
This paper presents a novel way to do unsupervised pretraining in a deep convolutional network setting (though likely applicable to fully-connected nets as well). The method is that of ‘spatial constrasting’, i.e. of building triplets from patches of input images and learning a presentation that assigns a high score for patches coming from the same image and a low score for patches from diferent images. The method is simple enough that I am surprised that no-one has tried this before (at least according to the previous work in the submission). Here are some comments: The usage of P(f_i1 | f_i2) in Section 4.1 is a bit odd. May be worth defining mathematically what kind of probability the authors are talking about, or just taking that part out (“probability” can be replaced with another word). I would like to know more about how the method is using the “batch statistics” (end of Section 4.2) by sampling from it, unless the authors simply mean that the just sample from all the possible triples in their batch. Shouldn’t the number of patches sampled in Algorithm 1 be a hyper-parameter rather than just be 1? Have the authors tried any other value? I think there are some missing details in the paper, like the patch size or whether the authors have played with it at all (I think this is an important hyper-parameter). The STL results are quite impressive, but CIFAR-10 maybe not so much. For CIFAR I’d expect that one can try to pre-train on, say, Imagenet + CIFAR to build a better representation. Have the authors considered this? All in all, this is an interesting piece of work with some obvious applications, and it seems relatively straightforward to implemenent and try. I think I would’ve liked more understanding of what the spatial contrasting actually learns, more empirical studies on the effects of various parameter choices (e.g., patch size) and more attempts at beating the state of the art (e.g. CIFAR).
The paper proposes a formulation for unsupervised learning of ConvNets based on the distance between patches sampled from the same and different images. The novelty of the method is rather limited as it's similar to [Doersch et al. 2015] and [Dosovitsky et al. 2015]. The evaluation is only performed on the small datasets, which limits the potential impact of the contribution.
This paper proposes an unsupervised training objective based on patch contrasting for visual representation learning using deep neural networks. In particular, the feature representations of the patches from the same image are encouraged to be closer than the those from different images. The distance ratios of positive training pairs are optimized. The proposed method are empirically shown to be effective as an initialization method for supervised training. Strengths: - The training objective is reasonable. In particular, high-level features show translation invariance. - The proposed methods are effective for initializing neural networks for supervised training on several datasets. Weaknesses: - The methods are technically similar to the “exemplar network” (Dosovitskiy 2015). Cropping patches from a single image can be taken as a type of data augmentation, which is comparable to the data augmentation of positive sample (the exemplar) in (Dosovitskiy 2015). - The paper is experimentally misleading. The results reported in this paper are based on fine-tuning the whole network with supervision. However, in Table 2, the results of exemplar convnets (Dosovitskiy 2015) is from unsupervised feature learning (the network is not finetuned with labeled samples, and only a classifier is trained upon the features). Therefore, the comparison is not fair. I suspect that exemplar convnets (Dosovitskiy 2015) would achieve similar improvements from fine-tuning; so, without such comparisons (head-to-head comparison with and without fine-tuning based on the same architecture except for the loss), the experimental results are not fully convincing. Regarding the comparison to “What-where” autoencoder (Zhao et al, 2015), it will be interesting to compare against it in large-scale settings, as shown by Zhang et al, ICML 2016 (Augmenting Supervised Neural Networks with Unsupervised Objectives for Large-Scale Image Classification). Training an AlexNet is not very time-consuming with latest (e.g., TITAN-X level) GPUs. The proposed method seems useful only for natural images where different patches from the same image can be similar to each other.
The proposed self supervised loss is formulated using a Siamese architecture and encourages patches from the same image to lie closer in feature space than a contrasting patch taken from a different, random image. The loss is very similar in spirit to that of Doersch et al. ICCV 2015 and Isola et al. ICLR 2016 workshop. It seems that the proposed loss is actually a simplified version of Doersch et al. ICCV 2015 in that it does not make use of the spatial offset, a freely available self supervised signal in natural images. Intuitively, it seems that the self-supervised problem posed by this method is strictly simpler, and therefore less powerful, than that of the aforementioned work. I would like to see more discussion on the comparison of these two approaches. Nevertheless the proposed method seems to be effective in achieving good empirical results using this simple loss. Though more implementation details should be provided, such as the effect of patch size, overlap between sampled patches, and any other important measures taken to avoid trivial solutions.
This paper presents a novel way to do unsupervised pretraining in a deep convolutional network setting (though likely applicable to fully-connected nets as well). The method is that of ‘spatial constrasting’, i.e. of building triplets from patches of input images and learning a presentation that assigns a high score for patches coming from the same image and a low score for patches from diferent images. The method is simple enough that I am surprised that no-one has tried this before (at least according to the previous work in the submission). Here are some comments: The usage of P(f_i1 | f_i2) in Section 4.1 is a bit odd. May be worth defining mathematically what kind of probability the authors are talking about, or just taking that part out (“probability” can be replaced with another word). I would like to know more about how the method is using the “batch statistics” (end of Section 4.2) by sampling from it, unless the authors simply mean that the just sample from all the possible triples in their batch. Shouldn’t the number of patches sampled in Algorithm 1 be a hyper-parameter rather than just be 1? Have the authors tried any other value? I think there are some missing details in the paper, like the patch size or whether the authors have played with it at all (I think this is an important hyper-parameter). The STL results are quite impressive, but CIFAR-10 maybe not so much. For CIFAR I’d expect that one can try to pre-train on, say, Imagenet + CIFAR to build a better representation. Have the authors considered this? All in all, this is an interesting piece of work with some obvious applications, and it seems relatively straightforward to implemenent and try. I think I would’ve liked more understanding of what the spatial contrasting actually learns, more empirical studies on the effects of various parameter choices (e.g., patch size) and more attempts at beating the state of the art (e.g. CIFAR).
This paper presents a novel way to do unsupervised pretraining in a deep convolutional network setting (though likely applicable to fully-connected nets as well). The method is that of ‘spatial constrasting’, i.e. of building triplets from patches of input images and learning a presentation that assigns a high score for patches coming from the same image and a low score for patches from diferent images. The method is simple enough that I am surprised that no-one has tried this before (at least according to the previous work in the submission). Here are some comments: The usage of P(f_i1 | f_i2) in Section 4.1 is a bit odd. May be worth defining mathematically what kind of probability the authors are talking about, or just taking that part out (“probability” can be replaced with another word). I would like to know more about how the method is using the “batch statistics” (end of Section 4.2) by sampling from it, unless the authors simply mean that the just sample from all the possible triples in their batch. Shouldn’t the number of patches sampled in Algorithm 1 be a hyper-parameter rather than just be 1? Have the authors tried any other value? I think there are some missing details in the paper, like the patch size or whether the authors have played with it at all (I think this is an important hyper-parameter). The STL results are quite impressive, but CIFAR-10 maybe not so much. For CIFAR I’d expect that one can try to pre-train on, say, Imagenet + CIFAR to build a better representation. Have the authors considered this? All in all, this is an interesting piece of work with some obvious applications, and it seems relatively straightforward to implemenent and try. I think I would’ve liked more understanding of what the spatial contrasting actually learns, more empirical studies on the effects of various parameter choices (e.g., patch size) and more attempts at beating the state of the art (e.g. CIFAR).
The paper proposes a formulation for unsupervised learning of ConvNets based on the distance between patches sampled from the same and different images. The novelty of the method is rather limited as it's similar to [Doersch et al. 2015] and [Dosovitsky et al. 2015]. The evaluation is only performed on the small datasets, which limits the potential impact of the contribution.
This paper proposes an unsupervised training objective based on patch contrasting for visual representation learning using deep neural networks. In particular, the feature representations of the patches from the same image are encouraged to be closer than the those from different images. The distance ratios of positive training pairs are optimized. The proposed method are empirically shown to be effective as an initialization method for supervised training. Strengths: - The training objective is reasonable. In particular, high-level features show translation invariance. - The proposed methods are effective for initializing neural networks for supervised training on several datasets. Weaknesses: - The methods are technically similar to the “exemplar network” (Dosovitskiy 2015). Cropping patches from a single image can be taken as a type of data augmentation, which is comparable to the data augmentation of positive sample (the exemplar) in (Dosovitskiy 2015). - The paper is experimentally misleading. The results reported in this paper are based on fine-tuning the whole network with supervision. However, in Table 2, the results of exemplar convnets (Dosovitskiy 2015) is from unsupervised feature learning (the network is not finetuned with labeled samples, and only a classifier is trained upon the features). Therefore, the comparison is not fair. I suspect that exemplar convnets (Dosovitskiy 2015) would achieve similar improvements from fine-tuning; so, without such comparisons (head-to-head comparison with and without fine-tuning based on the same architecture except for the loss), the experimental results are not fully convincing. Regarding the comparison to “What-where” autoencoder (Zhao et al, 2015), it will be interesting to compare against it in large-scale settings, as shown by Zhang et al, ICML 2016 (Augmenting Supervised Neural Networks with Unsupervised Objectives for Large-Scale Image Classification). Training an AlexNet is not very time-consuming with latest (e.g., TITAN-X level) GPUs. The proposed method seems useful only for natural images where different patches from the same image can be similar to each other.
The proposed self supervised loss is formulated using a Siamese architecture and encourages patches from the same image to lie closer in feature space than a contrasting patch taken from a different, random image. The loss is very similar in spirit to that of Doersch et al. ICCV 2015 and Isola et al. ICLR 2016 workshop. It seems that the proposed loss is actually a simplified version of Doersch et al. ICCV 2015 in that it does not make use of the spatial offset, a freely available self supervised signal in natural images. Intuitively, it seems that the self-supervised problem posed by this method is strictly simpler, and therefore less powerful, than that of the aforementioned work. I would like to see more discussion on the comparison of these two approaches. Nevertheless the proposed method seems to be effective in achieving good empirical results using this simple loss. Though more implementation details should be provided, such as the effect of patch size, overlap between sampled patches, and any other important measures taken to avoid trivial solutions.
This paper presents a novel way to do unsupervised pretraining in a deep convolutional network setting (though likely applicable to fully-connected nets as well). The method is that of ‘spatial constrasting’, i.e. of building triplets from patches of input images and learning a presentation that assigns a high score for patches coming from the same image and a low score for patches from diferent images. The method is simple enough that I am surprised that no-one has tried this before (at least according to the previous work in the submission). Here are some comments: The usage of P(f_i1 | f_i2) in Section 4.1 is a bit odd. May be worth defining mathematically what kind of probability the authors are talking about, or just taking that part out (“probability” can be replaced with another word). I would like to know more about how the method is using the “batch statistics” (end of Section 4.2) by sampling from it, unless the authors simply mean that the just sample from all the possible triples in their batch. Shouldn’t the number of patches sampled in Algorithm 1 be a hyper-parameter rather than just be 1? Have the authors tried any other value? I think there are some missing details in the paper, like the patch size or whether the authors have played with it at all (I think this is an important hyper-parameter). The STL results are quite impressive, but CIFAR-10 maybe not so much. For CIFAR I’d expect that one can try to pre-train on, say, Imagenet + CIFAR to build a better representation. Have the authors considered this? All in all, this is an interesting piece of work with some obvious applications, and it seems relatively straightforward to implemenent and try. I think I would’ve liked more understanding of what the spatial contrasting actually learns, more empirical studies on the effects of various parameter choices (e.g., patch size) and more attempts at beating the state of the art (e.g. CIFAR).
This paper explores the ability of nonlinear recurrent neural networks to account for neural response properties that have otherwise eluded the ability of other models. A multilayer rnn is trained to imitate the stimulus-response mapping measured from actual retinal ganglion cells in response to a sequence of natural images. The rnn performs significantly better, especially in accounting for transient responses, than conventional LN/GLM models. This work is an important step in understanding the nonlinear response properties of visual neurons. Recent results have shown that the responses of even retinal ganglion cells in response to natural movies are difficult to explain in terms of standard receptive field models. So this presents an important challenge to the field. If we even had *a* model that works, it would be a starting point. So this work should be seen in that light. The challenge now of course is to tease apart what the rnn is doing. Perhaps it could now be pruned and simplified to see what parts are critical to performance. It would have been nice to see such an analysis. Nevertheless this result is a good first start and I think important for people to know about. I am a bit confused about what is being called a "movie." My understanding is that it is essentially a sequence of unrelated images shown for 1 sec. each. But then it is stated that the "frame rate" is 1/8.33 ms. I think this must refer to the refresh rate of the monitor, right? I would guess that the deviations from the LN model are even stronger when you show actual dynamic natural scenes - i.e., real movies. Here I would expect the rnn to have an even more profound effect, and potentially be much more informative.
This work is an important step in developing the tools for understanding the nonlinear response properties of visual neurons. The methods are sound and the results are meaningful. Reviewer 3 gave a much lower score than the other two reviewers because Rev 3 does not appreciate the improvement of prediction performance as an advance in itself. For understanding of the visual algorithms in the brain, however, prediction performance is the most critical success criterion. The paper provides convincing evidence that the approach is promising and likely to facilitate further advances towards achieving this long-term goal. I am confident enough to defend acceptance of this paper for a poster.
We have posted a new revision of the manuscript in which we changed the subset of cells for which we show results to better align with a previous study. The results remain unchanged. One of our example cells in Figure 3 was no longer in the criteria-passing subset so we are showing responses from a different OFF example cell. Additionally, we added a supplementary figure showing comparisons using a normalized log-likelihood metric, posted a link to a video of the stimulus, added “Responses” to the title, and made minor cosmetic changes to the figures.
This paper explores the ability of nonlinear recurrent neural networks to account for neural response properties that have otherwise eluded the ability of other models. A multilayer rnn is trained to imitate the stimulus-response mapping measured from actual retinal ganglion cells in response to a sequence of natural images. The rnn performs significantly better, especially in accounting for transient responses, than conventional LN/GLM models. This work is an important step in understanding the nonlinear response properties of visual neurons. Recent results have shown that the responses of even retinal ganglion cells in response to natural movies are difficult to explain in terms of standard receptive field models. So this presents an important challenge to the field. If we even had *a* model that works, it would be a starting point. So this work should be seen in that light. The challenge now of course is to tease apart what the rnn is doing. Perhaps it could now be pruned and simplified to see what parts are critical to performance. It would have been nice to see such an analysis. Nevertheless this result is a good first start and I think important for people to know about. I am a bit confused about what is being called a "movie." My understanding is that it is essentially a sequence of unrelated images shown for 1 sec. each. But then it is stated that the "frame rate" is 1/8.33 ms. I think this must refer to the refresh rate of the monitor, right? I would guess that the deviations from the LN model are even stronger when you show actual dynamic natural scenes - i.e., real movies. Here I would expect the rnn to have an even more profound effect, and potentially be much more informative.
This is a clearly written paper with a nice, if straightforward, result: RNNs can be good predictive models of neuron firing rates in the retina. On the one hand, the primary scientific contribution seems to just be to confirm that this approach works. On this particular stimulus locked task the gains from using the RNN seemed relatively modest, and it hasn't yet taught us anything new about the biology. On the other hand, this (along with the concurrent work of McIntosh et al.) is introducing neural network modeling to a field that isn't currently using it, and where it should prove very effective. I think it would be very interesting to see the results of applying a framework like this one with LFP and other neurons as input and on a shorter discretization time scale. I suspect followup work building on this proof of concept will be increasingly exciting. Minor comments: Sec 3.2: I didn't understand the role of the 0.833 ms bins. Use "epoch" throughout, rather than alternating between "epoch" and "pass through data". Fig. 4 would be better with the x-axis on a log scale.
This paper fits models to spike trains of retinal ganglion cells that are driven by natural images. I think the title should thus include the word “activity” at the end for otherwise it is actually formally incorrect. Anyhow, this paper proposes more specifically a recurrent network for this time series prediction and compares it to what seems to be the previous approach of a generalized linear model. Overall the stated paradigm is that when one can predict the spikes well then one can look into the model and learn how nature does it. In general the paper sounds plausible, though I am not convinced that I learned a lot. The results in figure 2 show that the RNN model can predict the spikes a bit better. So this is nice. But now what? You have shown that a more complicated model can produce better fits to the data, though there are of course still some variations to the real data. Your initial outline was that a better predictive model helps you to better understand the neural processing in the retina. So tell us what you learned. I am not a specialist of the retina, but I know that there are several layers and recurrencies in the retina, so I am not so surprised that the new model is better than the GLM. It seems that more complicated recurrent models such as LSTM do not improve the performance according to a statement in the paper. However, comparisons on this level are also difficult as a more complex models needs more data. Hence, I would actually expect that more layers and even a more detailed model of the retina could eventually improve the prediction even further. I was also a bit puzzled that all the neurons in the network share all the same parameters (weights). While the results show that these simplified models can capture a lot of the spike train characteristics, couldn’t a model with free parameters eventually outperform this one (with correspondingly more training data)?
This paper explores the ability of nonlinear recurrent neural networks to account for neural response properties that have otherwise eluded the ability of other models. A multilayer rnn is trained to imitate the stimulus-response mapping measured from actual retinal ganglion cells in response to a sequence of natural images. The rnn performs significantly better, especially in accounting for transient responses, than conventional LN/GLM models. This work is an important step in understanding the nonlinear response properties of visual neurons. Recent results have shown that the responses of even retinal ganglion cells in response to natural movies are difficult to explain in terms of standard receptive field models. So this presents an important challenge to the field. If we even had *a* model that works, it would be a starting point. So this work should be seen in that light. The challenge now of course is to tease apart what the rnn is doing. Perhaps it could now be pruned and simplified to see what parts are critical to performance. It would have been nice to see such an analysis. Nevertheless this result is a good first start and I think important for people to know about. I am a bit confused about what is being called a "movie." My understanding is that it is essentially a sequence of unrelated images shown for 1 sec. each. But then it is stated that the "frame rate" is 1/8.33 ms. I think this must refer to the refresh rate of the monitor, right? I would guess that the deviations from the LN model are even stronger when you show actual dynamic natural scenes - i.e., real movies. Here I would expect the rnn to have an even more profound effect, and potentially be much more informative.
This work is an important step in developing the tools for understanding the nonlinear response properties of visual neurons. The methods are sound and the results are meaningful. Reviewer 3 gave a much lower score than the other two reviewers because Rev 3 does not appreciate the improvement of prediction performance as an advance in itself. For understanding of the visual algorithms in the brain, however, prediction performance is the most critical success criterion. The paper provides convincing evidence that the approach is promising and likely to facilitate further advances towards achieving this long-term goal. I am confident enough to defend acceptance of this paper for a poster.
We have posted a new revision of the manuscript in which we changed the subset of cells for which we show results to better align with a previous study. The results remain unchanged. One of our example cells in Figure 3 was no longer in the criteria-passing subset so we are showing responses from a different OFF example cell. Additionally, we added a supplementary figure showing comparisons using a normalized log-likelihood metric, posted a link to a video of the stimulus, added “Responses” to the title, and made minor cosmetic changes to the figures.
This paper explores the ability of nonlinear recurrent neural networks to account for neural response properties that have otherwise eluded the ability of other models. A multilayer rnn is trained to imitate the stimulus-response mapping measured from actual retinal ganglion cells in response to a sequence of natural images. The rnn performs significantly better, especially in accounting for transient responses, than conventional LN/GLM models. This work is an important step in understanding the nonlinear response properties of visual neurons. Recent results have shown that the responses of even retinal ganglion cells in response to natural movies are difficult to explain in terms of standard receptive field models. So this presents an important challenge to the field. If we even had *a* model that works, it would be a starting point. So this work should be seen in that light. The challenge now of course is to tease apart what the rnn is doing. Perhaps it could now be pruned and simplified to see what parts are critical to performance. It would have been nice to see such an analysis. Nevertheless this result is a good first start and I think important for people to know about. I am a bit confused about what is being called a "movie." My understanding is that it is essentially a sequence of unrelated images shown for 1 sec. each. But then it is stated that the "frame rate" is 1/8.33 ms. I think this must refer to the refresh rate of the monitor, right? I would guess that the deviations from the LN model are even stronger when you show actual dynamic natural scenes - i.e., real movies. Here I would expect the rnn to have an even more profound effect, and potentially be much more informative.
This is a clearly written paper with a nice, if straightforward, result: RNNs can be good predictive models of neuron firing rates in the retina. On the one hand, the primary scientific contribution seems to just be to confirm that this approach works. On this particular stimulus locked task the gains from using the RNN seemed relatively modest, and it hasn't yet taught us anything new about the biology. On the other hand, this (along with the concurrent work of McIntosh et al.) is introducing neural network modeling to a field that isn't currently using it, and where it should prove very effective. I think it would be very interesting to see the results of applying a framework like this one with LFP and other neurons as input and on a shorter discretization time scale. I suspect followup work building on this proof of concept will be increasingly exciting. Minor comments: Sec 3.2: I didn't understand the role of the 0.833 ms bins. Use "epoch" throughout, rather than alternating between "epoch" and "pass through data". Fig. 4 would be better with the x-axis on a log scale.
This paper fits models to spike trains of retinal ganglion cells that are driven by natural images. I think the title should thus include the word “activity” at the end for otherwise it is actually formally incorrect. Anyhow, this paper proposes more specifically a recurrent network for this time series prediction and compares it to what seems to be the previous approach of a generalized linear model. Overall the stated paradigm is that when one can predict the spikes well then one can look into the model and learn how nature does it. In general the paper sounds plausible, though I am not convinced that I learned a lot. The results in figure 2 show that the RNN model can predict the spikes a bit better. So this is nice. But now what? You have shown that a more complicated model can produce better fits to the data, though there are of course still some variations to the real data. Your initial outline was that a better predictive model helps you to better understand the neural processing in the retina. So tell us what you learned. I am not a specialist of the retina, but I know that there are several layers and recurrencies in the retina, so I am not so surprised that the new model is better than the GLM. It seems that more complicated recurrent models such as LSTM do not improve the performance according to a statement in the paper. However, comparisons on this level are also difficult as a more complex models needs more data. Hence, I would actually expect that more layers and even a more detailed model of the retina could eventually improve the prediction even further. I was also a bit puzzled that all the neurons in the network share all the same parameters (weights). While the results show that these simplified models can capture a lot of the spike train characteristics, couldn’t a model with free parameters eventually outperform this one (with correspondingly more training data)?
This work combines a LDA-type topic model with a RNN and models this by having an additive effect on the predictive distribution via the topic parameters. A variational auto-encoder is used to infer the topic distribution for a given piece of text and the RNN is trained as a RNNLM. The last hidden state of the RNNLM and the topic parameters are then concatenated to use as a feature representation. The paper is well written and easy to understand. Using the topic as an additive effect on the vocabulary allows for easy inference but intuitively I would expect the topic to affect the dynamics too, e.g. the state of the RNN. The results on using this model as a feature extractor for IMDB are quite strong. Is the RNN fine-tuned on the labelled IMDB data? However, the results for PTB are weaker. From the original paper, an ensemble of 2 LSTMs is able to match the topicRNN score. This method of jointly modelling topics and a language model seems effective and relatively easy to implement. Finally, the IMDB result is no longer state of the art since this result appeared in May (Miyato et al., Adversarial Training Methods for Semi-Supervised Text Classification). Some questions: How important is the stop word modelling? What do the results look like if l_t = 0.5 for all t? It seems surprising that the RNN was more effective than the LSTM. Was gradient clipping tried in the topicLSTM case? Do GRUs also fail to work? It is also unfortunate that the model requires a stop-word list. Is the link in footnote 4 the one that is used in the experiments? Does factoring out the topics in this way allow the RNN to scale better with more neurons? How reasonable does the topic distribution look for individual documents? How peaked do they tend to be? Can you show some examples of the inferred distribution? The topics look odd for IMDB with the top word of two of the topics being the same: 'campbell'. It would be interesting to compare these topics with those inferred by LDA on the same datasets. Minor comments: Below figure 2: GHz -> GB Gamma is not defined.
Though the have been attempts to incorporate both "topic-like" and "sequence-like" methods in the past (e.g, the work of Hanna Wallach, Amit Gruber and other), they were quite computationally expensive, especially when high-order ngrams are incorporated. This is a modern take on this challenge: using RNNs and the VAE / inference network framework. The results are quite convincing, and the paper is well written. Pros: -- clean and simple model -- sufficiently convincing experimentation Cons: -- other ways to model interaction between RNN and topic representation could be considered (see comments of R2 and R1)
We thank the reviewers and the anonymous commenters for the helpful feedback and questions! We first summarize the main idea of this paper below: Neural network-based language models have achieved state of the art results on many NLP tasks. One difficult problem is to capture long-range dependencies as motivated in the introduction of this paper. We propose to solve this by integrating latent topics as context and jointly training these contextual features with the parameters of an RNN language model. We provide a natural way of doing this integration by modeling stop words that are excluded by topic models but needed for sequential language models. This is done via binary classification where the probability of being a stop word is dictated by the hidden layer of the RNN. This modeling approach is possible when the contextual features as provided by the topics are passed directly to the softmax output layer of the RNN as additional bias. We illustrate the performance of this approach on two tasks and two datasets: word prediction on PTB and sentiment analysis on IMDB. We provide competitive perplexity scores on PTB showing more generalization capabilities (for example we only need a TopicGRU with 100 neurons to achieve a better perplexity than stacking 2 LSTMs with 200 neurons each ---112.4 vs 115.9). "This method of jointly modeling topics and a language model seems effective and relatively easy to implement." quoted from AnonReviewer1. We have revised the paper and added the following changes: 1- we added a line on the middle of page 7 to clarify even more how we compute the topic vector theta using a sliding window for word prediction. 2- we added the test perplexity scores for TopicRNN, TopicLSTM, and TopicGRU as required by AnonReviewer3. 3- we added the inferred distributions from some documents as required by AnonReviewer1. 4- we added an explanation of why we passed the topics directly to the output layer at the bottom of page 4. We answer each reviewer individually. See below.
I have a question regrading on the language modeling part. I believe it seems unfair to get a global word distribution(i.e. document topic) first and then use it to do word prediction. The RNN model would never do this and would perform not very good on the very beginning of this article. So does the ppl performance increase comes from this? What if the RNN model gets a global embedding first and then do the word prediction?
This paper introduces a model that blends ideas from generative topic models with those from recurrent neural network language models. The authors evaluate the proposed approach on a document level classification benchmark as well as a language modeling benchmark and it seems to work well. There is also some analysis as to topics learned by the model and its ability to generate text. Overall the paper is clearly written and with the code promised by the authors others should be able to re-implement the approach. I have 2 potentially major questions I would ask the authors to address: 1 - LDA topic models make an exchangability (bag of words) assumption. The discussion of the generative story for TopicRNN should explicitly discuss whether this assumption is also made. On the surface it appears it is since y_t is sampled using only the document topic vector and h_t but we know that in practice h_t comes from a recurrent model that observes y_t-1. Not clear how this clean exposition of the generative model relates to what is actually done. In the Generating sequential text section it’s clear the topic model can’t generate words without using y_1 - t-1 but this seems inconsistent with the generative model specification. This needs to be shown in the paper and made clear to have a complete paper. 2 - The topic model only allows for linear interactions of the topic vector theta. It seems like this might be required to keep the generative model tractable but seems like a very poor assumption. We would expect the topic representation to have rich interactions with a language model to create nonlinear adjustments to word probabilities for a document. Please add discussion as to why this modeling choice exists and if possible how future work could modify that assumption (or explain why it’s not such a bad assumption as one might imagine) Figure 2 colors very difficult to distinguish.
This paper presents TopicRNN, a combination of LDA and RNN that augments traditional RNN with latent topics by having a switching variable that includes/excludes additive effects from latent topics when generating a word. Experiments on two tasks are performed: language modeling on PTB, and sentiment analysis on IMBD. The authors show that TopicRNN outperforms vanilla RNN on PTB and achieves SOTA result on IMDB. Some questions and comments: - In Table 2, how do you use LDA features for RNN (RNN LDA features)? - I would like to see results from LSTM included here, even though it is lower perplexity than TopicRNN. I think it's still useful to see how much adding latent topics close the gap between RNN and LSTM. - The generated text in Table 3 are not meaningful to me. What is this supposed to highlight? Is this generated text for topic "trading"? What about the IMDB one? - How scalable is the proposed method for large vocabulary size (>10K)? - What is the accuracy on IMDB if the extracted features is used directly to perform classification? (instead of being passed to a neural network with one hidden state). I think this is a fairer comparison to BoW, LDA, and SVM methods presented as baselines.
This work combines a LDA-type topic model with a RNN and models this by having an additive effect on the predictive distribution via the topic parameters. A variational auto-encoder is used to infer the topic distribution for a given piece of text and the RNN is trained as a RNNLM. The last hidden state of the RNNLM and the topic parameters are then concatenated to use as a feature representation. The paper is well written and easy to understand. Using the topic as an additive effect on the vocabulary allows for easy inference but intuitively I would expect the topic to affect the dynamics too, e.g. the state of the RNN. The results on using this model as a feature extractor for IMDB are quite strong. Is the RNN fine-tuned on the labelled IMDB data? However, the results for PTB are weaker. From the original paper, an ensemble of 2 LSTMs is able to match the topicRNN score. This method of jointly modelling topics and a language model seems effective and relatively easy to implement. Finally, the IMDB result is no longer state of the art since this result appeared in May (Miyato et al., Adversarial Training Methods for Semi-Supervised Text Classification). Some questions: How important is the stop word modelling? What do the results look like if l_t = 0.5 for all t? It seems surprising that the RNN was more effective than the LSTM. Was gradient clipping tried in the topicLSTM case? Do GRUs also fail to work? It is also unfortunate that the model requires a stop-word list. Is the link in footnote 4 the one that is used in the experiments? Does factoring out the topics in this way allow the RNN to scale better with more neurons? How reasonable does the topic distribution look for individual documents? How peaked do they tend to be? Can you show some examples of the inferred distribution? The topics look odd for IMDB with the top word of two of the topics being the same: 'campbell'. It would be interesting to compare these topics with those inferred by LDA on the same datasets. Minor comments: Below figure 2: GHz -> GB Gamma is not defined.
This work combines a LDA-type topic model with a RNN and models this by having an additive effect on the predictive distribution via the topic parameters. A variational auto-encoder is used to infer the topic distribution for a given piece of text and the RNN is trained as a RNNLM. The last hidden state of the RNNLM and the topic parameters are then concatenated to use as a feature representation. The paper is well written and easy to understand. Using the topic as an additive effect on the vocabulary allows for easy inference but intuitively I would expect the topic to affect the dynamics too, e.g. the state of the RNN. The results on using this model as a feature extractor for IMDB are quite strong. Is the RNN fine-tuned on the labelled IMDB data? However, the results for PTB are weaker. From the original paper, an ensemble of 2 LSTMs is able to match the topicRNN score. This method of jointly modelling topics and a language model seems effective and relatively easy to implement. Finally, the IMDB result is no longer state of the art since this result appeared in May (Miyato et al., Adversarial Training Methods for Semi-Supervised Text Classification). Some questions: How important is the stop word modelling? What do the results look like if l_t = 0.5 for all t? It seems surprising that the RNN was more effective than the LSTM. Was gradient clipping tried in the topicLSTM case? Do GRUs also fail to work? It is also unfortunate that the model requires a stop-word list. Is the link in footnote 4 the one that is used in the experiments? Does factoring out the topics in this way allow the RNN to scale better with more neurons? How reasonable does the topic distribution look for individual documents? How peaked do they tend to be? Can you show some examples of the inferred distribution? The topics look odd for IMDB with the top word of two of the topics being the same: 'campbell'. It would be interesting to compare these topics with those inferred by LDA on the same datasets. Minor comments: Below figure 2: GHz -> GB Gamma is not defined.
Though the have been attempts to incorporate both "topic-like" and "sequence-like" methods in the past (e.g, the work of Hanna Wallach, Amit Gruber and other), they were quite computationally expensive, especially when high-order ngrams are incorporated. This is a modern take on this challenge: using RNNs and the VAE / inference network framework. The results are quite convincing, and the paper is well written. Pros: -- clean and simple model -- sufficiently convincing experimentation Cons: -- other ways to model interaction between RNN and topic representation could be considered (see comments of R2 and R1)
We thank the reviewers and the anonymous commenters for the helpful feedback and questions! We first summarize the main idea of this paper below: Neural network-based language models have achieved state of the art results on many NLP tasks. One difficult problem is to capture long-range dependencies as motivated in the introduction of this paper. We propose to solve this by integrating latent topics as context and jointly training these contextual features with the parameters of an RNN language model. We provide a natural way of doing this integration by modeling stop words that are excluded by topic models but needed for sequential language models. This is done via binary classification where the probability of being a stop word is dictated by the hidden layer of the RNN. This modeling approach is possible when the contextual features as provided by the topics are passed directly to the softmax output layer of the RNN as additional bias. We illustrate the performance of this approach on two tasks and two datasets: word prediction on PTB and sentiment analysis on IMDB. We provide competitive perplexity scores on PTB showing more generalization capabilities (for example we only need a TopicGRU with 100 neurons to achieve a better perplexity than stacking 2 LSTMs with 200 neurons each ---112.4 vs 115.9). "This method of jointly modeling topics and a language model seems effective and relatively easy to implement." quoted from AnonReviewer1. We have revised the paper and added the following changes: 1- we added a line on the middle of page 7 to clarify even more how we compute the topic vector theta using a sliding window for word prediction. 2- we added the test perplexity scores for TopicRNN, TopicLSTM, and TopicGRU as required by AnonReviewer3. 3- we added the inferred distributions from some documents as required by AnonReviewer1. 4- we added an explanation of why we passed the topics directly to the output layer at the bottom of page 4. We answer each reviewer individually. See below.
I have a question regrading on the language modeling part. I believe it seems unfair to get a global word distribution(i.e. document topic) first and then use it to do word prediction. The RNN model would never do this and would perform not very good on the very beginning of this article. So does the ppl performance increase comes from this? What if the RNN model gets a global embedding first and then do the word prediction?
This paper introduces a model that blends ideas from generative topic models with those from recurrent neural network language models. The authors evaluate the proposed approach on a document level classification benchmark as well as a language modeling benchmark and it seems to work well. There is also some analysis as to topics learned by the model and its ability to generate text. Overall the paper is clearly written and with the code promised by the authors others should be able to re-implement the approach. I have 2 potentially major questions I would ask the authors to address: 1 - LDA topic models make an exchangability (bag of words) assumption. The discussion of the generative story for TopicRNN should explicitly discuss whether this assumption is also made. On the surface it appears it is since y_t is sampled using only the document topic vector and h_t but we know that in practice h_t comes from a recurrent model that observes y_t-1. Not clear how this clean exposition of the generative model relates to what is actually done. In the Generating sequential text section it’s clear the topic model can’t generate words without using y_1 - t-1 but this seems inconsistent with the generative model specification. This needs to be shown in the paper and made clear to have a complete paper. 2 - The topic model only allows for linear interactions of the topic vector theta. It seems like this might be required to keep the generative model tractable but seems like a very poor assumption. We would expect the topic representation to have rich interactions with a language model to create nonlinear adjustments to word probabilities for a document. Please add discussion as to why this modeling choice exists and if possible how future work could modify that assumption (or explain why it’s not such a bad assumption as one might imagine) Figure 2 colors very difficult to distinguish.
This paper presents TopicRNN, a combination of LDA and RNN that augments traditional RNN with latent topics by having a switching variable that includes/excludes additive effects from latent topics when generating a word. Experiments on two tasks are performed: language modeling on PTB, and sentiment analysis on IMBD. The authors show that TopicRNN outperforms vanilla RNN on PTB and achieves SOTA result on IMDB. Some questions and comments: - In Table 2, how do you use LDA features for RNN (RNN LDA features)? - I would like to see results from LSTM included here, even though it is lower perplexity than TopicRNN. I think it's still useful to see how much adding latent topics close the gap between RNN and LSTM. - The generated text in Table 3 are not meaningful to me. What is this supposed to highlight? Is this generated text for topic "trading"? What about the IMDB one? - How scalable is the proposed method for large vocabulary size (>10K)? - What is the accuracy on IMDB if the extracted features is used directly to perform classification? (instead of being passed to a neural network with one hidden state). I think this is a fairer comparison to BoW, LDA, and SVM methods presented as baselines.
This work combines a LDA-type topic model with a RNN and models this by having an additive effect on the predictive distribution via the topic parameters. A variational auto-encoder is used to infer the topic distribution for a given piece of text and the RNN is trained as a RNNLM. The last hidden state of the RNNLM and the topic parameters are then concatenated to use as a feature representation. The paper is well written and easy to understand. Using the topic as an additive effect on the vocabulary allows for easy inference but intuitively I would expect the topic to affect the dynamics too, e.g. the state of the RNN. The results on using this model as a feature extractor for IMDB are quite strong. Is the RNN fine-tuned on the labelled IMDB data? However, the results for PTB are weaker. From the original paper, an ensemble of 2 LSTMs is able to match the topicRNN score. This method of jointly modelling topics and a language model seems effective and relatively easy to implement. Finally, the IMDB result is no longer state of the art since this result appeared in May (Miyato et al., Adversarial Training Methods for Semi-Supervised Text Classification). Some questions: How important is the stop word modelling? What do the results look like if l_t = 0.5 for all t? It seems surprising that the RNN was more effective than the LSTM. Was gradient clipping tried in the topicLSTM case? Do GRUs also fail to work? It is also unfortunate that the model requires a stop-word list. Is the link in footnote 4 the one that is used in the experiments? Does factoring out the topics in this way allow the RNN to scale better with more neurons? How reasonable does the topic distribution look for individual documents? How peaked do they tend to be? Can you show some examples of the inferred distribution? The topics look odd for IMDB with the top word of two of the topics being the same: 'campbell'. It would be interesting to compare these topics with those inferred by LDA on the same datasets. Minor comments: Below figure 2: GHz -> GB Gamma is not defined.
All reviewers have carefully looked at the paper and weakly support acceptance of the paper. Program Chairs also looked at this paper and believe that its contribution is too marginal and incremental in its current form. We encourage the authors to resubmit.
We have released the demo of a fully-understandable ISAN network for counting parenthesis at
We would like to thank all reviewers for their careful and thorough reviews, and for recommending paper acceptance. Based on your specific actionable feedback, we have since improved the paper with additional analysis and experiments. We hope that based on these new results, you will raise your scores and more confidently recommend acceptance. In particular we have included a new analysis in Section 5 which provides an end-to-end interpretation of the functioning of the ISAN on a parenthesis counting task. For this example we believe we have really cracked the case’, and fully explain the neural network behavior. We have also included a new analysis (Figure 4c) that quantifies the importance of past characters for current predictions. Furthermore, we have prepared a standalone IPython demo featuring our implementation of the ISAN on the parenthesis counting task and are waiting for approval to release it. Lastly we have a included a new plot (Figure 6) that uses the kappa_word as an embedding space, clearly showing that semantic structure arises on a word level, even though the model is only trained on next character prediction. In terms of new experimental validation we have added the following comparisons (see Section 4.1): 1) fully linear dynamics (without switching) with linear readouts 2) fully linear dynamics (without switching) but with non-linear readouts 3) naive bayes These experiments highlight the crucial importance of input switching for the performance of the ISAN. To investigate the limits of the input switched architecture we also ran experiments to compare the ISAN to the LSTM on a word-fragment task with a large number of inputs and find that it performs less well than the LSTM, with a gap of around 0.15 bits / char-pair (details given below). Finally, we have improved the text in a number of places to address reviewer concerns. These changes are detailed in the per-reviewer responses.
Summary: The authors present a simple RNN with linear dynamics for language modeling. The linear dynamics greatly enhance the interpretability of the model, as well as provide the potential to improve performance by caching the dynamics for common sub-sequences. Overall, the quantitative comparison on a benchmark task is underwhelming. It’s unclear why the authors didn’t consider a more common dataset, and they only considered a single dataset. On the other hand, they present a number of well-executed techniques for analyzing the behavior of the model, many of which would be impossible to do for a non-linear RNN. Overall, I recommend that the paper is accepted, despite the results. It provides an interesting read and an important contribution to the research dialogue. Feedback The paper could be improved by shortening the number of analysis experiments and increasing the discussion of related sequence models. Some of the experiments were very compelling, whereas some of them (eg. 4.6) sort of feels like you’re just showing the reader that the model fits the data well, not that the model has any particularly important property. We trust that the model fits the data well, since you get reasonable perplexity results. LSTMS/GRUs are great for for language modeling for data with rigid combinatorial structure, such as nested parenthesis. It would have been nice if you compared your model to non-linear methods on this sort of data. Don’t be scared of negative results! It would be interesting if the non-linear methods were substantially better on these tasks. You should definitely add a discussion of Belanger and Kakade 2015 to the related work. They have different motivations (fast, scalable learning algorithms) rather than you (interpretable latent state dynamics and simple credit assignment for future predictions given past). On the other hand, they also have linear dynamics, and look at the singular vectors of the transition matrix to analyze the model. More broadly, it would be useful for readers if you discussed LDS more directly. A lot of this comparison came up in the openreview discussion, and I recommend folding this into the paper. For example, it would be useful to emphasize that the bias vectors correspond to columns of the Kalman gain matrix. One last thing regarding LDS: your model corresponds to Kalman filtering but in an LDS you can also do Kalman smoothing, where state vectors are inferred using the future in addition to the past observations. Could you do something similar in your model? What if you said that each matrix is a sparse/convex combination of a set of dictionary matrices? This parameter sharing could provide even more interpretability, since the characters are then represented by the low-dimensional weights used to combine the dictionary elements. This could also provide more scalability to word-level problems.
Summary: The authors propose an input switched affine network to do character-level language modeling, a kind of RNN without pointwise nonlinearity, but with switching the transition matrix & bias based on the input character. This is motivated by intelligibility, since it allows decomposition of output contribution into these kappa_st terms, and use of basic linear algebra to probe the network. Regarding myself as a reviewer, I am quite sure I understood the main ideas and arguments of this paper, but am not an expert on RNN language models or intelligibility/interpretability in ML. I did not read any papers with a similar premise - closest related work I'm familiar with would be deconvnet for insight into vision-CNNs. PRO: I think this is original and novel work. This work is high quality, well written, and clearly is the result of a lot of work. I found section 4.5 about projecting into readout subspace vs "computational" subspace most interesting and meaningful. CON: + The main hesitation I have is that the results on both parts (ISAN model, and analysis of it) are not entirely convincing: (1) ISAN is only trained on small task (text8), not clear whether it can be a strong char-LM on larger scale tasks, (2) nor do the analysis sections provide all that much real insight in the learned network. (1b) Other caveat towards ISAN architecture: this model in its proposed form is really only fit for small-vocabulary (i.e. character-based) language modeling, not a general RNN with large-vocab discrete input nor continuous input. (2a) For analysis: many cute plots and fun ideas of quantities to look at, but not much concrete insights. (2b) Not very clear which analysis is specific to the ISAN model, and which ideas will generalize to general nonlinear RNNs. (2c) Re sec 4.2 - 4.3: It seems that the quantity kappa_st on which analysis rests, isn't all that meaningful. Elaborating a bit on what I wrote in the question: For example: Fig 2, for input letter "u" in revenue, there's a red spot where '_' character massively positively impacts the logit of 'e'. This seems quite meaningless, what would be the meaning of influence of '_' character? So it looks ot me that the switching matrix W_u (and prior W_n W_e etc) are using previous state in an interesting way to produce that following e. So that metric kappa_st just doesn't seem very meaningful. This remark relates to the last paragraph of Sec4.2. Even though the list of cons here is longer than pro's, I recommend accept; specifically because the originality of this work will in any case make it more vulnerable to critiques. This work is well-motivated, very well-executed, and can inspire many more interesting investigations along these lines.
The authors present a character language model that gains some interpretability without large losses in predictivity. CONTRIBUTION: I'd characterize the paper as some experimental investigation of a cute insight. Recall that multi-class logistic regression allows you to apportion credit for a prediction to the input features: some features raised the probability of the correct class, while others lowered it. This paper points out that a sufficiently simple RNN model architecture is log-linear in the same way, so you can apportion credit for a prediction among elements of the past history. PROS: The paper is quite well-written and was fun to read. It's nice to see that a simple architecture still does respectably. It's easy to imagine using this model for a classroom assignment. It should be easy to implement, and the students could replicate the authors' investigation of what influences the network's predictions. The authors present some nice visualizations. Section 5.2 also describes some computational benefits. CAVEATS ON PREDICTIVE ACCURACY: * Figure 1 says that the ISAN has "near identical performance to other architectures." But this appears true only when comparing the largest models. Explanation: It appears that for smaller parameter sizes, a GRU still beats the authors' model by 22% to 39% in the usual metric of perplexity per word (ppw). (That's how LM people usually report performance, with a 10% reduction in ppw traditionally being considered a good Ph.D. dissertation. I assumed an average of 7 chars/word when converting cross-entropy/char to perplexity/word.) * In addition, it's not known whether this model family will remain competitive beyond the toy situations tested here. Explanation: The authors tried it only on character-based language modeling, and only on a 10M-char dataset, so their ppw is extremely high: 2135 for the best models in this paper. By contrast, a word-based RNN LM trained on 44M words gets ppw of 133, and trained on 800M words gets ppw of 51. [Numbers copied from the paper I cited before:
Very cool work. Here's some possibly related work on treating text as one-hots coming from a latent linear dynamical system over unobserved embeddings, "A Linear Dynamical System Model for Text" by Belanger and Kakade:
All reviewers have carefully looked at the paper and weakly support acceptance of the paper. Program Chairs also looked at this paper and believe that its contribution is too marginal and incremental in its current form. We encourage the authors to resubmit.
We have released the demo of a fully-understandable ISAN network for counting parenthesis at
We would like to thank all reviewers for their careful and thorough reviews, and for recommending paper acceptance. Based on your specific actionable feedback, we have since improved the paper with additional analysis and experiments. We hope that based on these new results, you will raise your scores and more confidently recommend acceptance. In particular we have included a new analysis in Section 5 which provides an end-to-end interpretation of the functioning of the ISAN on a parenthesis counting task. For this example we believe we have really cracked the case’, and fully explain the neural network behavior. We have also included a new analysis (Figure 4c) that quantifies the importance of past characters for current predictions. Furthermore, we have prepared a standalone IPython demo featuring our implementation of the ISAN on the parenthesis counting task and are waiting for approval to release it. Lastly we have a included a new plot (Figure 6) that uses the kappa_word as an embedding space, clearly showing that semantic structure arises on a word level, even though the model is only trained on next character prediction. In terms of new experimental validation we have added the following comparisons (see Section 4.1): 1) fully linear dynamics (without switching) with linear readouts 2) fully linear dynamics (without switching) but with non-linear readouts 3) naive bayes These experiments highlight the crucial importance of input switching for the performance of the ISAN. To investigate the limits of the input switched architecture we also ran experiments to compare the ISAN to the LSTM on a word-fragment task with a large number of inputs and find that it performs less well than the LSTM, with a gap of around 0.15 bits / char-pair (details given below). Finally, we have improved the text in a number of places to address reviewer concerns. These changes are detailed in the per-reviewer responses.
Summary: The authors present a simple RNN with linear dynamics for language modeling. The linear dynamics greatly enhance the interpretability of the model, as well as provide the potential to improve performance by caching the dynamics for common sub-sequences. Overall, the quantitative comparison on a benchmark task is underwhelming. It’s unclear why the authors didn’t consider a more common dataset, and they only considered a single dataset. On the other hand, they present a number of well-executed techniques for analyzing the behavior of the model, many of which would be impossible to do for a non-linear RNN. Overall, I recommend that the paper is accepted, despite the results. It provides an interesting read and an important contribution to the research dialogue. Feedback The paper could be improved by shortening the number of analysis experiments and increasing the discussion of related sequence models. Some of the experiments were very compelling, whereas some of them (eg. 4.6) sort of feels like you’re just showing the reader that the model fits the data well, not that the model has any particularly important property. We trust that the model fits the data well, since you get reasonable perplexity results. LSTMS/GRUs are great for for language modeling for data with rigid combinatorial structure, such as nested parenthesis. It would have been nice if you compared your model to non-linear methods on this sort of data. Don’t be scared of negative results! It would be interesting if the non-linear methods were substantially better on these tasks. You should definitely add a discussion of Belanger and Kakade 2015 to the related work. They have different motivations (fast, scalable learning algorithms) rather than you (interpretable latent state dynamics and simple credit assignment for future predictions given past). On the other hand, they also have linear dynamics, and look at the singular vectors of the transition matrix to analyze the model. More broadly, it would be useful for readers if you discussed LDS more directly. A lot of this comparison came up in the openreview discussion, and I recommend folding this into the paper. For example, it would be useful to emphasize that the bias vectors correspond to columns of the Kalman gain matrix. One last thing regarding LDS: your model corresponds to Kalman filtering but in an LDS you can also do Kalman smoothing, where state vectors are inferred using the future in addition to the past observations. Could you do something similar in your model? What if you said that each matrix is a sparse/convex combination of a set of dictionary matrices? This parameter sharing could provide even more interpretability, since the characters are then represented by the low-dimensional weights used to combine the dictionary elements. This could also provide more scalability to word-level problems.
Summary: The authors propose an input switched affine network to do character-level language modeling, a kind of RNN without pointwise nonlinearity, but with switching the transition matrix & bias based on the input character. This is motivated by intelligibility, since it allows decomposition of output contribution into these kappa_st terms, and use of basic linear algebra to probe the network. Regarding myself as a reviewer, I am quite sure I understood the main ideas and arguments of this paper, but am not an expert on RNN language models or intelligibility/interpretability in ML. I did not read any papers with a similar premise - closest related work I'm familiar with would be deconvnet for insight into vision-CNNs. PRO: I think this is original and novel work. This work is high quality, well written, and clearly is the result of a lot of work. I found section 4.5 about projecting into readout subspace vs "computational" subspace most interesting and meaningful. CON: + The main hesitation I have is that the results on both parts (ISAN model, and analysis of it) are not entirely convincing: (1) ISAN is only trained on small task (text8), not clear whether it can be a strong char-LM on larger scale tasks, (2) nor do the analysis sections provide all that much real insight in the learned network. (1b) Other caveat towards ISAN architecture: this model in its proposed form is really only fit for small-vocabulary (i.e. character-based) language modeling, not a general RNN with large-vocab discrete input nor continuous input. (2a) For analysis: many cute plots and fun ideas of quantities to look at, but not much concrete insights. (2b) Not very clear which analysis is specific to the ISAN model, and which ideas will generalize to general nonlinear RNNs. (2c) Re sec 4.2 - 4.3: It seems that the quantity kappa_st on which analysis rests, isn't all that meaningful. Elaborating a bit on what I wrote in the question: For example: Fig 2, for input letter "u" in revenue, there's a red spot where '_' character massively positively impacts the logit of 'e'. This seems quite meaningless, what would be the meaning of influence of '_' character? So it looks ot me that the switching matrix W_u (and prior W_n W_e etc) are using previous state in an interesting way to produce that following e. So that metric kappa_st just doesn't seem very meaningful. This remark relates to the last paragraph of Sec4.2. Even though the list of cons here is longer than pro's, I recommend accept; specifically because the originality of this work will in any case make it more vulnerable to critiques. This work is well-motivated, very well-executed, and can inspire many more interesting investigations along these lines.
The authors present a character language model that gains some interpretability without large losses in predictivity. CONTRIBUTION: I'd characterize the paper as some experimental investigation of a cute insight. Recall that multi-class logistic regression allows you to apportion credit for a prediction to the input features: some features raised the probability of the correct class, while others lowered it. This paper points out that a sufficiently simple RNN model architecture is log-linear in the same way, so you can apportion credit for a prediction among elements of the past history. PROS: The paper is quite well-written and was fun to read. It's nice to see that a simple architecture still does respectably. It's easy to imagine using this model for a classroom assignment. It should be easy to implement, and the students could replicate the authors' investigation of what influences the network's predictions. The authors present some nice visualizations. Section 5.2 also describes some computational benefits. CAVEATS ON PREDICTIVE ACCURACY: * Figure 1 says that the ISAN has "near identical performance to other architectures." But this appears true only when comparing the largest models. Explanation: It appears that for smaller parameter sizes, a GRU still beats the authors' model by 22% to 39% in the usual metric of perplexity per word (ppw). (That's how LM people usually report performance, with a 10% reduction in ppw traditionally being considered a good Ph.D. dissertation. I assumed an average of 7 chars/word when converting cross-entropy/char to perplexity/word.) * In addition, it's not known whether this model family will remain competitive beyond the toy situations tested here. Explanation: The authors tried it only on character-based language modeling, and only on a 10M-char dataset, so their ppw is extremely high: 2135 for the best models in this paper. By contrast, a word-based RNN LM trained on 44M words gets ppw of 133, and trained on 800M words gets ppw of 51. [Numbers copied from the paper I cited before:
Very cool work. Here's some possibly related work on treating text as one-hots coming from a latent linear dynamical system over unobserved embeddings, "A Linear Dynamical System Model for Text" by Belanger and Kakade:
This paper describes how to approximate the FastText approach such that its memory footprint is reduced by several orders of magnitude, while preserving its classification accuracy. The original FastText approach was based on a linear classifier on top of bag-of-words embeddings. This type of method is extremely fast to train and test, but the model size can be quite large. This paper focuses on approximating the original approach with lossy compression techniques. Namely, the embeddings and classifier matrices A and B are compressed with Product Quantization, and an aggressive dictionary pruning is carried out. Experiments on various datasets (either with small or large number of classes) are conducted to tune the parameters and demonstrate the effectiveness of the approach. With a negligible loss in classification accuracy, an important reduction in term of model size (memory footprint) can be achieved, in the order of 1001000 folds compared to the original size. The paper is well written overall. The goal is clearly defined and well carried out, as well as the experiments. Different options for compressing the model data are evaluated and compared (e.g. PQ vs LSH), which is also interesting. Nevertheless the paper does not propose by itself any novel idea for text classification. It just focuses on adapting existing lossy compression techniques, which is not necessarily a problem. Specifically, it introduces: - a straightforward variant of PQ for unnormalized vectors, - dictionary pruning is cast as a set covering problem (which is NP-hard), but a greedy approach is shown to yield excellent results nonetheless, - hashing tricks and bloom filter are simply borrowed from previous papers. These techniques are quite generic and could as well be used in other works. Here are some minor problems with the paper: - it is not made clear how the full model size is computed. What is exactly in the model? Which proportion of the full size do the A and B matrices, the dictionary, and the rest, account for? It is hard to follow where is the size bottleneck, which also seems to depend on the target application (i.e. small or large number of test classes). It would have been nice to provide a formula to calculate the total model size as a function of all parameters (k,b for PQ and K for dictionary, number of classes). - some parts lack clarity. For instance, the greedy approach to prune the dictionary is exposed in less than 4 lines (top of page 5), though it is far from being straightforward. Likewise, it is not clear why the binary search used for the hashing trick would introduce an overhead of a few hundreds of KB. Overall this looks like a solid work, but with potentially limited impact research-wise.
The submission describes a method for compressing shallow and wide text classification models. The paper is well written, but the proposed method is not particularly novel, as it's comprised of existing model compression techniques. Overall, the contributions are incremental and the potential impact seems rather limited.
The paper presents a few tricks to compress a wide and shallow text classification model based on n-gram features. These tricks include (1) using (optimized) product quantization to compress embedding weights (2) pruning some of the vocabulary elements (3) hashing to reduce the storage of the vocabulary (this is a minor component of the paper). The paper focuses on models with very large vocabularies and shows a reduction in the size of the models at a relatively minor reduction of the accuracy. The problem of compressing neural models is important and interesting. The methods section of the paper is well written with good high level comments and references. However, the machine learning contributions of the paper are marginal to me. The experiments are not too convincing mainly focusing on benchmarks that are not commonly used. The implications of the paper on the state-of-the-art RNN text classification models is unclear. The use of (optimized) product quantization for approximating inner product is not particularly novel. Previous work also considered doing this. Most of the reduction in the model sizes comes from pruning vocabulary elements. The method proposed for pruning vocabulary elements is simply based on the assumption that embeddings with larger L2 norm are more important. A coverage heuristic is taken into account too. From a machine learning point of view, the proper baseline to solve this problem is to have a set of (relaxed) binary coefficients for each embedding vector and learn the coefficients jointly with the weights. An L1 regularizer on the coefficients can be used to encourage sparsity. From a practical point of view, I believe an important baseline is missing: what if one simply uses fewer vocabulary elements (e.g based on subword units - see
The paper proposes a series of tricks for compressing fast (linear) text classification models. The paper is clearly written, and the results are quite strong. The main compression is achieved via product quantization, a technique which has been explored in other applications within the neural network model compression literature. In addition to the Gong et al. work which was cited, it would be worth mentioning Quantized Convolutional Neural Networks for Mobile Devices (CVPR 2016,
This paper describes how to approximate the FastText approach such that its memory footprint is reduced by several orders of magnitude, while preserving its classification accuracy. The original FastText approach was based on a linear classifier on top of bag-of-words embeddings. This type of method is extremely fast to train and test, but the model size can be quite large. This paper focuses on approximating the original approach with lossy compression techniques. Namely, the embeddings and classifier matrices A and B are compressed with Product Quantization, and an aggressive dictionary pruning is carried out. Experiments on various datasets (either with small or large number of classes) are conducted to tune the parameters and demonstrate the effectiveness of the approach. With a negligible loss in classification accuracy, an important reduction in term of model size (memory footprint) can be achieved, in the order of 1001000 folds compared to the original size. The paper is well written overall. The goal is clearly defined and well carried out, as well as the experiments. Different options for compressing the model data are evaluated and compared (e.g. PQ vs LSH), which is also interesting. Nevertheless the paper does not propose by itself any novel idea for text classification. It just focuses on adapting existing lossy compression techniques, which is not necessarily a problem. Specifically, it introduces: - a straightforward variant of PQ for unnormalized vectors, - dictionary pruning is cast as a set covering problem (which is NP-hard), but a greedy approach is shown to yield excellent results nonetheless, - hashing tricks and bloom filter are simply borrowed from previous papers. These techniques are quite generic and could as well be used in other works. Here are some minor problems with the paper: - it is not made clear how the full model size is computed. What is exactly in the model? Which proportion of the full size do the A and B matrices, the dictionary, and the rest, account for? It is hard to follow where is the size bottleneck, which also seems to depend on the target application (i.e. small or large number of test classes). It would have been nice to provide a formula to calculate the total model size as a function of all parameters (k,b for PQ and K for dictionary, number of classes). - some parts lack clarity. For instance, the greedy approach to prune the dictionary is exposed in less than 4 lines (top of page 5), though it is far from being straightforward. Likewise, it is not clear why the binary search used for the hashing trick would introduce an overhead of a few hundreds of KB. Overall this looks like a solid work, but with potentially limited impact research-wise.
This paper describes how to approximate the FastText approach such that its memory footprint is reduced by several orders of magnitude, while preserving its classification accuracy. The original FastText approach was based on a linear classifier on top of bag-of-words embeddings. This type of method is extremely fast to train and test, but the model size can be quite large. This paper focuses on approximating the original approach with lossy compression techniques. Namely, the embeddings and classifier matrices A and B are compressed with Product Quantization, and an aggressive dictionary pruning is carried out. Experiments on various datasets (either with small or large number of classes) are conducted to tune the parameters and demonstrate the effectiveness of the approach. With a negligible loss in classification accuracy, an important reduction in term of model size (memory footprint) can be achieved, in the order of 1001000 folds compared to the original size. The paper is well written overall. The goal is clearly defined and well carried out, as well as the experiments. Different options for compressing the model data are evaluated and compared (e.g. PQ vs LSH), which is also interesting. Nevertheless the paper does not propose by itself any novel idea for text classification. It just focuses on adapting existing lossy compression techniques, which is not necessarily a problem. Specifically, it introduces: - a straightforward variant of PQ for unnormalized vectors, - dictionary pruning is cast as a set covering problem (which is NP-hard), but a greedy approach is shown to yield excellent results nonetheless, - hashing tricks and bloom filter are simply borrowed from previous papers. These techniques are quite generic and could as well be used in other works. Here are some minor problems with the paper: - it is not made clear how the full model size is computed. What is exactly in the model? Which proportion of the full size do the A and B matrices, the dictionary, and the rest, account for? It is hard to follow where is the size bottleneck, which also seems to depend on the target application (i.e. small or large number of test classes). It would have been nice to provide a formula to calculate the total model size as a function of all parameters (k,b for PQ and K for dictionary, number of classes). - some parts lack clarity. For instance, the greedy approach to prune the dictionary is exposed in less than 4 lines (top of page 5), though it is far from being straightforward. Likewise, it is not clear why the binary search used for the hashing trick would introduce an overhead of a few hundreds of KB. Overall this looks like a solid work, but with potentially limited impact research-wise.
The submission describes a method for compressing shallow and wide text classification models. The paper is well written, but the proposed method is not particularly novel, as it's comprised of existing model compression techniques. Overall, the contributions are incremental and the potential impact seems rather limited.
The paper presents a few tricks to compress a wide and shallow text classification model based on n-gram features. These tricks include (1) using (optimized) product quantization to compress embedding weights (2) pruning some of the vocabulary elements (3) hashing to reduce the storage of the vocabulary (this is a minor component of the paper). The paper focuses on models with very large vocabularies and shows a reduction in the size of the models at a relatively minor reduction of the accuracy. The problem of compressing neural models is important and interesting. The methods section of the paper is well written with good high level comments and references. However, the machine learning contributions of the paper are marginal to me. The experiments are not too convincing mainly focusing on benchmarks that are not commonly used. The implications of the paper on the state-of-the-art RNN text classification models is unclear. The use of (optimized) product quantization for approximating inner product is not particularly novel. Previous work also considered doing this. Most of the reduction in the model sizes comes from pruning vocabulary elements. The method proposed for pruning vocabulary elements is simply based on the assumption that embeddings with larger L2 norm are more important. A coverage heuristic is taken into account too. From a machine learning point of view, the proper baseline to solve this problem is to have a set of (relaxed) binary coefficients for each embedding vector and learn the coefficients jointly with the weights. An L1 regularizer on the coefficients can be used to encourage sparsity. From a practical point of view, I believe an important baseline is missing: what if one simply uses fewer vocabulary elements (e.g based on subword units - see
The paper proposes a series of tricks for compressing fast (linear) text classification models. The paper is clearly written, and the results are quite strong. The main compression is achieved via product quantization, a technique which has been explored in other applications within the neural network model compression literature. In addition to the Gong et al. work which was cited, it would be worth mentioning Quantized Convolutional Neural Networks for Mobile Devices (CVPR 2016,
This paper describes how to approximate the FastText approach such that its memory footprint is reduced by several orders of magnitude, while preserving its classification accuracy. The original FastText approach was based on a linear classifier on top of bag-of-words embeddings. This type of method is extremely fast to train and test, but the model size can be quite large. This paper focuses on approximating the original approach with lossy compression techniques. Namely, the embeddings and classifier matrices A and B are compressed with Product Quantization, and an aggressive dictionary pruning is carried out. Experiments on various datasets (either with small or large number of classes) are conducted to tune the parameters and demonstrate the effectiveness of the approach. With a negligible loss in classification accuracy, an important reduction in term of model size (memory footprint) can be achieved, in the order of 1001000 folds compared to the original size. The paper is well written overall. The goal is clearly defined and well carried out, as well as the experiments. Different options for compressing the model data are evaluated and compared (e.g. PQ vs LSH), which is also interesting. Nevertheless the paper does not propose by itself any novel idea for text classification. It just focuses on adapting existing lossy compression techniques, which is not necessarily a problem. Specifically, it introduces: - a straightforward variant of PQ for unnormalized vectors, - dictionary pruning is cast as a set covering problem (which is NP-hard), but a greedy approach is shown to yield excellent results nonetheless, - hashing tricks and bloom filter are simply borrowed from previous papers. These techniques are quite generic and could as well be used in other works. Here are some minor problems with the paper: - it is not made clear how the full model size is computed. What is exactly in the model? Which proportion of the full size do the A and B matrices, the dictionary, and the rest, account for? It is hard to follow where is the size bottleneck, which also seems to depend on the target application (i.e. small or large number of test classes). It would have been nice to provide a formula to calculate the total model size as a function of all parameters (k,b for PQ and K for dictionary, number of classes). - some parts lack clarity. For instance, the greedy approach to prune the dictionary is exposed in less than 4 lines (top of page 5), though it is far from being straightforward. Likewise, it is not clear why the binary search used for the hashing trick would introduce an overhead of a few hundreds of KB. Overall this looks like a solid work, but with potentially limited impact research-wise.
Update: After reading the rebuttal comments and the revised paper, I'm leaving the rating as it was before. This paper proposes an unsupervised algorithm for transferring samples from one domain to another (related) domain under the constraint that some predefined f returns same result for the input and the result. Pros: 1. The paper presents an interesting idea of comparing samples from different domains using a fixed perceptual function f. 2. The proposed method produces visually appealing results on several datasets 3. The authors demonstrate how their approach can be used for domain adaptation and obtain improved results on the SVHN->MNIST task 4. The paper is well-written and easy to read Cons: 1. The novelty of the method is relatively minor (I consider f-constancy term as the main contribution) 2. It feels like the proposed approach would break for more dissimilar domains. The method relies on a fixed f which is trained on the source domain. This f can potentially drop information important for obtaining 1) better reconstructions in the target domain 2) more tightly related x and g(f(x)). I think the authors should consider either training all the modules in the model end-to-end or incorporating target samples into the training of f. 3. A single domain adaptation experiment is definitely not enough to consider the proposed method as a universal alternative to the existing DA approaches. I would also like to point out that using super-resolved outputs as opposed to the actual model’s outputs can produce a false impression of the visual quality of the transferred samples. I’d suggest moving original outputs from the appendix into the main part.
In section 5.1, you said that you employ the extra training split of SVHN for two purposes: learning the function f and as a unsupervised training set s. So can I say that, in Table 2, your results are based on SVHN 'extra' training split as s domain traning set? But it is different from other methods since SVHN dataset includes train set, test set and extra training split. As far as I know, other methods are using train set as the s domain unsupervised tranining data.
The authors propose a application of GANs to map images to new domains with no labels. E.g., an MNIST 3 is used to generate a SVHN 3. Ablation analysis is given to help understand the model. The results are (subjectively) impressive and the approach could be used for cross-domain transfer, an important problem. All in all, a strong paper.
This paper presents an unsupervised domain transfer from the image of domain S to the image of domain T. It was really refreshing that this conversion was possible without any mapping data. For example, in the paper, the model can transfer the SVHN image '3' to the MNIST image '3' without the mapping data. The model can be roughly divided into GAN and Content Extractor (f in the paper). 1. GAN During training, the discriminator sees the mnist image and learns to determine it as a real image. And with GAN loss, the generator learns to get the mnist image as output when it receives an svhn image as input to deceive the discriminator. 2. Content Extractor If the model use only GAN loss, the content in the image may not be retained even if the domain is changed. For example, the generator may convert the svhn image '3' to the mnist image '2' to deceive the discriminator. In this paper, authors introduce a new function called 'f' to maintain the content. The generator includes f and generates a fake mnist image when it receives an svhn image as input. The original svhn image and the generated fake mnist image are put back into f. Then additional loss function is set so that the resulting values are the same. Here, f is learning to extract content regardless of domain. I felt very fresh in this paper so i implemented this paper myself. Here is the code I implemented.
We thank the reviewers for their time and insights. All 3 reviewers seem to agree that the work is interesting, well-written and presents extensive experiments. R1 & R2 both note the f-constancy as a novelty of this work. Also, the fact that the method does not require training pairs for the two domains is noted by R3 as a major contribution, which “could be impactful in broad problem context”. R2 & R3 both agree that the output generations are visually appealing. We have no factual dispute with the reviewers and replied to each individually below. The open review discussion has been extremely beneficial to us and the paper has been revised in order to address all actionable items raised throughout the review period. We thank the reviewers for their thoughtful and constructive reviews and all other community members who shared their comments. During the review period, the work has already been cited several times, reimplemented on github, and drawn considerable attention.
Following the request of the reviewing team, we have just uploaded a new version of our manuscript, which includes expression preserving experiments. The new experiments are in Appendix B. In order to provide a quick way to track the changes from the original submission, we color all modifications in red. Thank you for the extremely useful feedback.
This paper presents an unsupervised image transformation method that maps a sample from source domain to target domain. The major contribution lies in that it does not require aligned training pairs from two domains. The model is based on GANs. To make it work in the unsupervised setting, this paper decomposes the generation function into two modules: an encoder that identify a common feature space between two domains and an decoder that generates samples in the target domain. To avoid trivial solutions, this paper proposed two additional losses that penalize 1) the feature difference between a source sample and its transformed sample and 2) the pixel difference between a target sample and its re-generated sample. This paper presents extensive experiments on transferring SVHN digit images to MNIST style and transferring face images to emoji style. +The proposed learning method enables unsupervised domain transfer that could be impactful in broad problem contexts. +This paper presents careful ablation studies to analyze the effects of different components of the system, which is helpful for understanding the paper. +The transferred images are visually impressive and quantitative results also show the image identities are preserved across domains to some degree. -It will be more interesting to show results in other domains such as texts and images. -In addition to the face identities, it is also of great interest to analyze how well the facial attributes are preserved when mapping to target domain.
Update: thank you for running more experiments, and add more explanations in the manuscript. They addressed most of my concerns, so I updated the score accordingly. The work aims at learning a generative function G that can maps input from source domain to the target domain, such that a given representation function f remain unchanged accepting inputs from either domain. The criteria is termed f-constancy. The proposed method is evaluated on two visual domain adaptation tasks. The paper is relatively easy to follow, and the authors provided quite extensive experimental results on the two datasets. f-constancy is the main novelty of the work. It seems counter-intuitive to force the function G to be of g o f, i.e., starting from a restricted function f which might have already lost information. As in the face dataset, f is learned to optimize the performance of certain task on some external dataset. It is not clear if an input from the source or target domain can be recovered from applying G as in equation (5) and (6). Also, the f function is learned with a particular task in mind. As in the two experiments, the representation function f is learned to identify the digits in the source SVHN dataset or the identity of some face dataset. As a result, the procedure has to be repeated if we were to perform domain adaptation for the same domains but for different tasks, such as recognizing expressions instead of identity. Do the authors have insight on why the baseline method proposed in equation (1) and (2) perform so poorly? Figure 5 shows some visual comparison between style transfer and the proposed method. It is not clear though which method is better. Will it be possible to apply style transfer to generate emojis from photos and repeat the experiments shown in table 4?
Update: After reading the rebuttal comments and the revised paper, I'm leaving the rating as it was before. This paper proposes an unsupervised algorithm for transferring samples from one domain to another (related) domain under the constraint that some predefined f returns same result for the input and the result. Pros: 1. The paper presents an interesting idea of comparing samples from different domains using a fixed perceptual function f. 2. The proposed method produces visually appealing results on several datasets 3. The authors demonstrate how their approach can be used for domain adaptation and obtain improved results on the SVHN->MNIST task 4. The paper is well-written and easy to read Cons: 1. The novelty of the method is relatively minor (I consider f-constancy term as the main contribution) 2. It feels like the proposed approach would break for more dissimilar domains. The method relies on a fixed f which is trained on the source domain. This f can potentially drop information important for obtaining 1) better reconstructions in the target domain 2) more tightly related x and g(f(x)). I think the authors should consider either training all the modules in the model end-to-end or incorporating target samples into the training of f. 3. A single domain adaptation experiment is definitely not enough to consider the proposed method as a universal alternative to the existing DA approaches. I would also like to point out that using super-resolved outputs as opposed to the actual model’s outputs can produce a false impression of the visual quality of the transferred samples. I’d suggest moving original outputs from the appendix into the main part.
Following the ongoing discussion with the reviewing team as well as with other readers, we have just uploaded a new version of our manuscript, which is aimed at improving clarity. In order to provide a quick way to track the changes, we color all modifications in red. In addition, as mentioned below, we will soon share our open implementation in Torch. Thank you all for the extremely useful feedback.
Given the goal of transferring samples from source domain to target domain, I am wondering whether it is necessary to simultaneously train the model to reconstruct images in the target domain. Any ablation studies on this?
Hi, it is really interesting and I want to re-implement this work. But the paper doesn't provide much information about the configuration of the generator. For example, do you use a fully connected layer as the first layer? If true, what is the dimension? How many filters of each deconvolution layer do you use ? Thank you very much.
In section 5.1, you map SVHN-trained representation to 32*32 grayscale images, but the encoder f's input is three-channel, how do you solve this ? Why don't you transform RGB SVHN images into grayscale images or replicate the grayscale MNIST images three times ? I mean why don't you use the same image format for training f and the adversarial network?
Can the authors comment on what they are doing in terms of end-use/goal in addition to what we know from style transfer already?
This work is really interesting, but I find the network architecture and loss functions in this work are extremely similar to that in the work "Generating Images with Perceptual Similarity Metrics based on Deep Networks" by Alexey Dosoviskiy and Thomas Brox. The loss function in this work vs that of the latter: L_const is equal to L_feat; L_tid is equall to L_img; L_GANG is equal to L_adv. However, I do not see the later's name in your reference paper list. Besides, have you forgotten to print a 'log' before D_3(x) in your L_D loss on page 4?
Update: After reading the rebuttal comments and the revised paper, I'm leaving the rating as it was before. This paper proposes an unsupervised algorithm for transferring samples from one domain to another (related) domain under the constraint that some predefined f returns same result for the input and the result. Pros: 1. The paper presents an interesting idea of comparing samples from different domains using a fixed perceptual function f. 2. The proposed method produces visually appealing results on several datasets 3. The authors demonstrate how their approach can be used for domain adaptation and obtain improved results on the SVHN->MNIST task 4. The paper is well-written and easy to read Cons: 1. The novelty of the method is relatively minor (I consider f-constancy term as the main contribution) 2. It feels like the proposed approach would break for more dissimilar domains. The method relies on a fixed f which is trained on the source domain. This f can potentially drop information important for obtaining 1) better reconstructions in the target domain 2) more tightly related x and g(f(x)). I think the authors should consider either training all the modules in the model end-to-end or incorporating target samples into the training of f. 3. A single domain adaptation experiment is definitely not enough to consider the proposed method as a universal alternative to the existing DA approaches. I would also like to point out that using super-resolved outputs as opposed to the actual model’s outputs can produce a false impression of the visual quality of the transferred samples. I’d suggest moving original outputs from the appendix into the main part.
In section 5.1, you said that you employ the extra training split of SVHN for two purposes: learning the function f and as a unsupervised training set s. So can I say that, in Table 2, your results are based on SVHN 'extra' training split as s domain traning set? But it is different from other methods since SVHN dataset includes train set, test set and extra training split. As far as I know, other methods are using train set as the s domain unsupervised tranining data.
The authors propose a application of GANs to map images to new domains with no labels. E.g., an MNIST 3 is used to generate a SVHN 3. Ablation analysis is given to help understand the model. The results are (subjectively) impressive and the approach could be used for cross-domain transfer, an important problem. All in all, a strong paper.
This paper presents an unsupervised domain transfer from the image of domain S to the image of domain T. It was really refreshing that this conversion was possible without any mapping data. For example, in the paper, the model can transfer the SVHN image '3' to the MNIST image '3' without the mapping data. The model can be roughly divided into GAN and Content Extractor (f in the paper). 1. GAN During training, the discriminator sees the mnist image and learns to determine it as a real image. And with GAN loss, the generator learns to get the mnist image as output when it receives an svhn image as input to deceive the discriminator. 2. Content Extractor If the model use only GAN loss, the content in the image may not be retained even if the domain is changed. For example, the generator may convert the svhn image '3' to the mnist image '2' to deceive the discriminator. In this paper, authors introduce a new function called 'f' to maintain the content. The generator includes f and generates a fake mnist image when it receives an svhn image as input. The original svhn image and the generated fake mnist image are put back into f. Then additional loss function is set so that the resulting values are the same. Here, f is learning to extract content regardless of domain. I felt very fresh in this paper so i implemented this paper myself. Here is the code I implemented.
We thank the reviewers for their time and insights. All 3 reviewers seem to agree that the work is interesting, well-written and presents extensive experiments. R1 & R2 both note the f-constancy as a novelty of this work. Also, the fact that the method does not require training pairs for the two domains is noted by R3 as a major contribution, which “could be impactful in broad problem context”. R2 & R3 both agree that the output generations are visually appealing. We have no factual dispute with the reviewers and replied to each individually below. The open review discussion has been extremely beneficial to us and the paper has been revised in order to address all actionable items raised throughout the review period. We thank the reviewers for their thoughtful and constructive reviews and all other community members who shared their comments. During the review period, the work has already been cited several times, reimplemented on github, and drawn considerable attention.
Following the request of the reviewing team, we have just uploaded a new version of our manuscript, which includes expression preserving experiments. The new experiments are in Appendix B. In order to provide a quick way to track the changes from the original submission, we color all modifications in red. Thank you for the extremely useful feedback.
This paper presents an unsupervised image transformation method that maps a sample from source domain to target domain. The major contribution lies in that it does not require aligned training pairs from two domains. The model is based on GANs. To make it work in the unsupervised setting, this paper decomposes the generation function into two modules: an encoder that identify a common feature space between two domains and an decoder that generates samples in the target domain. To avoid trivial solutions, this paper proposed two additional losses that penalize 1) the feature difference between a source sample and its transformed sample and 2) the pixel difference between a target sample and its re-generated sample. This paper presents extensive experiments on transferring SVHN digit images to MNIST style and transferring face images to emoji style. +The proposed learning method enables unsupervised domain transfer that could be impactful in broad problem contexts. +This paper presents careful ablation studies to analyze the effects of different components of the system, which is helpful for understanding the paper. +The transferred images are visually impressive and quantitative results also show the image identities are preserved across domains to some degree. -It will be more interesting to show results in other domains such as texts and images. -In addition to the face identities, it is also of great interest to analyze how well the facial attributes are preserved when mapping to target domain.
Update: thank you for running more experiments, and add more explanations in the manuscript. They addressed most of my concerns, so I updated the score accordingly. The work aims at learning a generative function G that can maps input from source domain to the target domain, such that a given representation function f remain unchanged accepting inputs from either domain. The criteria is termed f-constancy. The proposed method is evaluated on two visual domain adaptation tasks. The paper is relatively easy to follow, and the authors provided quite extensive experimental results on the two datasets. f-constancy is the main novelty of the work. It seems counter-intuitive to force the function G to be of g o f, i.e., starting from a restricted function f which might have already lost information. As in the face dataset, f is learned to optimize the performance of certain task on some external dataset. It is not clear if an input from the source or target domain can be recovered from applying G as in equation (5) and (6). Also, the f function is learned with a particular task in mind. As in the two experiments, the representation function f is learned to identify the digits in the source SVHN dataset or the identity of some face dataset. As a result, the procedure has to be repeated if we were to perform domain adaptation for the same domains but for different tasks, such as recognizing expressions instead of identity. Do the authors have insight on why the baseline method proposed in equation (1) and (2) perform so poorly? Figure 5 shows some visual comparison between style transfer and the proposed method. It is not clear though which method is better. Will it be possible to apply style transfer to generate emojis from photos and repeat the experiments shown in table 4?
Update: After reading the rebuttal comments and the revised paper, I'm leaving the rating as it was before. This paper proposes an unsupervised algorithm for transferring samples from one domain to another (related) domain under the constraint that some predefined f returns same result for the input and the result. Pros: 1. The paper presents an interesting idea of comparing samples from different domains using a fixed perceptual function f. 2. The proposed method produces visually appealing results on several datasets 3. The authors demonstrate how their approach can be used for domain adaptation and obtain improved results on the SVHN->MNIST task 4. The paper is well-written and easy to read Cons: 1. The novelty of the method is relatively minor (I consider f-constancy term as the main contribution) 2. It feels like the proposed approach would break for more dissimilar domains. The method relies on a fixed f which is trained on the source domain. This f can potentially drop information important for obtaining 1) better reconstructions in the target domain 2) more tightly related x and g(f(x)). I think the authors should consider either training all the modules in the model end-to-end or incorporating target samples into the training of f. 3. A single domain adaptation experiment is definitely not enough to consider the proposed method as a universal alternative to the existing DA approaches. I would also like to point out that using super-resolved outputs as opposed to the actual model’s outputs can produce a false impression of the visual quality of the transferred samples. I’d suggest moving original outputs from the appendix into the main part.
Following the ongoing discussion with the reviewing team as well as with other readers, we have just uploaded a new version of our manuscript, which is aimed at improving clarity. In order to provide a quick way to track the changes, we color all modifications in red. In addition, as mentioned below, we will soon share our open implementation in Torch. Thank you all for the extremely useful feedback.
Given the goal of transferring samples from source domain to target domain, I am wondering whether it is necessary to simultaneously train the model to reconstruct images in the target domain. Any ablation studies on this?
Hi, it is really interesting and I want to re-implement this work. But the paper doesn't provide much information about the configuration of the generator. For example, do you use a fully connected layer as the first layer? If true, what is the dimension? How many filters of each deconvolution layer do you use ? Thank you very much.
In section 5.1, you map SVHN-trained representation to 32*32 grayscale images, but the encoder f's input is three-channel, how do you solve this ? Why don't you transform RGB SVHN images into grayscale images or replicate the grayscale MNIST images three times ? I mean why don't you use the same image format for training f and the adversarial network?
Can the authors comment on what they are doing in terms of end-use/goal in addition to what we know from style transfer already?
This work is really interesting, but I find the network architecture and loss functions in this work are extremely similar to that in the work "Generating Images with Perceptual Similarity Metrics based on Deep Networks" by Alexey Dosoviskiy and Thomas Brox. The loss function in this work vs that of the latter: L_const is equal to L_feat; L_tid is equall to L_img; L_GANG is equal to L_adv. However, I do not see the later's name in your reference paper list. Besides, have you forgotten to print a 'log' before D_3(x) in your L_D loss on page 4?
This paper presents a system approach to combine multiple modalities to perform classification in a practical scenario (e-commerce). In general, I find the proposed approach in the paper sound and solid, but do not see novelty in the paper: feature fusion and decision time fusion are both standard practices in multi-modal analysis, and the rest of the paper offers no surprise in implementing such approaches. This seems to be a better fit for venues that focus more on production systems, and seems to be a bad fit for ICLR where the focus is more on research of novel algorithms and theories.
Three knowledgable reviewers recommend rejection. While the application is interesting and of commercial value, the technical contribution falls below the ICLR's bar. I encourage the authors to improve the paper and submit it to a future conference.
This paper introduces a large-scale multi-model product classification system. The model consists of three modules, Image CNN (VGG 16 architecture), text CNN (Kim 2014) and decision-level fusion policies. The authors have tried several fusion methods: including policies taking inputs from text and image CNN probabilities; choose either CNN; average the predictions; end-to-end training. Experimental results show that text CNN alone works better than image CNN and multi-model fusion can improve the accuracy by a small margin. It is a little bit surprising that end-to-end feature level fusion works worse than text CNN alone. The writing is clear and there are a lot of useful practical experiences of learning large-scale model. However, I lean toward rejecting the paper because the following: 1) No other dataset reported. The authors haven't mentioned releasing the walmart dataset and it is going to be really hard to reproduce the results without the dataset. 2) Technical novelty is limited. All the decision-level fusion policies have been investigated by some previous methods before. 3) Performance gain is also limited.
This paper tackles the problem of multi-modal classification of text and images. Pros: - Interesting dataset and application. Cons: - The results are rather lacklustre, showing a very mild improvement compared to the oracle improvement. But perhaps some insights as to whether the incorrect decisions are humanly possible would help with significance of the results. - Could have explored some intermediate architectures such as feature fusion + class probabilities with/without finetuning. There are no feature fusion results reported. - No evaluation on standard datasets or comparison to previous works. What is the policy learnt for CP-1? Given 2 input class probabilities, how does the network perform better than max or mean?
This paper presents a system approach to combine multiple modalities to perform classification in a practical scenario (e-commerce). In general, I find the proposed approach in the paper sound and solid, but do not see novelty in the paper: feature fusion and decision time fusion are both standard practices in multi-modal analysis, and the rest of the paper offers no surprise in implementing such approaches. This seems to be a better fit for venues that focus more on production systems, and seems to be a bad fit for ICLR where the focus is more on research of novel algorithms and theories.
Hi, I am Jung-Woo Ha and the authour of a paper you cited in your work. In the references of your manuscript, I think that "Hyuna Pyo, Jung-Woo Ha, and Jeonghee Kim. Large-scale item categorization in e-commerce using multiple recurrent neural networks. 2010." should be changed into "Jung-Woo Ha, Hyuna Pyo, and Jeonghee Kim. Large-scale item categorization in e-commerce using multiple recurrent neural networks. In Proceedings of KDD 2016." The url is
This paper presents a system approach to combine multiple modalities to perform classification in a practical scenario (e-commerce). In general, I find the proposed approach in the paper sound and solid, but do not see novelty in the paper: feature fusion and decision time fusion are both standard practices in multi-modal analysis, and the rest of the paper offers no surprise in implementing such approaches. This seems to be a better fit for venues that focus more on production systems, and seems to be a bad fit for ICLR where the focus is more on research of novel algorithms and theories.
Three knowledgable reviewers recommend rejection. While the application is interesting and of commercial value, the technical contribution falls below the ICLR's bar. I encourage the authors to improve the paper and submit it to a future conference.
This paper introduces a large-scale multi-model product classification system. The model consists of three modules, Image CNN (VGG 16 architecture), text CNN (Kim 2014) and decision-level fusion policies. The authors have tried several fusion methods: including policies taking inputs from text and image CNN probabilities; choose either CNN; average the predictions; end-to-end training. Experimental results show that text CNN alone works better than image CNN and multi-model fusion can improve the accuracy by a small margin. It is a little bit surprising that end-to-end feature level fusion works worse than text CNN alone. The writing is clear and there are a lot of useful practical experiences of learning large-scale model. However, I lean toward rejecting the paper because the following: 1) No other dataset reported. The authors haven't mentioned releasing the walmart dataset and it is going to be really hard to reproduce the results without the dataset. 2) Technical novelty is limited. All the decision-level fusion policies have been investigated by some previous methods before. 3) Performance gain is also limited.
This paper tackles the problem of multi-modal classification of text and images. Pros: - Interesting dataset and application. Cons: - The results are rather lacklustre, showing a very mild improvement compared to the oracle improvement. But perhaps some insights as to whether the incorrect decisions are humanly possible would help with significance of the results. - Could have explored some intermediate architectures such as feature fusion + class probabilities with/without finetuning. There are no feature fusion results reported. - No evaluation on standard datasets or comparison to previous works. What is the policy learnt for CP-1? Given 2 input class probabilities, how does the network perform better than max or mean?
This paper presents a system approach to combine multiple modalities to perform classification in a practical scenario (e-commerce). In general, I find the proposed approach in the paper sound and solid, but do not see novelty in the paper: feature fusion and decision time fusion are both standard practices in multi-modal analysis, and the rest of the paper offers no surprise in implementing such approaches. This seems to be a better fit for venues that focus more on production systems, and seems to be a bad fit for ICLR where the focus is more on research of novel algorithms and theories.
Hi, I am Jung-Woo Ha and the authour of a paper you cited in your work. In the references of your manuscript, I think that "Hyuna Pyo, Jung-Woo Ha, and Jeonghee Kim. Large-scale item categorization in e-commerce using multiple recurrent neural networks. 2010." should be changed into "Jung-Woo Ha, Hyuna Pyo, and Jeonghee Kim. Large-scale item categorization in e-commerce using multiple recurrent neural networks. In Proceedings of KDD 2016." The url is
CONTRIBUTIONS When training LSTMs, many of the intermediate gradients are close to zero due to the flat shape of the tanh and sigmoid nonlinearities far from the origin. This paper shows that rounding these small gradients to zero results in matrices with up to 80% sparsity during training, and that training character-level LSTM language models with this sparsification does not significantly change the final performance of the model. The authors argue that this sparsity could be exploited with specialized hardware to improve the energy efficiency and speed of recurrent network training. NOVELTY Thresholding gradients to induce sparsity and improve efficiency in RNN training is a novel result to my knowledge. MISSING CITATIONS Prior work has explored low-precision arithmetic for recurrent neural network language models: Hubara et al, “Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations”,
The main point of the paper was that sparsifying gradients does not hurt performance; however, this in itself is not enough for a publication in this venue. As noted by R1 and R2, showing how this can help in more energy efficient training would make for a good paper; without that aspect the paper only presents an observation that is not too surprising to the practitioners in this area. Further, while the main point of the paper was relatively clear, the scientific presentation was not rigorous enough. All the reviewers point out that several details were missing (including test set performance, reporting of results on the different sets, etc). Paper would be strengthened by a better exploration of the problem.
The findings of applying sparsity in the backward gradients for training LSTMs is interesting. But the paper seems incomplete without the proper experimental justification. Only the validation loss is reported which is definitely insufficient. Proper testing results and commonly reported evaluation criterion needs to be included to support the claim of no degradation when applying the proposed sparsity technique. Also actual justification of the gains in terms of speed and efficiency would make the paper much stronger.
This paper presents the observation that it is possible to utilize sparse operations in the training of LSTM networks without loss of accuracy. This observation is novel (although not too surprising) to my knowledge, but I must state that I am not very familiar with research on fast RNN implmentations. Minor note: The LSTM language model does not use a 'word2vec' layer. It is simply a linear embedding layer. Word2vec is the name of a specific model which is not directly to character level language models. The paper presents the central observation clearly. However, it will be much more convincing if a well known dataset and experiment set up are used, such as Graves (2013) or Sutskever et al (2014), and actual training, validation and test performances are reported. While the main observation is certainly interesting, I think it is not sufficient to be the subject of a full conference paper without implementation (or simulation) and benchmarking of the promised speedups on multiple tasks. For example, how would the gains be affected by various architecture choices? At present this is an interesting technical report and I would like to see more detailed results in the future.
CONTRIBUTIONS When training LSTMs, many of the intermediate gradients are close to zero due to the flat shape of the tanh and sigmoid nonlinearities far from the origin. This paper shows that rounding these small gradients to zero results in matrices with up to 80% sparsity during training, and that training character-level LSTM language models with this sparsification does not significantly change the final performance of the model. The authors argue that this sparsity could be exploited with specialized hardware to improve the energy efficiency and speed of recurrent network training. NOVELTY Thresholding gradients to induce sparsity and improve efficiency in RNN training is a novel result to my knowledge. MISSING CITATIONS Prior work has explored low-precision arithmetic for recurrent neural network language models: Hubara et al, “Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations”,
CONTRIBUTIONS When training LSTMs, many of the intermediate gradients are close to zero due to the flat shape of the tanh and sigmoid nonlinearities far from the origin. This paper shows that rounding these small gradients to zero results in matrices with up to 80% sparsity during training, and that training character-level LSTM language models with this sparsification does not significantly change the final performance of the model. The authors argue that this sparsity could be exploited with specialized hardware to improve the energy efficiency and speed of recurrent network training. NOVELTY Thresholding gradients to induce sparsity and improve efficiency in RNN training is a novel result to my knowledge. MISSING CITATIONS Prior work has explored low-precision arithmetic for recurrent neural network language models: Hubara et al, “Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations”,
The main point of the paper was that sparsifying gradients does not hurt performance; however, this in itself is not enough for a publication in this venue. As noted by R1 and R2, showing how this can help in more energy efficient training would make for a good paper; without that aspect the paper only presents an observation that is not too surprising to the practitioners in this area. Further, while the main point of the paper was relatively clear, the scientific presentation was not rigorous enough. All the reviewers point out that several details were missing (including test set performance, reporting of results on the different sets, etc). Paper would be strengthened by a better exploration of the problem.
The findings of applying sparsity in the backward gradients for training LSTMs is interesting. But the paper seems incomplete without the proper experimental justification. Only the validation loss is reported which is definitely insufficient. Proper testing results and commonly reported evaluation criterion needs to be included to support the claim of no degradation when applying the proposed sparsity technique. Also actual justification of the gains in terms of speed and efficiency would make the paper much stronger.
This paper presents the observation that it is possible to utilize sparse operations in the training of LSTM networks without loss of accuracy. This observation is novel (although not too surprising) to my knowledge, but I must state that I am not very familiar with research on fast RNN implmentations. Minor note: The LSTM language model does not use a 'word2vec' layer. It is simply a linear embedding layer. Word2vec is the name of a specific model which is not directly to character level language models. The paper presents the central observation clearly. However, it will be much more convincing if a well known dataset and experiment set up are used, such as Graves (2013) or Sutskever et al (2014), and actual training, validation and test performances are reported. While the main observation is certainly interesting, I think it is not sufficient to be the subject of a full conference paper without implementation (or simulation) and benchmarking of the promised speedups on multiple tasks. For example, how would the gains be affected by various architecture choices? At present this is an interesting technical report and I would like to see more detailed results in the future.
CONTRIBUTIONS When training LSTMs, many of the intermediate gradients are close to zero due to the flat shape of the tanh and sigmoid nonlinearities far from the origin. This paper shows that rounding these small gradients to zero results in matrices with up to 80% sparsity during training, and that training character-level LSTM language models with this sparsification does not significantly change the final performance of the model. The authors argue that this sparsity could be exploited with specialized hardware to improve the energy efficiency and speed of recurrent network training. NOVELTY Thresholding gradients to induce sparsity and improve efficiency in RNN training is a novel result to my knowledge. MISSING CITATIONS Prior work has explored low-precision arithmetic for recurrent neural network language models: Hubara et al, “Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations”,
This paper proposes an idea of looking n-steps backward when modelling sequences with RNNs. The proposed RNN does not only use the previous hidden state (t-1) but also looks further back ( (t - k) steps, where k=1,2,3,4 ). The paper also proposes a few different ways to aggregate multiple hidden states from the past. The reviewer can see few issues with this paper. Firstly, the writing of this paper requires improvement. The introduction and abstract are wasting too much space just to explain unrelated facts or to describe already well-known things in the literature. Some of the statements written in the paper are misleading. For instance, it explains, “Among various neural network models, recurrent neural networks (RNNs) are appealing for modeling sequential data because they can capture long term dependency in sequential data using a simple mechanism of recurrent feedback” and then it says RNNs cannot actually capture long-term dependencies that well. RNNs are appealing in the first place because they can handle variable length sequences and can model temporal relationships between each symbol in a sequence. The criticism against LSTMs is hard to accept when it says: LSTMs are slow and because of the slowness, they are hard to scale at larger tasks. But we all know that some companies are already using gigantic seq2seq models for their production (LSTMs are used as building blocks in their systems). This indicates that the LSTMs can be practically used in a very large-scale setting. Secondly, the idea proposed in the paper is incremental and not new to the field. There are other previous works that propose to use direct connections to the previous hidden states [1]. However, the previous works do not use aggregation of multiple number of previous hidden states. Most importantly, the paper fails to deliver a proper analysis on whether its main contribution is actually helpful to improve the problem posed in the paper. The new architecture is said that it handles the long-term dependencies better, however, there is no rigorous proof or intuitive design in the architecture that help us to understand why it should work better. By the design of the architecture, and speaking in very high-level, it seems like the model maybe helpful to mitigate the vanishing gradients issue by a linear factor. It is always a good practice to have at least one page to analyze the empirical findings in the paper. Thirdly, the baseline models used in this paper are very weak. Their are plenty of other models that are trained and tested on word-level language modelling task using Penn Treebank corpus, but the paper only contains a few of outdated models. I cannot fully agree on the statement “To the best of our knowledge, this is the best performance on PTB under the same training condition”, these days, RNN-based methods usually score below 80 in terms of the test perplexity, which are far lower than 100 achieved in this paper. [1] Zhang et al., “Architectural Complexity Measures of Recurrent Neural Networks”, NIPS’16
Paper presents the idea of using higher order recurrence in LSTMs. The ideas are well presented and easy to follow. However, the results are far from convincing, easily being below well established numbers in the domain. Since the mode is but a very simple extension of the baseline recurrent models using LSTMs that are state of the art on language modelling, it should have been easy to make a fair comparison by replacing the LSTMs of the state of the art models with higher order versions, but the authors did not do that. Their claimed numbers for SOTA are far from previously reported numbers in that setup, as pointed out by reviewers.
The authors of the paper explore the idea of incorporating skip connections *over time* for RNNs. Even though the basic idea is not particularly innovative, a few proposals on how to merge that information into the current hidden state with different pooling functions are evaluated. The different models are compared on two popular text benchmarks. Some points. 1) The experiments feature only NLP and only prediction tasks. It would have been nice to see the models in other domains, i.e. modelling a conditional distribution p(y|x), not only p(x). Further, sensory input data such as audio or video would have given further insight. 2) As pointed out by other reviewers, it does not feel as if the comparisons to other models are fair. SOTA on NLP changes quickly and it is hard to place the experiments in the complete picture. 3) It is claimed that this helps long-term prediction. I think the paper lacks a corresponding analysis, as pointed out in an earlier question of mine. 4) It is claimed that LSTM trains slow and is hard to scale. For one does this not match my personal experience. Then, the prevalence of LSTM systems in production systems (e.g. Google, Baidu, Microsoft, ...) clearly speaks against this. I like the basic idea of the paper, but the points above make me think it is not ready for publication.
I think the backbone of the paper is interesting and could lead to something potentially quite useful. I like the idea of connecting signal processing with recurrent network and then using tools from one setting in the other. However, while the work has nuggets of very interesting observations, I feel they can be put together in a better way. I think the writeup and everything can be improved and I urge the authors to strive for this if the paper doesn't go through. I think some of the ideas of how to connect to the past are interesting, it would be nice to have more experiments or to try to understand better why this connections help and how.
This paper proposes an idea of looking n-steps backward when modelling sequences with RNNs. The proposed RNN does not only use the previous hidden state (t-1) but also looks further back ( (t - k) steps, where k=1,2,3,4 ). The paper also proposes a few different ways to aggregate multiple hidden states from the past. The reviewer can see few issues with this paper. Firstly, the writing of this paper requires improvement. The introduction and abstract are wasting too much space just to explain unrelated facts or to describe already well-known things in the literature. Some of the statements written in the paper are misleading. For instance, it explains, “Among various neural network models, recurrent neural networks (RNNs) are appealing for modeling sequential data because they can capture long term dependency in sequential data using a simple mechanism of recurrent feedback” and then it says RNNs cannot actually capture long-term dependencies that well. RNNs are appealing in the first place because they can handle variable length sequences and can model temporal relationships between each symbol in a sequence. The criticism against LSTMs is hard to accept when it says: LSTMs are slow and because of the slowness, they are hard to scale at larger tasks. But we all know that some companies are already using gigantic seq2seq models for their production (LSTMs are used as building blocks in their systems). This indicates that the LSTMs can be practically used in a very large-scale setting. Secondly, the idea proposed in the paper is incremental and not new to the field. There are other previous works that propose to use direct connections to the previous hidden states [1]. However, the previous works do not use aggregation of multiple number of previous hidden states. Most importantly, the paper fails to deliver a proper analysis on whether its main contribution is actually helpful to improve the problem posed in the paper. The new architecture is said that it handles the long-term dependencies better, however, there is no rigorous proof or intuitive design in the architecture that help us to understand why it should work better. By the design of the architecture, and speaking in very high-level, it seems like the model maybe helpful to mitigate the vanishing gradients issue by a linear factor. It is always a good practice to have at least one page to analyze the empirical findings in the paper. Thirdly, the baseline models used in this paper are very weak. Their are plenty of other models that are trained and tested on word-level language modelling task using Penn Treebank corpus, but the paper only contains a few of outdated models. I cannot fully agree on the statement “To the best of our knowledge, this is the best performance on PTB under the same training condition”, these days, RNN-based methods usually score below 80 in terms of the test perplexity, which are far lower than 100 achieved in this paper. [1] Zhang et al., “Architectural Complexity Measures of Recurrent Neural Networks”, NIPS’16
This paper proposes an idea of looking n-steps backward when modelling sequences with RNNs. The proposed RNN does not only use the previous hidden state (t-1) but also looks further back ( (t - k) steps, where k=1,2,3,4 ). The paper also proposes a few different ways to aggregate multiple hidden states from the past. The reviewer can see few issues with this paper. Firstly, the writing of this paper requires improvement. The introduction and abstract are wasting too much space just to explain unrelated facts or to describe already well-known things in the literature. Some of the statements written in the paper are misleading. For instance, it explains, “Among various neural network models, recurrent neural networks (RNNs) are appealing for modeling sequential data because they can capture long term dependency in sequential data using a simple mechanism of recurrent feedback” and then it says RNNs cannot actually capture long-term dependencies that well. RNNs are appealing in the first place because they can handle variable length sequences and can model temporal relationships between each symbol in a sequence. The criticism against LSTMs is hard to accept when it says: LSTMs are slow and because of the slowness, they are hard to scale at larger tasks. But we all know that some companies are already using gigantic seq2seq models for their production (LSTMs are used as building blocks in their systems). This indicates that the LSTMs can be practically used in a very large-scale setting. Secondly, the idea proposed in the paper is incremental and not new to the field. There are other previous works that propose to use direct connections to the previous hidden states [1]. However, the previous works do not use aggregation of multiple number of previous hidden states. Most importantly, the paper fails to deliver a proper analysis on whether its main contribution is actually helpful to improve the problem posed in the paper. The new architecture is said that it handles the long-term dependencies better, however, there is no rigorous proof or intuitive design in the architecture that help us to understand why it should work better. By the design of the architecture, and speaking in very high-level, it seems like the model maybe helpful to mitigate the vanishing gradients issue by a linear factor. It is always a good practice to have at least one page to analyze the empirical findings in the paper. Thirdly, the baseline models used in this paper are very weak. Their are plenty of other models that are trained and tested on word-level language modelling task using Penn Treebank corpus, but the paper only contains a few of outdated models. I cannot fully agree on the statement “To the best of our knowledge, this is the best performance on PTB under the same training condition”, these days, RNN-based methods usually score below 80 in terms of the test perplexity, which are far lower than 100 achieved in this paper. [1] Zhang et al., “Architectural Complexity Measures of Recurrent Neural Networks”, NIPS’16
Paper presents the idea of using higher order recurrence in LSTMs. The ideas are well presented and easy to follow. However, the results are far from convincing, easily being below well established numbers in the domain. Since the mode is but a very simple extension of the baseline recurrent models using LSTMs that are state of the art on language modelling, it should have been easy to make a fair comparison by replacing the LSTMs of the state of the art models with higher order versions, but the authors did not do that. Their claimed numbers for SOTA are far from previously reported numbers in that setup, as pointed out by reviewers.
The authors of the paper explore the idea of incorporating skip connections *over time* for RNNs. Even though the basic idea is not particularly innovative, a few proposals on how to merge that information into the current hidden state with different pooling functions are evaluated. The different models are compared on two popular text benchmarks. Some points. 1) The experiments feature only NLP and only prediction tasks. It would have been nice to see the models in other domains, i.e. modelling a conditional distribution p(y|x), not only p(x). Further, sensory input data such as audio or video would have given further insight. 2) As pointed out by other reviewers, it does not feel as if the comparisons to other models are fair. SOTA on NLP changes quickly and it is hard to place the experiments in the complete picture. 3) It is claimed that this helps long-term prediction. I think the paper lacks a corresponding analysis, as pointed out in an earlier question of mine. 4) It is claimed that LSTM trains slow and is hard to scale. For one does this not match my personal experience. Then, the prevalence of LSTM systems in production systems (e.g. Google, Baidu, Microsoft, ...) clearly speaks against this. I like the basic idea of the paper, but the points above make me think it is not ready for publication.
I think the backbone of the paper is interesting and could lead to something potentially quite useful. I like the idea of connecting signal processing with recurrent network and then using tools from one setting in the other. However, while the work has nuggets of very interesting observations, I feel they can be put together in a better way. I think the writeup and everything can be improved and I urge the authors to strive for this if the paper doesn't go through. I think some of the ideas of how to connect to the past are interesting, it would be nice to have more experiments or to try to understand better why this connections help and how.
This paper proposes an idea of looking n-steps backward when modelling sequences with RNNs. The proposed RNN does not only use the previous hidden state (t-1) but also looks further back ( (t - k) steps, where k=1,2,3,4 ). The paper also proposes a few different ways to aggregate multiple hidden states from the past. The reviewer can see few issues with this paper. Firstly, the writing of this paper requires improvement. The introduction and abstract are wasting too much space just to explain unrelated facts or to describe already well-known things in the literature. Some of the statements written in the paper are misleading. For instance, it explains, “Among various neural network models, recurrent neural networks (RNNs) are appealing for modeling sequential data because they can capture long term dependency in sequential data using a simple mechanism of recurrent feedback” and then it says RNNs cannot actually capture long-term dependencies that well. RNNs are appealing in the first place because they can handle variable length sequences and can model temporal relationships between each symbol in a sequence. The criticism against LSTMs is hard to accept when it says: LSTMs are slow and because of the slowness, they are hard to scale at larger tasks. But we all know that some companies are already using gigantic seq2seq models for their production (LSTMs are used as building blocks in their systems). This indicates that the LSTMs can be practically used in a very large-scale setting. Secondly, the idea proposed in the paper is incremental and not new to the field. There are other previous works that propose to use direct connections to the previous hidden states [1]. However, the previous works do not use aggregation of multiple number of previous hidden states. Most importantly, the paper fails to deliver a proper analysis on whether its main contribution is actually helpful to improve the problem posed in the paper. The new architecture is said that it handles the long-term dependencies better, however, there is no rigorous proof or intuitive design in the architecture that help us to understand why it should work better. By the design of the architecture, and speaking in very high-level, it seems like the model maybe helpful to mitigate the vanishing gradients issue by a linear factor. It is always a good practice to have at least one page to analyze the empirical findings in the paper. Thirdly, the baseline models used in this paper are very weak. Their are plenty of other models that are trained and tested on word-level language modelling task using Penn Treebank corpus, but the paper only contains a few of outdated models. I cannot fully agree on the statement “To the best of our knowledge, this is the best performance on PTB under the same training condition”, these days, RNN-based methods usually score below 80 in terms of the test perplexity, which are far lower than 100 achieved in this paper. [1] Zhang et al., “Architectural Complexity Measures of Recurrent Neural Networks”, NIPS’16
It has recently come to my attention that the objective proposed by the authors in this paper has in fact already been studied in the literature under the name 'reward-weighted regression', from e.g. ICML 2007 [1]. This has spawned several other works using the same objective (e.g. [2]). One can examine for instance the objective proposed in Section 3.4 from [2], from ICANN 2008. While this paper has already been accepted to ICLR, it would be beneficial for the authors to at least cite these works (and other related works) so that readers are aware of the previous origins of this idea. [1]
This paper proposes a nice algorithm for improved exploration in policy search RL settings. The method essentially optimizes a weighted combination of expected reward (essentially the REINFORCE objective without an entropy regularizer) plus a term from the reward augmented maximum likelihood objective (from a recent NIPS paper), and show that the resulting update can be made with a fairly small modification to the REINFORCE algorithm. The authors show improved performance on several sequential "program-like" domains like copying a string, adding, etc. I'm recommending this paper for acceptance, as I think the contribution here is a good one, and the basic approach very nicely offers a better exploration policy than the typical Boltzmann policy, using a fairly trivial modification to REINFORCE. But after re-reading I'm less enthusiastic, simply because the delta over previous work (namely the RAML paper) doesn't seem incredibly substantial. None of the domains in the experiments seem substantially challenges, and the fact that it can improve over REINFORCE isn't necessarily amazing. Pros: + Well-motivated (and simple) modification to REINFORCE to get better exploration + Demonstrably better performance with seemingly less hyperparameter tunies Cons: - Delta over RAML work isn't that clear, essentially is just a weighted combination between REINFORCE and RAML (though in RL context) - Experiments are good, but not outstanding relative to simple baselines
We thank all the reviewers for their thoughtful comments. We have made several updates to the paper in response: -- We have made adjustments to the introduction to make it clear that the motivation of the paper is twofold: (1) to introduce an RL algorithm that improves the performance and the exploration behavior of REINFORCE (2) to improve the behavior of RL methods on algorithmic tasks. -- In Section 6, we have made it clear that we only use total-episode reward for UREX and MENT. -- For more results, we have revised the paper to include an appendix with a simple bandit-like task with a large action space.
This paper proposes a novel exploration strategy that promotes exploration of under-appreciated reward regions. Proposed importance sampling based approach is a simple modification to REINFORCE and experiments in several algorithmic toy tasks show that the proposed model is performing better than REINFORCE and Q-learning. This paper shows promising results in automated algorithm discovery using reinforcement learning. However it is not very clear what is the main motivation of the paper. Is the main motivation better exploration for policy gradient methods? If so, authors should have benchmarked their algorithm with standard reinforcement learning tasks. While there is a huge body of literature on improving REINFORCE, authors have considered a simple version of REINFORCE on a non-standard task and say that UREX is better. If the main motivation is improving the performance in algorithm learning tasks, then the baselines are still weak. Authors should make it clear which is the main motivation. Also the action space is too small. In the beginning authors raise the concern that entropy regularization might not scale to larger action spaces. So a comparison of MENT and UREX in a large action space problem would give more insights on whether UREX is not affected by large action space. -------------------------- After rebuttal: I missed the action sequences argument when I pointed about small action space issue. For question regarding weak baseline, there are several tricks used in the literature to tackle high-variance issue for REINFORCE. For example, see Mnih & Gregor, 2014. I have increased my rating from 6 to 7. I still encourage the authors to improve their baseline.
overview: This work proposes to link trajectory log-probabilities and rewards by defining under-appreciated rewards. This suggests that there is a linear relationship between trajectory rewards and their log-probability which can be exploited by measuring the resulting mismatch. That is, when an action sequence under-appreciates its reward, its log-probability is increased. This method is a simple modification to the well-known REINFORCE method, requiring only one extra hyperparameter tau, and intuitively provides us with a better exploration mechanism than epsilon-greedy or random exploration. The method is tested on algorithmic environments, and compared to entropy-regularized REINFORCE and double Q-learning, and performs equally or better than those two baselines, especially in more complex environments. remarks: - the focus in the introduction on algorithmic tasks may be a double-edged sword. It is an interesting domain to test your hypothesis and benchmark your method. At the same time, it distracts the reader from the (IMO) generality of the proposed method. - in the introduction you say the reward is sparse, in section 6, on tasks 1-5, you then say there is a reward at each correct emission, i.e. each time step. This is only 'corrected' to end-of-episode-reward in section 7.4, after having discussed results. I'd move or mention this in section 6. - approach seems quite sensible to tau being in the same range as logpi(a|h), but you only try tau=0.1 for UREX. I'm not sure I understand nor agree with this experimentation choice. - an alternative to grid search is random search (Bergstra&Bengio, 2012). It may illustrate better hyperparameter robustness, and allow you to explore more in the same number of experiment. opinion: - An interesting approach to policy-gradient, to be sure. It tackles the very important question of "how should agents explore?" - I'm ambivalent to claiming that an algorithm is robust to hyperparmeters simply because it performs better on the selected hyperparameter range. All you really show is that it performs well some amount of time when the hyperparams lay in that range. Couldn't it be that MENT needs different hyperparameters? (Just being devil's advocate here) - I see why matching 1/tau with logpi is the obvious choice, but it implies a very strong prior: that the reward (to a factor of 1/tau) lies in the same space as the log policy. One point of failure I see (but correct me if I'm wrong) is that as the length of the trajectory grows the reward is expected to grow linearly, so short ways to get some reward will be less explored than long ways of getting the same reward, creating an imbalance unless the reward is shaped such that shorter trajectories get more reward (which is only the case in task 6). - It might have been good to also compare with methods explicitly trying to explore better with value-functions (e.g. prioritized experience replay, Schaul et al 2015) - At the risk of repeating myself, tau plays a major role in this method, but there is little analysis on its effect on experiments. The methodology and reasoning is clearly explained and I think this paper communicates its message very well. That message is novel, albeit a minor modification to a well-known algorithm, it is well motivated and, I think, a welcome addition to literature concerning exploration in RL. The experiments are chosen accordingly, and results seem to reflect the hypothesis of the authors. I realize the tyranny of extensive experimentation and the scarcity of time, but I do think that this paper would benefit from more (or cleverer) experimentation, as well as demonstrating more explicitly the impact of the method on exploration. Reading this paper convinced me that measuring mismatch between a trajectory's observed reward and its probability given the current policy is a clever (and well motivated) thing to do. Yet, I think that the paper could have a more convincing empirical argument, even if it is for toy tasks.
The paper proposes a new algorithm based on REINFORCE which aims at exploring under-appreciate action sequences. The idea is to compare the probability of a sequence of actions under the current policy with the estimated reward. Actions where the current policy under-estimate the reward will provide a higher feedback, thus encouraging exploration of particular sequences of actions. The UREX model is tested on 6 algortihmic RL problems and show interesting properties in comparison to the standard regularized REINFORCE (MENT) model and to Q-Learning. The model is interesting, well defined and well explained. As far as I know, the UREX model is an original model which will certainly be useful for the RL community. The only drawback of the paper is to restrict the evaluation of this algortihm to algorithmic problems that are specific while it would be easy to test the proposed model onto other standard RL problems. This would clearly help to make the article stronger and I greatly encourage the authors to add some other tasks in their paper.
Hi, I feel the weight "w_tau(ak|h)" proposed in Equation (10) is quite crucial to model the *under appreciation*. Equation (10) is linked to the optimal policy distribution proposed by Equation (5). The weight should be proportional to */_theta, and * is proposed in Equation (5) based on the rewards given optimal policy. But apparently, in Equation (10), the reward term "r" used is from the underlying policy, which is not optimal. Otherwise, it's not quite possible to compute that for gradient update. Hope you could clarify on this. Thank you!
It has recently come to my attention that the objective proposed by the authors in this paper has in fact already been studied in the literature under the name 'reward-weighted regression', from e.g. ICML 2007 [1]. This has spawned several other works using the same objective (e.g. [2]). One can examine for instance the objective proposed in Section 3.4 from [2], from ICANN 2008. While this paper has already been accepted to ICLR, it would be beneficial for the authors to at least cite these works (and other related works) so that readers are aware of the previous origins of this idea. [1]
This paper proposes a nice algorithm for improved exploration in policy search RL settings. The method essentially optimizes a weighted combination of expected reward (essentially the REINFORCE objective without an entropy regularizer) plus a term from the reward augmented maximum likelihood objective (from a recent NIPS paper), and show that the resulting update can be made with a fairly small modification to the REINFORCE algorithm. The authors show improved performance on several sequential "program-like" domains like copying a string, adding, etc. I'm recommending this paper for acceptance, as I think the contribution here is a good one, and the basic approach very nicely offers a better exploration policy than the typical Boltzmann policy, using a fairly trivial modification to REINFORCE. But after re-reading I'm less enthusiastic, simply because the delta over previous work (namely the RAML paper) doesn't seem incredibly substantial. None of the domains in the experiments seem substantially challenges, and the fact that it can improve over REINFORCE isn't necessarily amazing. Pros: + Well-motivated (and simple) modification to REINFORCE to get better exploration + Demonstrably better performance with seemingly less hyperparameter tunies Cons: - Delta over RAML work isn't that clear, essentially is just a weighted combination between REINFORCE and RAML (though in RL context) - Experiments are good, but not outstanding relative to simple baselines
We thank all the reviewers for their thoughtful comments. We have made several updates to the paper in response: -- We have made adjustments to the introduction to make it clear that the motivation of the paper is twofold: (1) to introduce an RL algorithm that improves the performance and the exploration behavior of REINFORCE (2) to improve the behavior of RL methods on algorithmic tasks. -- In Section 6, we have made it clear that we only use total-episode reward for UREX and MENT. -- For more results, we have revised the paper to include an appendix with a simple bandit-like task with a large action space.
This paper proposes a novel exploration strategy that promotes exploration of under-appreciated reward regions. Proposed importance sampling based approach is a simple modification to REINFORCE and experiments in several algorithmic toy tasks show that the proposed model is performing better than REINFORCE and Q-learning. This paper shows promising results in automated algorithm discovery using reinforcement learning. However it is not very clear what is the main motivation of the paper. Is the main motivation better exploration for policy gradient methods? If so, authors should have benchmarked their algorithm with standard reinforcement learning tasks. While there is a huge body of literature on improving REINFORCE, authors have considered a simple version of REINFORCE on a non-standard task and say that UREX is better. If the main motivation is improving the performance in algorithm learning tasks, then the baselines are still weak. Authors should make it clear which is the main motivation. Also the action space is too small. In the beginning authors raise the concern that entropy regularization might not scale to larger action spaces. So a comparison of MENT and UREX in a large action space problem would give more insights on whether UREX is not affected by large action space. -------------------------- After rebuttal: I missed the action sequences argument when I pointed about small action space issue. For question regarding weak baseline, there are several tricks used in the literature to tackle high-variance issue for REINFORCE. For example, see Mnih & Gregor, 2014. I have increased my rating from 6 to 7. I still encourage the authors to improve their baseline.
overview: This work proposes to link trajectory log-probabilities and rewards by defining under-appreciated rewards. This suggests that there is a linear relationship between trajectory rewards and their log-probability which can be exploited by measuring the resulting mismatch. That is, when an action sequence under-appreciates its reward, its log-probability is increased. This method is a simple modification to the well-known REINFORCE method, requiring only one extra hyperparameter tau, and intuitively provides us with a better exploration mechanism than epsilon-greedy or random exploration. The method is tested on algorithmic environments, and compared to entropy-regularized REINFORCE and double Q-learning, and performs equally or better than those two baselines, especially in more complex environments. remarks: - the focus in the introduction on algorithmic tasks may be a double-edged sword. It is an interesting domain to test your hypothesis and benchmark your method. At the same time, it distracts the reader from the (IMO) generality of the proposed method. - in the introduction you say the reward is sparse, in section 6, on tasks 1-5, you then say there is a reward at each correct emission, i.e. each time step. This is only 'corrected' to end-of-episode-reward in section 7.4, after having discussed results. I'd move or mention this in section 6. - approach seems quite sensible to tau being in the same range as logpi(a|h), but you only try tau=0.1 for UREX. I'm not sure I understand nor agree with this experimentation choice. - an alternative to grid search is random search (Bergstra&Bengio, 2012). It may illustrate better hyperparameter robustness, and allow you to explore more in the same number of experiment. opinion: - An interesting approach to policy-gradient, to be sure. It tackles the very important question of "how should agents explore?" - I'm ambivalent to claiming that an algorithm is robust to hyperparmeters simply because it performs better on the selected hyperparameter range. All you really show is that it performs well some amount of time when the hyperparams lay in that range. Couldn't it be that MENT needs different hyperparameters? (Just being devil's advocate here) - I see why matching 1/tau with logpi is the obvious choice, but it implies a very strong prior: that the reward (to a factor of 1/tau) lies in the same space as the log policy. One point of failure I see (but correct me if I'm wrong) is that as the length of the trajectory grows the reward is expected to grow linearly, so short ways to get some reward will be less explored than long ways of getting the same reward, creating an imbalance unless the reward is shaped such that shorter trajectories get more reward (which is only the case in task 6). - It might have been good to also compare with methods explicitly trying to explore better with value-functions (e.g. prioritized experience replay, Schaul et al 2015) - At the risk of repeating myself, tau plays a major role in this method, but there is little analysis on its effect on experiments. The methodology and reasoning is clearly explained and I think this paper communicates its message very well. That message is novel, albeit a minor modification to a well-known algorithm, it is well motivated and, I think, a welcome addition to literature concerning exploration in RL. The experiments are chosen accordingly, and results seem to reflect the hypothesis of the authors. I realize the tyranny of extensive experimentation and the scarcity of time, but I do think that this paper would benefit from more (or cleverer) experimentation, as well as demonstrating more explicitly the impact of the method on exploration. Reading this paper convinced me that measuring mismatch between a trajectory's observed reward and its probability given the current policy is a clever (and well motivated) thing to do. Yet, I think that the paper could have a more convincing empirical argument, even if it is for toy tasks.
The paper proposes a new algorithm based on REINFORCE which aims at exploring under-appreciate action sequences. The idea is to compare the probability of a sequence of actions under the current policy with the estimated reward. Actions where the current policy under-estimate the reward will provide a higher feedback, thus encouraging exploration of particular sequences of actions. The UREX model is tested on 6 algortihmic RL problems and show interesting properties in comparison to the standard regularized REINFORCE (MENT) model and to Q-Learning. The model is interesting, well defined and well explained. As far as I know, the UREX model is an original model which will certainly be useful for the RL community. The only drawback of the paper is to restrict the evaluation of this algortihm to algorithmic problems that are specific while it would be easy to test the proposed model onto other standard RL problems. This would clearly help to make the article stronger and I greatly encourage the authors to add some other tasks in their paper.
Hi, I feel the weight "w_tau(ak|h)" proposed in Equation (10) is quite crucial to model the *under appreciation*. Equation (10) is linked to the optimal policy distribution proposed by Equation (5). The weight should be proportional to */_theta, and * is proposed in Equation (5) based on the rewards given optimal policy. But apparently, in Equation (10), the reward term "r" used is from the underlying policy, which is not optimal. Otherwise, it's not quite possible to compute that for gradient update. Hope you could clarify on this. Thank you!
Pros : - New representation with nice properties that are derived and compared with a mathematical baseline and background - A simple algorithm to obtain the representation Cons : - The paper sounds like an applied maths paper, but further analysis on the nature of the representation could be done, for instance, by understanding the nature of each layer, or at least, the first.
This work proposes learning of local representations of planar curves using convolutional neural networks. Invariance to rigid transformations and discriminability are enforced with a metric learning framework using a siamese architecture. Preliminary experiments on toy datasets compare favorably with predefined geometric invariants (both differential and integral). The reviewers found value on the problem set-up and the proposed model, and were generally satisfied with the author's response. They also expressed concern that the experimental section is currently a bit weak and does not include any real data. Also, the paper does not offer theoretical insights that inform us about the design of the representation or about the provable invariance guarantees. All things considered, the AC recommends acceptance in the form of a poster, but strongly encourages the authors to strengthen the work both in the experimental and the theoretical aspects.
Based on the constructive suggestions of the reviewers, we have updated the paper with two minor changes: (1.) We have added a figure in the appendix section, showing the learned filters from the first layer of the network. (2.) We have added an additional line in Section 4: "Since we are training for a local descriptor of a curve, that is, a function whose value at a point depends only on its local neighborhood, a negative example must pair curves such that corresponding points on each curve must have different local neighborhoods", in order to highlight the locality property of our framework.
I'm torn on this one. Seeing the MPEG-7 dataset and references to curvature scale space brought to mind the old saying that "if it's not worth doing, it's not worth doing well." There is no question that the MPEG-7 dataset/benchmark got saturated long ago, and it's quite surprising to see it in a submission to a modern ML conference. I brought up the question of "why use this representation" with the authors and they said their "main purpose was to connect the theory of differential geometry of curves with the computational engine of a convolutional neural network." Fair enough. I agree these are seemingly different fields, and the authors deserve some credit for connecting them. If we give them the benefit of the doubt that this was worth doing, then the approach they pursue using a Siamese configuration makes sense, and their adaptation of deep convnet frameworks to 1D signals is reasonable. To the extent that the old invariant based methods made use of smoothed/filtered representations coupled with nonlinearities, it's sensible to revisit this problem using convnets. I wouldn't mind seeing this paper accepted, since it's different from the mainstream, but I worry about there being too narrow an audience at ICLR that still cares about this type of shape representation.
Authors show that a contrastive loss for a Siamese architecture can be used for learning representations for planar curves. With the proposed framework, authors are able to learn a representation which is comparable to traditional differential or integral invariants, as evaluated on few toy examples. The paper is generally well written and shows an interesting application of the Siamese architecture. However, the experimental evaluation and the results show that these are rather preliminary results as not many of the choices are validated. My biggest concern is in the choice of the negative samples, as the network basically learns only to distinguish between shapes at different scales, instead of recognizing different shapes. It is well known fact that in order to achieve a good performance with the contrastive loss, one has to be careful about the hard negative sampling, as using too easy negatives may lead to inferior results. Thus, this may be the underlying reason for such choice of the negatives? Unfortunately, this is not discussed in the paper. Furthermore the paper misses a more thorough quantitative evaluation and concentrates more on showing particular examples, instead of measuring more robust statistics over multiple curves (invariance to noise and sampling artifacts). In general, the paper shows interesting first steps in this direction, however it is not clear whether the experimental section is strong and thorough enough for the ICLR conference. Also the novelty of the proposed idea is limited as Siamese networks are used for many years and this work only shows that they can be applied to a different task.
Pros : - New representation with nice properties that are derived and compared with a mathematical baseline and background - A simple algorithm to obtain the representation Cons : - The paper sounds like an applied maths paper, but further analysis on the nature of the representation could be done, for instance, by understanding the nature of each layer, or at least, the first.
Pros : - New representation with nice properties that are derived and compared with a mathematical baseline and background - A simple algorithm to obtain the representation Cons : - The paper sounds like an applied maths paper, but further analysis on the nature of the representation could be done, for instance, by understanding the nature of each layer, or at least, the first.
This work proposes learning of local representations of planar curves using convolutional neural networks. Invariance to rigid transformations and discriminability are enforced with a metric learning framework using a siamese architecture. Preliminary experiments on toy datasets compare favorably with predefined geometric invariants (both differential and integral). The reviewers found value on the problem set-up and the proposed model, and were generally satisfied with the author's response. They also expressed concern that the experimental section is currently a bit weak and does not include any real data. Also, the paper does not offer theoretical insights that inform us about the design of the representation or about the provable invariance guarantees. All things considered, the AC recommends acceptance in the form of a poster, but strongly encourages the authors to strengthen the work both in the experimental and the theoretical aspects.
Based on the constructive suggestions of the reviewers, we have updated the paper with two minor changes: (1.) We have added a figure in the appendix section, showing the learned filters from the first layer of the network. (2.) We have added an additional line in Section 4: "Since we are training for a local descriptor of a curve, that is, a function whose value at a point depends only on its local neighborhood, a negative example must pair curves such that corresponding points on each curve must have different local neighborhoods", in order to highlight the locality property of our framework.
I'm torn on this one. Seeing the MPEG-7 dataset and references to curvature scale space brought to mind the old saying that "if it's not worth doing, it's not worth doing well." There is no question that the MPEG-7 dataset/benchmark got saturated long ago, and it's quite surprising to see it in a submission to a modern ML conference. I brought up the question of "why use this representation" with the authors and they said their "main purpose was to connect the theory of differential geometry of curves with the computational engine of a convolutional neural network." Fair enough. I agree these are seemingly different fields, and the authors deserve some credit for connecting them. If we give them the benefit of the doubt that this was worth doing, then the approach they pursue using a Siamese configuration makes sense, and their adaptation of deep convnet frameworks to 1D signals is reasonable. To the extent that the old invariant based methods made use of smoothed/filtered representations coupled with nonlinearities, it's sensible to revisit this problem using convnets. I wouldn't mind seeing this paper accepted, since it's different from the mainstream, but I worry about there being too narrow an audience at ICLR that still cares about this type of shape representation.
Authors show that a contrastive loss for a Siamese architecture can be used for learning representations for planar curves. With the proposed framework, authors are able to learn a representation which is comparable to traditional differential or integral invariants, as evaluated on few toy examples. The paper is generally well written and shows an interesting application of the Siamese architecture. However, the experimental evaluation and the results show that these are rather preliminary results as not many of the choices are validated. My biggest concern is in the choice of the negative samples, as the network basically learns only to distinguish between shapes at different scales, instead of recognizing different shapes. It is well known fact that in order to achieve a good performance with the contrastive loss, one has to be careful about the hard negative sampling, as using too easy negatives may lead to inferior results. Thus, this may be the underlying reason for such choice of the negatives? Unfortunately, this is not discussed in the paper. Furthermore the paper misses a more thorough quantitative evaluation and concentrates more on showing particular examples, instead of measuring more robust statistics over multiple curves (invariance to noise and sampling artifacts). In general, the paper shows interesting first steps in this direction, however it is not clear whether the experimental section is strong and thorough enough for the ICLR conference. Also the novelty of the proposed idea is limited as Siamese networks are used for many years and this work only shows that they can be applied to a different task.
Pros : - New representation with nice properties that are derived and compared with a mathematical baseline and background - A simple algorithm to obtain the representation Cons : - The paper sounds like an applied maths paper, but further analysis on the nature of the representation could be done, for instance, by understanding the nature of each layer, or at least, the first.
This paper presents a mathematical analysis of how information is propagated through deep feed-forward neural networks, with novel analysis addressing the problem of vanishing and exploding gradients in the backward pass of backpropagation and the use of the dropout algorithm. The paper is clear and well-written, the analysis is thorough, and the experimental results showing agreement with the model are very nice.
This is one of the two top papers in my stack. In total the reviews are a little bit on the light side in terms of level of detail and there are some concerns regarding how useful the results are from a practical point of view. However, I am confident that the paper should be accepted.
Just making sure, your experiments with dropout follows the convention that during evaluation, it is turned off?
I'm not familiar enough with mean-field techniques to judge the soundness of Eq 2, but I'm willing to roll with it. Minor point on presentation: Speaking of the "evolution" of x_i;a as it travels through the network could give some readers helpful intuition, but for me it was confusing because x_*;a is the immutable input vector, and it's the just-introduced z and y variables that represent its so-called evolution, no? In interpreting this analysis - A network may be trainable if information does not pass through it, if the training steps, by whatever reason, perturb the weights so that information starts to pass through it (without subsequently perturbing the weights to stop information from passing through it.) Perhaps this could be clarified by a definition of “training algorithm”? Comments on central claims: Previous work on initializing neural networks to promote information flow (e.g. Glorot & Bengio,
The paper expands a recent mean-field approximation of deep random neural networks to study depth-dependent information propagation, its phase-dependence and the influence of drop-out. The paper is extremely well written, the mathematical analysis is thorough and numerical experiments are included that underscore the theoretical results. Overall the paper stands out as one of the few papers that thoroughly analyses training and performance of deep nets.
This paper presents a mathematical analysis of how information is propagated through deep feed-forward neural networks, with novel analysis addressing the problem of vanishing and exploding gradients in the backward pass of backpropagation and the use of the dropout algorithm. The paper is clear and well-written, the analysis is thorough, and the experimental results showing agreement with the model are very nice.
This paper presents a mathematical analysis of how information is propagated through deep feed-forward neural networks, with novel analysis addressing the problem of vanishing and exploding gradients in the backward pass of backpropagation and the use of the dropout algorithm. The paper is clear and well-written, the analysis is thorough, and the experimental results showing agreement with the model are very nice.
This is one of the two top papers in my stack. In total the reviews are a little bit on the light side in terms of level of detail and there are some concerns regarding how useful the results are from a practical point of view. However, I am confident that the paper should be accepted.
Just making sure, your experiments with dropout follows the convention that during evaluation, it is turned off?
I'm not familiar enough with mean-field techniques to judge the soundness of Eq 2, but I'm willing to roll with it. Minor point on presentation: Speaking of the "evolution" of x_i;a as it travels through the network could give some readers helpful intuition, but for me it was confusing because x_*;a is the immutable input vector, and it's the just-introduced z and y variables that represent its so-called evolution, no? In interpreting this analysis - A network may be trainable if information does not pass through it, if the training steps, by whatever reason, perturb the weights so that information starts to pass through it (without subsequently perturbing the weights to stop information from passing through it.) Perhaps this could be clarified by a definition of “training algorithm”? Comments on central claims: Previous work on initializing neural networks to promote information flow (e.g. Glorot & Bengio,
The paper expands a recent mean-field approximation of deep random neural networks to study depth-dependent information propagation, its phase-dependence and the influence of drop-out. The paper is extremely well written, the mathematical analysis is thorough and numerical experiments are included that underscore the theoretical results. Overall the paper stands out as one of the few papers that thoroughly analyses training and performance of deep nets.
This paper presents a mathematical analysis of how information is propagated through deep feed-forward neural networks, with novel analysis addressing the problem of vanishing and exploding gradients in the backward pass of backpropagation and the use of the dropout algorithm. The paper is clear and well-written, the analysis is thorough, and the experimental results showing agreement with the model are very nice.
This work reframes paragraph vectors from a generative point of view and in so doing, motivates the existing method of inferring paragraph vectors as well as applying a L2 regularizer on the paragraph embeddings. The work also motivates joint learning of a classifier on the paragraph vectors to perform text classification. The paper has numerous citation issues both in formatting within the text and the formatting of the bibliography, e.g. on some occasions including first names, on others not. I suggest the authors use a software package like BibTex to have a more consistent bibliography. There seems to be little novelty in this work. The authors claim that there is no proposed method for inferring unseen documents for paragraph vectors. This is untrue. In the original paragraph vector paper, the authors show that to get a new vector, the rest of the model parameters are held fixed and gradient descent is performed on the new paragraph vector. This means the original dataset is not needed when inferring a paragraph vector for new text. This work seems to be essentially doing the same thing when finding the MAP estimate for a new vector. Thus the only contribution from the generative paragraph vector framing is the regularization on the embedding matrix. The supervised generative paragraph vector amounts to jointly training a linear classifier on the paragraph vectors, while inference for the paragraph vector is unchanged. For the n-gram based approach, the authors should cite Li et al., 2015. In the experiments, table 1 and 2 are badly formatted with .0 being truncated. The authors also do not state the size of the paragraph vector. Finally the SGPV results are actually worse than that reported in the original paragraph vector paper where SST-1 got 48.7 and SST-2 got 86.3. Bofang Li, Tao Liu, Xiaoyong Du, Deyuan Zhang, Zhe Zhao, Learning Document Embeddings by Predicting N-grams for Sentiment Classification of Long Movie Reviews, 2015.
The contribution of this paper generally boils down to adding a prior to the latent representations of the paragraph in the Paragraph Vector model. An especially problematic point about this paper is the claim that the original paper considered only the transductive setting (i.e. it could not induce representations of new documents). It is not accurate, they also used gradient descent at test time. Though I agree that regularizing the original model is a reasonable thing to do, I share the reviewers' feeling that the contribution is minimal. There are also some serious issues with presentation (as noted by the reviewers), I am surprised that the authors have not addressed them during the review period.
While this paper has some decent accuracy numbers, it is hard to argue for acceptance given the following: 1) motivation based on the incorrect assumption that the Paragraph Vector wouldn't work on unseen data 2) Numerous basic formatting and Bibtex citation issues. Lack of novelty of yet another standard directed LDA-like bag of words/bigram model.
It feels that this paper is structured around a shortcoming of the original paragraph vectors paper, namely an alleged inability to infer representation for text outside of the training data. I am reasonably sure that this is not the case. Unfortunately on that basis, the premise for the work presented here no longer holds, which renders most of the subsequent discussion void. While I recommend this paper be rejected, I encourage the authors to revisit the novel aspects of the idea presented here and see if that can be turned into a different type of paper going forward.
This work reframes paragraph vectors from a generative point of view and in so doing, motivates the existing method of inferring paragraph vectors as well as applying a L2 regularizer on the paragraph embeddings. The work also motivates joint learning of a classifier on the paragraph vectors to perform text classification. The paper has numerous citation issues both in formatting within the text and the formatting of the bibliography, e.g. on some occasions including first names, on others not. I suggest the authors use a software package like BibTex to have a more consistent bibliography. There seems to be little novelty in this work. The authors claim that there is no proposed method for inferring unseen documents for paragraph vectors. This is untrue. In the original paragraph vector paper, the authors show that to get a new vector, the rest of the model parameters are held fixed and gradient descent is performed on the new paragraph vector. This means the original dataset is not needed when inferring a paragraph vector for new text. This work seems to be essentially doing the same thing when finding the MAP estimate for a new vector. Thus the only contribution from the generative paragraph vector framing is the regularization on the embedding matrix. The supervised generative paragraph vector amounts to jointly training a linear classifier on the paragraph vectors, while inference for the paragraph vector is unchanged. For the n-gram based approach, the authors should cite Li et al., 2015. In the experiments, table 1 and 2 are badly formatted with .0 being truncated. The authors also do not state the size of the paragraph vector. Finally the SGPV results are actually worse than that reported in the original paragraph vector paper where SST-1 got 48.7 and SST-2 got 86.3. Bofang Li, Tao Liu, Xiaoyong Du, Deyuan Zhang, Zhe Zhao, Learning Document Embeddings by Predicting N-grams for Sentiment Classification of Long Movie Reviews, 2015.
This work reframes paragraph vectors from a generative point of view and in so doing, motivates the existing method of inferring paragraph vectors as well as applying a L2 regularizer on the paragraph embeddings. The work also motivates joint learning of a classifier on the paragraph vectors to perform text classification. The paper has numerous citation issues both in formatting within the text and the formatting of the bibliography, e.g. on some occasions including first names, on others not. I suggest the authors use a software package like BibTex to have a more consistent bibliography. There seems to be little novelty in this work. The authors claim that there is no proposed method for inferring unseen documents for paragraph vectors. This is untrue. In the original paragraph vector paper, the authors show that to get a new vector, the rest of the model parameters are held fixed and gradient descent is performed on the new paragraph vector. This means the original dataset is not needed when inferring a paragraph vector for new text. This work seems to be essentially doing the same thing when finding the MAP estimate for a new vector. Thus the only contribution from the generative paragraph vector framing is the regularization on the embedding matrix. The supervised generative paragraph vector amounts to jointly training a linear classifier on the paragraph vectors, while inference for the paragraph vector is unchanged. For the n-gram based approach, the authors should cite Li et al., 2015. In the experiments, table 1 and 2 are badly formatted with .0 being truncated. The authors also do not state the size of the paragraph vector. Finally the SGPV results are actually worse than that reported in the original paragraph vector paper where SST-1 got 48.7 and SST-2 got 86.3. Bofang Li, Tao Liu, Xiaoyong Du, Deyuan Zhang, Zhe Zhao, Learning Document Embeddings by Predicting N-grams for Sentiment Classification of Long Movie Reviews, 2015.
The contribution of this paper generally boils down to adding a prior to the latent representations of the paragraph in the Paragraph Vector model. An especially problematic point about this paper is the claim that the original paper considered only the transductive setting (i.e. it could not induce representations of new documents). It is not accurate, they also used gradient descent at test time. Though I agree that regularizing the original model is a reasonable thing to do, I share the reviewers' feeling that the contribution is minimal. There are also some serious issues with presentation (as noted by the reviewers), I am surprised that the authors have not addressed them during the review period.
While this paper has some decent accuracy numbers, it is hard to argue for acceptance given the following: 1) motivation based on the incorrect assumption that the Paragraph Vector wouldn't work on unseen data 2) Numerous basic formatting and Bibtex citation issues. Lack of novelty of yet another standard directed LDA-like bag of words/bigram model.
It feels that this paper is structured around a shortcoming of the original paragraph vectors paper, namely an alleged inability to infer representation for text outside of the training data. I am reasonably sure that this is not the case. Unfortunately on that basis, the premise for the work presented here no longer holds, which renders most of the subsequent discussion void. While I recommend this paper be rejected, I encourage the authors to revisit the novel aspects of the idea presented here and see if that can be turned into a different type of paper going forward.
This work reframes paragraph vectors from a generative point of view and in so doing, motivates the existing method of inferring paragraph vectors as well as applying a L2 regularizer on the paragraph embeddings. The work also motivates joint learning of a classifier on the paragraph vectors to perform text classification. The paper has numerous citation issues both in formatting within the text and the formatting of the bibliography, e.g. on some occasions including first names, on others not. I suggest the authors use a software package like BibTex to have a more consistent bibliography. There seems to be little novelty in this work. The authors claim that there is no proposed method for inferring unseen documents for paragraph vectors. This is untrue. In the original paragraph vector paper, the authors show that to get a new vector, the rest of the model parameters are held fixed and gradient descent is performed on the new paragraph vector. This means the original dataset is not needed when inferring a paragraph vector for new text. This work seems to be essentially doing the same thing when finding the MAP estimate for a new vector. Thus the only contribution from the generative paragraph vector framing is the regularization on the embedding matrix. The supervised generative paragraph vector amounts to jointly training a linear classifier on the paragraph vectors, while inference for the paragraph vector is unchanged. For the n-gram based approach, the authors should cite Li et al., 2015. In the experiments, table 1 and 2 are badly formatted with .0 being truncated. The authors also do not state the size of the paragraph vector. Finally the SGPV results are actually worse than that reported in the original paragraph vector paper where SST-1 got 48.7 and SST-2 got 86.3. Bofang Li, Tao Liu, Xiaoyong Du, Deyuan Zhang, Zhe Zhao, Learning Document Embeddings by Predicting N-grams for Sentiment Classification of Long Movie Reviews, 2015.
The paper provides a highly complex algebraic machinery to analyze the type of functions covered by convolutional network. As in most attempts in this direction in the literature, the ideal networks described in paper, which have to be interpretable as polynomials over tensors, do not match the type of CNNs used in practice: for instance the Relu non-linearity is replaced with a product of linear functions (or a sum of logs). While the paper is very technical to read, every concept is clearly stated and mathematical terminology properly introduced. Still, I think some the authors could make some effort to make the key concepts more accessible, and give a more intuitive understanding of what the separation rank means rather before piling up different mathematical interpretation. My SVM-era algebra is quite rusted, and I am not familiar with the separation rank framework: it would have been much easier for me to first fully understand a simple and gentle case (shallow network in section 5.3), than the general deep case. To summarize my understanding of the key theorem 1 result: - The upper bound of the separation rank is used to show that in the shallow case, this rank grows AT MOST linearly with the network size (as measured by the only hidden layer). So exponential network sizes are caused by this rank needing to grow exponentially, as required by the partition. - In the deep case, one also uses the case that the upper bound is linear in the size of the network (as measured by the last hidden layer), however, this situation is caused by the selection of a partition (Ilow, Jhigh), and the maximal rank induced by this partition is only linear anyway, hence the network size can remain linear. If tried my best to summarize the key point of this paper and still probably failed at it, which shows how complex is this notion of partition rank, and that its linear growth with network size can either be a good or bad thing depending on the setting. Hopefully, someone will come one day with an explanation that holds in a single slide. While this is worth publishing as conference paper in its present form, I have two suggestions that, IMHO, would make this work more significant: On the theory side, we are still very far from the completeness of the PAC bound papers of the "shallow era". In particular, the non-probabilistic lower and upper bound in theorem 1 are probably loose, and there is no PAC-like theory to tell us which one to use and what is the predicted impact on performance (not just the intuition). Also, in the prediction of the inductive bias, the other half is missing. This paper attempts to predict the maximal representation capacity of a DNN under bounded network size constraints, but one of the reason why this size has to be bounded is overfitting (justified by PAC or VC-dim like bounds). If we consider the expected risk as the sum of the empirical risk and the structural risk, this paper only seems to address fully the empirical risk minimization part, freezing the structural risk. On the practice side, an issue is that experiments in this paper mostly confirm what is obvious through intuition, or some simpler form of reasonings. For instance to use convolutions that join pixels which are symmetrical in images to detect symmetry. Basic hand-crafted pattern detectors, as they have been used in computer vision for decades, would just do the job. What would be a great motivation for using this framework is if it answered questions that simple human intuition cannot, and for which we are still in the dark: one example I could think of in the recent use of gated convolutions 'a trous' for 1D speech signal, popularized in Google WaveNet (
The paper uses the notion of separation rank from tensor algebra to analyze the correlations induced through convolution and pooling operations. They show that deep networks have exponentially larger separation ranks compared to shallow ones, and thus, can induce a much richer correlation structure compared to shallow networks. It is argued that this rich inductive bias is crucial for empirical success. The paper is technically solid. The reviewers note this, and also make a few suggestions on how to make the paper more accessible. The authors have taken this into account. In order to bridge the gap between theory and practice, it is essential for theory papers to be accessible. The paper covers related work pretty well. One aspect is misses is the recent geometric analysis of deep learning. Can the algebraic analysis be connected to geometric analysis of deep learning, e.g. in the following paper?
This paper investigates the fact why deep networks perform well in practice and how modifying the geometry of pooling can make the polynomially sized deep network to provide a function with exponentially high separation rank (for certain partitioning.) In the authors' previous works, they showed the superiority of deep networks over shallows when the activation function is ReLu and the pooling is max/mean pooling but in the current paper there is no activation function after conv and the pooling is just a multiplication of the node values. Although for the experimental results they've considered both scenarios. Actually, the general reasoning for this problem is hard, therefore, this drawback is not significant and the current contribution adds a reasonable amount of knowledge to the literature. This paper studies the convolutional arithmetic circuits and shows how this model can address the inductive biases and how pooling can adjust these biases. This interesting contribution gives an intuition about how deep network can capture the correlation between the input variables when its size is polynomial but and correlation is exponential. It worth to note that although the authors tried to express their notation and definitions carefully where they were very successful, it would be helpful if they elaborate a bit more on their definitions, expressions, and conclusions in the sense to make them more accessible.
The paper provides a highly complex algebraic machinery to analyze the type of functions covered by convolutional network. As in most attempts in this direction in the literature, the ideal networks described in paper, which have to be interpretable as polynomials over tensors, do not match the type of CNNs used in practice: for instance the Relu non-linearity is replaced with a product of linear functions (or a sum of logs). While the paper is very technical to read, every concept is clearly stated and mathematical terminology properly introduced. Still, I think some the authors could make some effort to make the key concepts more accessible, and give a more intuitive understanding of what the separation rank means rather before piling up different mathematical interpretation. My SVM-era algebra is quite rusted, and I am not familiar with the separation rank framework: it would have been much easier for me to first fully understand a simple and gentle case (shallow network in section 5.3), than the general deep case. To summarize my understanding of the key theorem 1 result: - The upper bound of the separation rank is used to show that in the shallow case, this rank grows AT MOST linearly with the network size (as measured by the only hidden layer). So exponential network sizes are caused by this rank needing to grow exponentially, as required by the partition. - In the deep case, one also uses the case that the upper bound is linear in the size of the network (as measured by the last hidden layer), however, this situation is caused by the selection of a partition (Ilow, Jhigh), and the maximal rank induced by this partition is only linear anyway, hence the network size can remain linear. If tried my best to summarize the key point of this paper and still probably failed at it, which shows how complex is this notion of partition rank, and that its linear growth with network size can either be a good or bad thing depending on the setting. Hopefully, someone will come one day with an explanation that holds in a single slide. While this is worth publishing as conference paper in its present form, I have two suggestions that, IMHO, would make this work more significant: On the theory side, we are still very far from the completeness of the PAC bound papers of the "shallow era". In particular, the non-probabilistic lower and upper bound in theorem 1 are probably loose, and there is no PAC-like theory to tell us which one to use and what is the predicted impact on performance (not just the intuition). Also, in the prediction of the inductive bias, the other half is missing. This paper attempts to predict the maximal representation capacity of a DNN under bounded network size constraints, but one of the reason why this size has to be bounded is overfitting (justified by PAC or VC-dim like bounds). If we consider the expected risk as the sum of the empirical risk and the structural risk, this paper only seems to address fully the empirical risk minimization part, freezing the structural risk. On the practice side, an issue is that experiments in this paper mostly confirm what is obvious through intuition, or some simpler form of reasonings. For instance to use convolutions that join pixels which are symmetrical in images to detect symmetry. Basic hand-crafted pattern detectors, as they have been used in computer vision for decades, would just do the job. What would be a great motivation for using this framework is if it answered questions that simple human intuition cannot, and for which we are still in the dark: one example I could think of in the recent use of gated convolutions 'a trous' for 1D speech signal, popularized in Google WaveNet (
This paper addresses the question of which functions are well suited to deep networks, as opposed to shallow networks. The basic intuition is convincing and fairly straightforward. Pooling operations bring together information. When information is correlated, it can be more efficiently used if the geometry of pooling regions matches the correlations so that it can be brought together more efficiently. Shallow networks without layers of localized pooling lack this mechanism to combine correlated information efficiently. The theoretical results are focused on convolutional arithmetic circuits, building on prior theoretical results of the authors. The results make use of the interesting technical notion of separability, which in some sense measures the degree to which a function can be represented as the composition of independent functions. Because separability is measured relative to a partition of the input, it is an appropriate mechanism for measuring the complexity of functions relative to a particular geometry of pooling operations. Many of the technical notions are pretty intuitive, although the tensor analysis is pretty terse and not easy to follow without knowledge of the authors’ prior work. In some sense the comparison between deep and shallow networks is somewhat misleading, since the shallow networks lack a hierarchical pooling structure. For example, a shallow convolutional network with RELU and max pooling does not really make sense, since the max occurs over the whole image. So it seems that the paper is really more of an analysis of the effect of pooling vs. not having pooling. For example, it is not clear that a deep CNN without pooling would be any more efficient than a shallow network, from this work. It is not clear how much the theoretical results depend on the use of a model with product pooling, and how they might be extended to the more common max pooling. Even if theoretical results are difficult to derive in this case, simple illustrative examples might be helpful. In fact, if the authors prepare a longer version of the paper for a journal I think the results could be made more intuitive if they could add a simple toy example of a function that can be efficiently represented with a convolutional arithmetic circuit when the pooling structure fits the correlations, and perhaps showing also how this could be represented with a convolutional network with RELU and max pooling. I would also appreciate a more explicit discussion of how the depth of a deep network affects the separability of functions that can be represented. A shallow network doesn’t have local pooling, so the difference between deep and shallow if perhaps mostly one of pooling vs. not pooling. However, practitioners find that very deep networks seem to be more effective than “deep” networks with only a few convolutional layers and pooling. The paper does not explicitly discuss whether their results provide insight into this behavior. Overall, I think that the paper attacks an important problem in an interesting way. It is not so convincing that this really gets to the heart of why depth is so important, because of the theoretical limitation to arithmetic circuits, and because the comparison is to shallow networks that are without localized pooling.
The paper provides a highly complex algebraic machinery to analyze the type of functions covered by convolutional network. As in most attempts in this direction in the literature, the ideal networks described in paper, which have to be interpretable as polynomials over tensors, do not match the type of CNNs used in practice: for instance the Relu non-linearity is replaced with a product of linear functions (or a sum of logs). While the paper is very technical to read, every concept is clearly stated and mathematical terminology properly introduced. Still, I think some the authors could make some effort to make the key concepts more accessible, and give a more intuitive understanding of what the separation rank means rather before piling up different mathematical interpretation. My SVM-era algebra is quite rusted, and I am not familiar with the separation rank framework: it would have been much easier for me to first fully understand a simple and gentle case (shallow network in section 5.3), than the general deep case. To summarize my understanding of the key theorem 1 result: - The upper bound of the separation rank is used to show that in the shallow case, this rank grows AT MOST linearly with the network size (as measured by the only hidden layer). So exponential network sizes are caused by this rank needing to grow exponentially, as required by the partition. - In the deep case, one also uses the case that the upper bound is linear in the size of the network (as measured by the last hidden layer), however, this situation is caused by the selection of a partition (Ilow, Jhigh), and the maximal rank induced by this partition is only linear anyway, hence the network size can remain linear. If tried my best to summarize the key point of this paper and still probably failed at it, which shows how complex is this notion of partition rank, and that its linear growth with network size can either be a good or bad thing depending on the setting. Hopefully, someone will come one day with an explanation that holds in a single slide. While this is worth publishing as conference paper in its present form, I have two suggestions that, IMHO, would make this work more significant: On the theory side, we are still very far from the completeness of the PAC bound papers of the "shallow era". In particular, the non-probabilistic lower and upper bound in theorem 1 are probably loose, and there is no PAC-like theory to tell us which one to use and what is the predicted impact on performance (not just the intuition). Also, in the prediction of the inductive bias, the other half is missing. This paper attempts to predict the maximal representation capacity of a DNN under bounded network size constraints, but one of the reason why this size has to be bounded is overfitting (justified by PAC or VC-dim like bounds). If we consider the expected risk as the sum of the empirical risk and the structural risk, this paper only seems to address fully the empirical risk minimization part, freezing the structural risk. On the practice side, an issue is that experiments in this paper mostly confirm what is obvious through intuition, or some simpler form of reasonings. For instance to use convolutions that join pixels which are symmetrical in images to detect symmetry. Basic hand-crafted pattern detectors, as they have been used in computer vision for decades, would just do the job. What would be a great motivation for using this framework is if it answered questions that simple human intuition cannot, and for which we are still in the dark: one example I could think of in the recent use of gated convolutions 'a trous' for 1D speech signal, popularized in Google WaveNet (
The paper uses the notion of separation rank from tensor algebra to analyze the correlations induced through convolution and pooling operations. They show that deep networks have exponentially larger separation ranks compared to shallow ones, and thus, can induce a much richer correlation structure compared to shallow networks. It is argued that this rich inductive bias is crucial for empirical success. The paper is technically solid. The reviewers note this, and also make a few suggestions on how to make the paper more accessible. The authors have taken this into account. In order to bridge the gap between theory and practice, it is essential for theory papers to be accessible. The paper covers related work pretty well. One aspect is misses is the recent geometric analysis of deep learning. Can the algebraic analysis be connected to geometric analysis of deep learning, e.g. in the following paper?
This paper investigates the fact why deep networks perform well in practice and how modifying the geometry of pooling can make the polynomially sized deep network to provide a function with exponentially high separation rank (for certain partitioning.) In the authors' previous works, they showed the superiority of deep networks over shallows when the activation function is ReLu and the pooling is max/mean pooling but in the current paper there is no activation function after conv and the pooling is just a multiplication of the node values. Although for the experimental results they've considered both scenarios. Actually, the general reasoning for this problem is hard, therefore, this drawback is not significant and the current contribution adds a reasonable amount of knowledge to the literature. This paper studies the convolutional arithmetic circuits and shows how this model can address the inductive biases and how pooling can adjust these biases. This interesting contribution gives an intuition about how deep network can capture the correlation between the input variables when its size is polynomial but and correlation is exponential. It worth to note that although the authors tried to express their notation and definitions carefully where they were very successful, it would be helpful if they elaborate a bit more on their definitions, expressions, and conclusions in the sense to make them more accessible.
The paper provides a highly complex algebraic machinery to analyze the type of functions covered by convolutional network. As in most attempts in this direction in the literature, the ideal networks described in paper, which have to be interpretable as polynomials over tensors, do not match the type of CNNs used in practice: for instance the Relu non-linearity is replaced with a product of linear functions (or a sum of logs). While the paper is very technical to read, every concept is clearly stated and mathematical terminology properly introduced. Still, I think some the authors could make some effort to make the key concepts more accessible, and give a more intuitive understanding of what the separation rank means rather before piling up different mathematical interpretation. My SVM-era algebra is quite rusted, and I am not familiar with the separation rank framework: it would have been much easier for me to first fully understand a simple and gentle case (shallow network in section 5.3), than the general deep case. To summarize my understanding of the key theorem 1 result: - The upper bound of the separation rank is used to show that in the shallow case, this rank grows AT MOST linearly with the network size (as measured by the only hidden layer). So exponential network sizes are caused by this rank needing to grow exponentially, as required by the partition. - In the deep case, one also uses the case that the upper bound is linear in the size of the network (as measured by the last hidden layer), however, this situation is caused by the selection of a partition (Ilow, Jhigh), and the maximal rank induced by this partition is only linear anyway, hence the network size can remain linear. If tried my best to summarize the key point of this paper and still probably failed at it, which shows how complex is this notion of partition rank, and that its linear growth with network size can either be a good or bad thing depending on the setting. Hopefully, someone will come one day with an explanation that holds in a single slide. While this is worth publishing as conference paper in its present form, I have two suggestions that, IMHO, would make this work more significant: On the theory side, we are still very far from the completeness of the PAC bound papers of the "shallow era". In particular, the non-probabilistic lower and upper bound in theorem 1 are probably loose, and there is no PAC-like theory to tell us which one to use and what is the predicted impact on performance (not just the intuition). Also, in the prediction of the inductive bias, the other half is missing. This paper attempts to predict the maximal representation capacity of a DNN under bounded network size constraints, but one of the reason why this size has to be bounded is overfitting (justified by PAC or VC-dim like bounds). If we consider the expected risk as the sum of the empirical risk and the structural risk, this paper only seems to address fully the empirical risk minimization part, freezing the structural risk. On the practice side, an issue is that experiments in this paper mostly confirm what is obvious through intuition, or some simpler form of reasonings. For instance to use convolutions that join pixels which are symmetrical in images to detect symmetry. Basic hand-crafted pattern detectors, as they have been used in computer vision for decades, would just do the job. What would be a great motivation for using this framework is if it answered questions that simple human intuition cannot, and for which we are still in the dark: one example I could think of in the recent use of gated convolutions 'a trous' for 1D speech signal, popularized in Google WaveNet (
This paper addresses the question of which functions are well suited to deep networks, as opposed to shallow networks. The basic intuition is convincing and fairly straightforward. Pooling operations bring together information. When information is correlated, it can be more efficiently used if the geometry of pooling regions matches the correlations so that it can be brought together more efficiently. Shallow networks without layers of localized pooling lack this mechanism to combine correlated information efficiently. The theoretical results are focused on convolutional arithmetic circuits, building on prior theoretical results of the authors. The results make use of the interesting technical notion of separability, which in some sense measures the degree to which a function can be represented as the composition of independent functions. Because separability is measured relative to a partition of the input, it is an appropriate mechanism for measuring the complexity of functions relative to a particular geometry of pooling operations. Many of the technical notions are pretty intuitive, although the tensor analysis is pretty terse and not easy to follow without knowledge of the authors’ prior work. In some sense the comparison between deep and shallow networks is somewhat misleading, since the shallow networks lack a hierarchical pooling structure. For example, a shallow convolutional network with RELU and max pooling does not really make sense, since the max occurs over the whole image. So it seems that the paper is really more of an analysis of the effect of pooling vs. not having pooling. For example, it is not clear that a deep CNN without pooling would be any more efficient than a shallow network, from this work. It is not clear how much the theoretical results depend on the use of a model with product pooling, and how they might be extended to the more common max pooling. Even if theoretical results are difficult to derive in this case, simple illustrative examples might be helpful. In fact, if the authors prepare a longer version of the paper for a journal I think the results could be made more intuitive if they could add a simple toy example of a function that can be efficiently represented with a convolutional arithmetic circuit when the pooling structure fits the correlations, and perhaps showing also how this could be represented with a convolutional network with RELU and max pooling. I would also appreciate a more explicit discussion of how the depth of a deep network affects the separability of functions that can be represented. A shallow network doesn’t have local pooling, so the difference between deep and shallow if perhaps mostly one of pooling vs. not pooling. However, practitioners find that very deep networks seem to be more effective than “deep” networks with only a few convolutional layers and pooling. The paper does not explicitly discuss whether their results provide insight into this behavior. Overall, I think that the paper attacks an important problem in an interesting way. It is not so convincing that this really gets to the heart of why depth is so important, because of the theoretical limitation to arithmetic circuits, and because the comparison is to shallow networks that are without localized pooling.
7 Summary: This paper describes the use of variational autoencoders for multi-view representation learning as an alternative to canonical correlation analysis (CCA), deep CCA (DCCA), and multi-view autoencoders (MVAE). Two variants of variational autoencoders (which the authors call VCCA and VCCA-private) are investigated. The method’s performances are compared on a synthetic MNIST dataset, the XRMB speech-articulation dataset, and the MIR-Flickr dataset. Review: Variational autoencoders are widely used and their performance for multi-view representation learning should be of interest to the ICLR community. The paper is well written and clear. The experiments are thorough. It is interesting that the performance of MVAE and VCCA is quite different given the similarity of their objective functions. I further find the analyses of the effects of dropout and private variables useful. As the authors point out, “VCCA does not optimize the same criterion, nor produce the same solution, as any linear or nonlinear CCA”. It would have been interesting to discuss the differences of a linear variant of VCCA and linear CCA, and to compare it quantitatively. While it might not make sense to use variational inference in the linear case, it would nevertheless help to understand the differences better. The derivations in Equation 3 and Equation 13 seem unnecessarily detailed given that VCCA and VCCA-p are special cases of VAE, only with certain architectural constraints. Perhaps move to the Appendix? In Section 3 the authors claim that “if we are able to generate realistic samples from the learned distribution, we can infer that we have discovered the underlying structure of the data”. This is not correct, a model which hasn’t learned a thing can have perfectly realistic samples (see Theis et al., 2016). Please remove or revise the sentence. Minor: In the equation between Equation 8 and 9, using notation N(x; g(z, theta), I) as in Equation 6 would make it clearer.
The application of VAEs to multiview settings, snd the general problem of learning representations over multiple modalities is of increasing importance. After discussion and reviewing the rebuttals, the reviewers felt that the paper is not yet ready. The application is strong, but that more can be done in terms of praticality, generalisation using other posteriors, and the comparisons made. For this reason, the paper is unfortunately not yet ready for inclusion in this year's proceedings.
Dear reviewers, We have modified our response to your review, acknowledging the related work pointed out by several reviewers, and clarifying our main contributions. Thanks!
This paper considers the case where multiple views of data are learned through a probabilistic deep neural network formulation. This makes the model non-linear (unlike e.g. CCA) but makes inference difficult. Therefore, the VAE framework is invoked for inference. In [Ref 1] the authors show that maximum likelihood estimation based on their linear latent model leads to the canonical correlation directions. But in the non-linear case with DNNs it's not clear (at least with the present analysis) what the solution is wrt to the canonical directions. There's no such analysis in the paper, hence I find it a stretch to refer to this model as a CCA type of model. In contrast, e.g. DCCA / DCCAE are taking the canonical correlation between features into account inside the objective and provide interpretations. [Ref 1] F. R. Bach and M. I. Jordan. A probabilistic interpretation of canonical correlation analysis. Technical Report 688, 2005. There is also a significant body of very related work on non-linear multi-view models which is not discussed in this paper. For example, there's been probabilistic non-linear multi-view models [Ref 2, 3], also extended to the Bayesian case with common/private spaces [Ref 4] and the variational / deep learning case [Ref 5]. [Ref 2] Ek et al. Gaussian process latent variable models for human pose estimation. MLMI, 2007. [Ref 3] Shon et al. Learning shared latent structure for image synthesis and robotic imitation. NIPS, 2006. [Ref 4] Damianou et al. Manifold relevance determination. ICML, 2012. [Ref 5] Damianou and Lawrence. Deep Gaussian processes. AISTATS, 2013. I can see the utility of this model as bringing together two elements: multi-view modeling and VAEs. This seems like an obvious idea but to the best of my knowledge it hasn't been done before and is actually a potentially very useful model. However, the question is, what is the proper way of extending VAE to multiple views? The paper didn't convince me that VAE can work well with multiple views using the shown straightforward construction. Specifically, VCCA doesn't seem to promote the state of the art in terms of results (it actually is overall below the SOA), while the VCCA-private seems a quite ill-posed model: the dimensionalities d have to be manually tuned with exhaustive search; further, the actual model does not provide a consinstent way of encouraging the private and common variables to avoid learning redundant information. Relying only on dropout for this seems a quite ad-hoc solution (in fact, from Fig. 4 (ver2) it seems that the dropout rate is quite crucial). Perhaps good performance might be achieved with a lot of tuning (which might be why the FLICKR results got better in ver2 without changing the model), but it seems quite difficult to optimize for the above reasons. From a purely experimental point of view, VCCA-private doesn't seem to promote the SOA either. Of course one wouldn't expect any new published paper to beat all previous baselines, but it seems that extension of VAE to multiple views is a very interesting idea which deserves some more investigation of how to do it efficiently. Another issue is the approximate posterior being parameterized only from one of the views. This makes the model less useful as a generic multi-view model, since it will misbehave in tasks other than classification. But if classification is the main objective, then one should compare to a proper classification model, e.g. a feedforward neural network. The plots of Fig. 8 are very nice. Overall, the paper convinced me that there is merit in attaching multiple views to VAE. However, it didn't convince me a) that the proposed way to achieve this is practical b) that there is a connection to CCA (other than being a method for multiple views). The bottom line is that, although the paper is interesting, it needs a little more work.
UPDATE: I have read the replies on this thread. My opinion has not changed. The authors propose deep VCCA, a deep version of the probabilistic CCA model by using likelihoods parameterized by nonlinear functions (neural nets). Variational inference is applied with an inference network and reparameterization gradients. An additional variant, termed VCCA-private, is also introduced, which includes local latent variables for each data point (view). A connection to the multi view auto encoder is also shown. Since the development of black box variational inference and variational auto-encoders, the methodology in model-specific papers like this one are arguably not very interesting. The model is a straightforward extension of probabilistic CCA with neural net parameterized likelihoods. Inference is mechanically the same as any black box approach using the reparameterization gradient and inference networks. The approach also uses a mean-field approximation, which is quite old given the many recent developments in more expressive approximations (see, e.g., Rezende and Mohamed (2015); Tran et al. (2016)). The connection to multi-view auto encoders is at first insightful, but no more than the difference between MAP and variational inference. This is a well-known insight: in the abstract, the authors argue that the key distinction is the additional sampling, but ultimately what matters is the KL regularizer. Even with noisy samples, the variances of a normal variational approximation would collapse to zero and thus become a point mass approximation, equivalent to optimizing a point estimate from the MVAE objective. (I suspect the authors know this to some degree due to their remarks in the paper, but it is unclear.) That said, I think the paper has strong merits in application. The experiments are strong, comparing to alternative multi-view approaches under a number of interesting data sets. While the use of "private variables" is simple, they demonstrate how it can successfully disentangle the per-view latent representation from the shared view. It would have been preferable to compare to methods using probabilistic inference, such as full Bayes for the linear CCA. There are also a number of approximations taken to almost be standard in the paper which may not be necessary, such as the use of a mean-field family or the use of an inference network. To separate out how much the approximate inference is influencing the fit of the model, I strongly recommend using MCMC and non-amortized variational inference on at least one experiment. + Rezende, D. J., & Mohamed, S. (2015). Variational Inference with Normalizing Flows. Presented at the International Conference on Machine Learning. + Tran, D., Ranganath, R., & Blei, D. M. (2016). The Variational Gaussian Process. Presented at the International Conference on Learning Representations.
7 Summary: This paper describes the use of variational autoencoders for multi-view representation learning as an alternative to canonical correlation analysis (CCA), deep CCA (DCCA), and multi-view autoencoders (MVAE). Two variants of variational autoencoders (which the authors call VCCA and VCCA-private) are investigated. The method’s performances are compared on a synthetic MNIST dataset, the XRMB speech-articulation dataset, and the MIR-Flickr dataset. Review: Variational autoencoders are widely used and their performance for multi-view representation learning should be of interest to the ICLR community. The paper is well written and clear. The experiments are thorough. It is interesting that the performance of MVAE and VCCA is quite different given the similarity of their objective functions. I further find the analyses of the effects of dropout and private variables useful. As the authors point out, “VCCA does not optimize the same criterion, nor produce the same solution, as any linear or nonlinear CCA”. It would have been interesting to discuss the differences of a linear variant of VCCA and linear CCA, and to compare it quantitatively. While it might not make sense to use variational inference in the linear case, it would nevertheless help to understand the differences better. The derivations in Equation 3 and Equation 13 seem unnecessarily detailed given that VCCA and VCCA-p are special cases of VAE, only with certain architectural constraints. Perhaps move to the Appendix? In Section 3 the authors claim that “if we are able to generate realistic samples from the learned distribution, we can infer that we have discovered the underlying structure of the data”. This is not correct, a model which hasn’t learned a thing can have perfectly realistic samples (see Theis et al., 2016). Please remove or revise the sentence. Minor: In the equation between Equation 8 and 9, using notation N(x; g(z, theta), I) as in Equation 6 would make it clearer.
Dear reviewers, We just uploaded a slightly modified version. This version -- contains the updated results on Flickr and the proposed methods significantly outperform others -- reorganized the figures and tables to make the paper more compact Thanks!
7 Summary: This paper describes the use of variational autoencoders for multi-view representation learning as an alternative to canonical correlation analysis (CCA), deep CCA (DCCA), and multi-view autoencoders (MVAE). Two variants of variational autoencoders (which the authors call VCCA and VCCA-private) are investigated. The method’s performances are compared on a synthetic MNIST dataset, the XRMB speech-articulation dataset, and the MIR-Flickr dataset. Review: Variational autoencoders are widely used and their performance for multi-view representation learning should be of interest to the ICLR community. The paper is well written and clear. The experiments are thorough. It is interesting that the performance of MVAE and VCCA is quite different given the similarity of their objective functions. I further find the analyses of the effects of dropout and private variables useful. As the authors point out, “VCCA does not optimize the same criterion, nor produce the same solution, as any linear or nonlinear CCA”. It would have been interesting to discuss the differences of a linear variant of VCCA and linear CCA, and to compare it quantitatively. While it might not make sense to use variational inference in the linear case, it would nevertheless help to understand the differences better. The derivations in Equation 3 and Equation 13 seem unnecessarily detailed given that VCCA and VCCA-p are special cases of VAE, only with certain architectural constraints. Perhaps move to the Appendix? In Section 3 the authors claim that “if we are able to generate realistic samples from the learned distribution, we can infer that we have discovered the underlying structure of the data”. This is not correct, a model which hasn’t learned a thing can have perfectly realistic samples (see Theis et al., 2016). Please remove or revise the sentence. Minor: In the equation between Equation 8 and 9, using notation N(x; g(z, theta), I) as in Equation 6 would make it clearer.
The application of VAEs to multiview settings, snd the general problem of learning representations over multiple modalities is of increasing importance. After discussion and reviewing the rebuttals, the reviewers felt that the paper is not yet ready. The application is strong, but that more can be done in terms of praticality, generalisation using other posteriors, and the comparisons made. For this reason, the paper is unfortunately not yet ready for inclusion in this year's proceedings.
Dear reviewers, We have modified our response to your review, acknowledging the related work pointed out by several reviewers, and clarifying our main contributions. Thanks!
This paper considers the case where multiple views of data are learned through a probabilistic deep neural network formulation. This makes the model non-linear (unlike e.g. CCA) but makes inference difficult. Therefore, the VAE framework is invoked for inference. In [Ref 1] the authors show that maximum likelihood estimation based on their linear latent model leads to the canonical correlation directions. But in the non-linear case with DNNs it's not clear (at least with the present analysis) what the solution is wrt to the canonical directions. There's no such analysis in the paper, hence I find it a stretch to refer to this model as a CCA type of model. In contrast, e.g. DCCA / DCCAE are taking the canonical correlation between features into account inside the objective and provide interpretations. [Ref 1] F. R. Bach and M. I. Jordan. A probabilistic interpretation of canonical correlation analysis. Technical Report 688, 2005. There is also a significant body of very related work on non-linear multi-view models which is not discussed in this paper. For example, there's been probabilistic non-linear multi-view models [Ref 2, 3], also extended to the Bayesian case with common/private spaces [Ref 4] and the variational / deep learning case [Ref 5]. [Ref 2] Ek et al. Gaussian process latent variable models for human pose estimation. MLMI, 2007. [Ref 3] Shon et al. Learning shared latent structure for image synthesis and robotic imitation. NIPS, 2006. [Ref 4] Damianou et al. Manifold relevance determination. ICML, 2012. [Ref 5] Damianou and Lawrence. Deep Gaussian processes. AISTATS, 2013. I can see the utility of this model as bringing together two elements: multi-view modeling and VAEs. This seems like an obvious idea but to the best of my knowledge it hasn't been done before and is actually a potentially very useful model. However, the question is, what is the proper way of extending VAE to multiple views? The paper didn't convince me that VAE can work well with multiple views using the shown straightforward construction. Specifically, VCCA doesn't seem to promote the state of the art in terms of results (it actually is overall below the SOA), while the VCCA-private seems a quite ill-posed model: the dimensionalities d have to be manually tuned with exhaustive search; further, the actual model does not provide a consinstent way of encouraging the private and common variables to avoid learning redundant information. Relying only on dropout for this seems a quite ad-hoc solution (in fact, from Fig. 4 (ver2) it seems that the dropout rate is quite crucial). Perhaps good performance might be achieved with a lot of tuning (which might be why the FLICKR results got better in ver2 without changing the model), but it seems quite difficult to optimize for the above reasons. From a purely experimental point of view, VCCA-private doesn't seem to promote the SOA either. Of course one wouldn't expect any new published paper to beat all previous baselines, but it seems that extension of VAE to multiple views is a very interesting idea which deserves some more investigation of how to do it efficiently. Another issue is the approximate posterior being parameterized only from one of the views. This makes the model less useful as a generic multi-view model, since it will misbehave in tasks other than classification. But if classification is the main objective, then one should compare to a proper classification model, e.g. a feedforward neural network. The plots of Fig. 8 are very nice. Overall, the paper convinced me that there is merit in attaching multiple views to VAE. However, it didn't convince me a) that the proposed way to achieve this is practical b) that there is a connection to CCA (other than being a method for multiple views). The bottom line is that, although the paper is interesting, it needs a little more work.
UPDATE: I have read the replies on this thread. My opinion has not changed. The authors propose deep VCCA, a deep version of the probabilistic CCA model by using likelihoods parameterized by nonlinear functions (neural nets). Variational inference is applied with an inference network and reparameterization gradients. An additional variant, termed VCCA-private, is also introduced, which includes local latent variables for each data point (view). A connection to the multi view auto encoder is also shown. Since the development of black box variational inference and variational auto-encoders, the methodology in model-specific papers like this one are arguably not very interesting. The model is a straightforward extension of probabilistic CCA with neural net parameterized likelihoods. Inference is mechanically the same as any black box approach using the reparameterization gradient and inference networks. The approach also uses a mean-field approximation, which is quite old given the many recent developments in more expressive approximations (see, e.g., Rezende and Mohamed (2015); Tran et al. (2016)). The connection to multi-view auto encoders is at first insightful, but no more than the difference between MAP and variational inference. This is a well-known insight: in the abstract, the authors argue that the key distinction is the additional sampling, but ultimately what matters is the KL regularizer. Even with noisy samples, the variances of a normal variational approximation would collapse to zero and thus become a point mass approximation, equivalent to optimizing a point estimate from the MVAE objective. (I suspect the authors know this to some degree due to their remarks in the paper, but it is unclear.) That said, I think the paper has strong merits in application. The experiments are strong, comparing to alternative multi-view approaches under a number of interesting data sets. While the use of "private variables" is simple, they demonstrate how it can successfully disentangle the per-view latent representation from the shared view. It would have been preferable to compare to methods using probabilistic inference, such as full Bayes for the linear CCA. There are also a number of approximations taken to almost be standard in the paper which may not be necessary, such as the use of a mean-field family or the use of an inference network. To separate out how much the approximate inference is influencing the fit of the model, I strongly recommend using MCMC and non-amortized variational inference on at least one experiment. + Rezende, D. J., & Mohamed, S. (2015). Variational Inference with Normalizing Flows. Presented at the International Conference on Machine Learning. + Tran, D., Ranganath, R., & Blei, D. M. (2016). The Variational Gaussian Process. Presented at the International Conference on Learning Representations.
7 Summary: This paper describes the use of variational autoencoders for multi-view representation learning as an alternative to canonical correlation analysis (CCA), deep CCA (DCCA), and multi-view autoencoders (MVAE). Two variants of variational autoencoders (which the authors call VCCA and VCCA-private) are investigated. The method’s performances are compared on a synthetic MNIST dataset, the XRMB speech-articulation dataset, and the MIR-Flickr dataset. Review: Variational autoencoders are widely used and their performance for multi-view representation learning should be of interest to the ICLR community. The paper is well written and clear. The experiments are thorough. It is interesting that the performance of MVAE and VCCA is quite different given the similarity of their objective functions. I further find the analyses of the effects of dropout and private variables useful. As the authors point out, “VCCA does not optimize the same criterion, nor produce the same solution, as any linear or nonlinear CCA”. It would have been interesting to discuss the differences of a linear variant of VCCA and linear CCA, and to compare it quantitatively. While it might not make sense to use variational inference in the linear case, it would nevertheless help to understand the differences better. The derivations in Equation 3 and Equation 13 seem unnecessarily detailed given that VCCA and VCCA-p are special cases of VAE, only with certain architectural constraints. Perhaps move to the Appendix? In Section 3 the authors claim that “if we are able to generate realistic samples from the learned distribution, we can infer that we have discovered the underlying structure of the data”. This is not correct, a model which hasn’t learned a thing can have perfectly realistic samples (see Theis et al., 2016). Please remove or revise the sentence. Minor: In the equation between Equation 8 and 9, using notation N(x; g(z, theta), I) as in Equation 6 would make it clearer.
Dear reviewers, We just uploaded a slightly modified version. This version -- contains the updated results on Flickr and the proposed methods significantly outperform others -- reorganized the figures and tables to make the paper more compact Thanks!
The paper introduces a time dependent recommender system based on point processes parametrized by time dependent user and item latent representations. The later are modeled as coupled – autoregressive processes – i.e. the representation of a user/item changes when he interacts with an item/user, and is a function of both the user and the item representations before time t. This is called coevolution here and the autoregressive process is called recurrent NN. The model may also incorporate heterogeneous inputs. Experiments are performed on several datasets, and the model is compared with different baselines. There are several contributions in the paper: 1) modeling recommendation via parametrized point processes where the parameter dynamics are modeled by latent user/item representations, 2) an optimization algorithm for maximizing the likelihood of this process, with different technical tricks that seem to break its intrinsic complexity, 3) evaluation experiments for time dependent recommendation. The paper by the same authors (NIPS 2016) describes a similar model of continuous time coevolution, and a similar evaluation. The difference lies in the details of the model: the point process model is not the same and of the latent factor dynamic model is slightly different, but the modeling approach and the arguments are exactly the same. By the end, one does not know what makes this model perform better than the one proposed in NIPS, is it the choice for the process, the new parametrization? Both are quite similar. There is no justification on the choice of the specific form of the point process in the two papers. Did the authors tried other forms as well? The same remark applies for the form of the dynamical process: the non-linearity used for the modeling of the latent user/item vectors here is limited to a sigmoid function, which probably does not change much w.r.t. a linear model, but there is no evidence of the role of this non linearity in the paper. Note that there are some inconsistencies between the results in the two papers. Concerning the evaluation, the authors introduce two criteria. I did not get exactly how they evaluate the item recommendation: it is mentioned that at each time t, the model predicts the item the user will interact with. Do you mean, the next item the user will interact with after time t? For the time prediction, why is it a relevant metric for recommendation? A comparison of the complexity, or execution time of the different methods would be helpful. The complexity of your method is apparently proportional to #items*#users, what are the complexity limits of your methods. Overall, the paper is quite nice and looks technically sound, albeit many details are missing. On the other hand, I have a mixed feeling because of the similarity with NIPS paper. The authors should have make a better work at convincing us that this is not a marginal extension of previous work by the authors. I was not convinced either by the evaluation criteria and there is no evidence that the model can be used for large datasets.
A nice paper, with sufficient experimental validation, and the idea of incorporating a form of change point detection is good. However, the technical contribution relative to the NIPS paper by the same authors is not significant, in that it primarily involves using an RNN instead of a Hawkes process to model the temporal dynamics. The results are significantly better than this earlier paper -- the authors should explore if this is due only to the RNN, or to the optimization method.
Dear reviewers, we have revised our paper according to your insightful suggestions and comments. 1) We highlight the importance and difficulty of modeling the nonlinearity in the point process models in the introduction part. 2) We added the discussion with Chen et.al ICML 2013 and a detailed comparison with Wang et.al NIPS 2016. 3) We added the experiment on the large Yelp dataset, which contains 1,005 users and 47,924 items. We run our algorithm and all the baselines on this one. The result of our algorithm is consistently better than alternatives. 4) We added the section 6.4.1 explaining the quantitative results we get in the experiment. Specifically, we studied the performance of different history length of users, and how the diversity of tastes of users affect the results by visualizing the user-item interaction graph. 5) We also added the section 6.4.2 that quantitatively compare the effect of different history length on Yelp dataset.
The paper introduces a time dependent recommender system based on point processes parametrized by time dependent user and item latent representations. The later are modeled as coupled – autoregressive processes – i.e. the representation of a user/item changes when he interacts with an item/user, and is a function of both the user and the item representations before time t. This is called coevolution here and the autoregressive process is called recurrent NN. The model may also incorporate heterogeneous inputs. Experiments are performed on several datasets, and the model is compared with different baselines. There are several contributions in the paper: 1) modeling recommendation via parametrized point processes where the parameter dynamics are modeled by latent user/item representations, 2) an optimization algorithm for maximizing the likelihood of this process, with different technical tricks that seem to break its intrinsic complexity, 3) evaluation experiments for time dependent recommendation. The paper by the same authors (NIPS 2016) describes a similar model of continuous time coevolution, and a similar evaluation. The difference lies in the details of the model: the point process model is not the same and of the latent factor dynamic model is slightly different, but the modeling approach and the arguments are exactly the same. By the end, one does not know what makes this model perform better than the one proposed in NIPS, is it the choice for the process, the new parametrization? Both are quite similar. There is no justification on the choice of the specific form of the point process in the two papers. Did the authors tried other forms as well? The same remark applies for the form of the dynamical process: the non-linearity used for the modeling of the latent user/item vectors here is limited to a sigmoid function, which probably does not change much w.r.t. a linear model, but there is no evidence of the role of this non linearity in the paper. Note that there are some inconsistencies between the results in the two papers. Concerning the evaluation, the authors introduce two criteria. I did not get exactly how they evaluate the item recommendation: it is mentioned that at each time t, the model predicts the item the user will interact with. Do you mean, the next item the user will interact with after time t? For the time prediction, why is it a relevant metric for recommendation? A comparison of the complexity, or execution time of the different methods would be helpful. The complexity of your method is apparently proportional to #items*#users, what are the complexity limits of your methods. Overall, the paper is quite nice and looks technically sound, albeit many details are missing. On the other hand, I have a mixed feeling because of the similarity with NIPS paper. The authors should have make a better work at convincing us that this is not a marginal extension of previous work by the authors. I was not convinced either by the evaluation criteria and there is no evidence that the model can be used for large datasets.
This paper proposes a method to model time changing dynamics in collaborative filtering. Comments: 1) The main idea of the paper is build upon similar to a previous work by the same group of author (Wang et.al KDD), the major difference appears to be change some of the latent factors to be RNN 2) The author describes a BPTT technique to train the model 3) The author introduced time prediction as a new metric to evaluate the effectiveness of time dependent model. However, this need to be condition on a given user-item pair. 4) It would be interesting to consider other metrics, for example - The switching time where a user changes his/her to another item - Jointly predict the next item and switching time. In summary, this is a paper that improves over an existing work on time dynamics model in recommender system. The time prediction metric is interesting and opens up interesting discussion on how we should evaluate recommender systems when time is involved (see also comments).
The paper seeks to predict user events (interactions with items at a particular point in time). Roughly speaking the contributions are as follows: (a) the paper models the co-evolutionary process of users' preferences toward items (b) the paper is able to incorporate external sources of information, such as user and item features (c) the process proposed is generative, so is able to estimate specific time-points at which events occur (d) the model is able to account for non-linearities in the above Following the pre-review questions, I understand that it is the combination of (a) and (c) that is the most novel aspect of the paper. A fully generative process which can be sampled is certainly nice (though of course, non-generative processes like regular old regression can estimate specific time points and such too, so not sure in practice how relevant this distinction is). Other than that the above parts have all appeared in some combination in previous work, though the combination of parts here certainly passes the novelty bar. I hadn't quite followed the issue mentioned in the pre-review discussion that the model requires multiple interactions per userXitem pair in order to fit the model (e.g. a user interacts with the same business multiple times). This is a slightly unusual setting compared to most temporal recommender systems work. I question to some extent whether this problem setting isn't a bit restrictive. That being said I take the point about why the authors had to subsample the Yelp data, but keeping only users with "hundreds" of events means that you're left with a very biased sample of the user base. Other than the above issues, the paper is technically nice, and the experiments include strong baselines and reports good performance.
Nice work, but is seems to me that it was already published at DLRS 2016 in September (
The paper introduces a time dependent recommender system based on point processes parametrized by time dependent user and item latent representations. The later are modeled as coupled – autoregressive processes – i.e. the representation of a user/item changes when he interacts with an item/user, and is a function of both the user and the item representations before time t. This is called coevolution here and the autoregressive process is called recurrent NN. The model may also incorporate heterogeneous inputs. Experiments are performed on several datasets, and the model is compared with different baselines. There are several contributions in the paper: 1) modeling recommendation via parametrized point processes where the parameter dynamics are modeled by latent user/item representations, 2) an optimization algorithm for maximizing the likelihood of this process, with different technical tricks that seem to break its intrinsic complexity, 3) evaluation experiments for time dependent recommendation. The paper by the same authors (NIPS 2016) describes a similar model of continuous time coevolution, and a similar evaluation. The difference lies in the details of the model: the point process model is not the same and of the latent factor dynamic model is slightly different, but the modeling approach and the arguments are exactly the same. By the end, one does not know what makes this model perform better than the one proposed in NIPS, is it the choice for the process, the new parametrization? Both are quite similar. There is no justification on the choice of the specific form of the point process in the two papers. Did the authors tried other forms as well? The same remark applies for the form of the dynamical process: the non-linearity used for the modeling of the latent user/item vectors here is limited to a sigmoid function, which probably does not change much w.r.t. a linear model, but there is no evidence of the role of this non linearity in the paper. Note that there are some inconsistencies between the results in the two papers. Concerning the evaluation, the authors introduce two criteria. I did not get exactly how they evaluate the item recommendation: it is mentioned that at each time t, the model predicts the item the user will interact with. Do you mean, the next item the user will interact with after time t? For the time prediction, why is it a relevant metric for recommendation? A comparison of the complexity, or execution time of the different methods would be helpful. The complexity of your method is apparently proportional to #items*#users, what are the complexity limits of your methods. Overall, the paper is quite nice and looks technically sound, albeit many details are missing. On the other hand, I have a mixed feeling because of the similarity with NIPS paper. The authors should have make a better work at convincing us that this is not a marginal extension of previous work by the authors. I was not convinced either by the evaluation criteria and there is no evidence that the model can be used for large datasets.
A nice paper, with sufficient experimental validation, and the idea of incorporating a form of change point detection is good. However, the technical contribution relative to the NIPS paper by the same authors is not significant, in that it primarily involves using an RNN instead of a Hawkes process to model the temporal dynamics. The results are significantly better than this earlier paper -- the authors should explore if this is due only to the RNN, or to the optimization method.
Dear reviewers, we have revised our paper according to your insightful suggestions and comments. 1) We highlight the importance and difficulty of modeling the nonlinearity in the point process models in the introduction part. 2) We added the discussion with Chen et.al ICML 2013 and a detailed comparison with Wang et.al NIPS 2016. 3) We added the experiment on the large Yelp dataset, which contains 1,005 users and 47,924 items. We run our algorithm and all the baselines on this one. The result of our algorithm is consistently better than alternatives. 4) We added the section 6.4.1 explaining the quantitative results we get in the experiment. Specifically, we studied the performance of different history length of users, and how the diversity of tastes of users affect the results by visualizing the user-item interaction graph. 5) We also added the section 6.4.2 that quantitatively compare the effect of different history length on Yelp dataset.
The paper introduces a time dependent recommender system based on point processes parametrized by time dependent user and item latent representations. The later are modeled as coupled – autoregressive processes – i.e. the representation of a user/item changes when he interacts with an item/user, and is a function of both the user and the item representations before time t. This is called coevolution here and the autoregressive process is called recurrent NN. The model may also incorporate heterogeneous inputs. Experiments are performed on several datasets, and the model is compared with different baselines. There are several contributions in the paper: 1) modeling recommendation via parametrized point processes where the parameter dynamics are modeled by latent user/item representations, 2) an optimization algorithm for maximizing the likelihood of this process, with different technical tricks that seem to break its intrinsic complexity, 3) evaluation experiments for time dependent recommendation. The paper by the same authors (NIPS 2016) describes a similar model of continuous time coevolution, and a similar evaluation. The difference lies in the details of the model: the point process model is not the same and of the latent factor dynamic model is slightly different, but the modeling approach and the arguments are exactly the same. By the end, one does not know what makes this model perform better than the one proposed in NIPS, is it the choice for the process, the new parametrization? Both are quite similar. There is no justification on the choice of the specific form of the point process in the two papers. Did the authors tried other forms as well? The same remark applies for the form of the dynamical process: the non-linearity used for the modeling of the latent user/item vectors here is limited to a sigmoid function, which probably does not change much w.r.t. a linear model, but there is no evidence of the role of this non linearity in the paper. Note that there are some inconsistencies between the results in the two papers. Concerning the evaluation, the authors introduce two criteria. I did not get exactly how they evaluate the item recommendation: it is mentioned that at each time t, the model predicts the item the user will interact with. Do you mean, the next item the user will interact with after time t? For the time prediction, why is it a relevant metric for recommendation? A comparison of the complexity, or execution time of the different methods would be helpful. The complexity of your method is apparently proportional to #items*#users, what are the complexity limits of your methods. Overall, the paper is quite nice and looks technically sound, albeit many details are missing. On the other hand, I have a mixed feeling because of the similarity with NIPS paper. The authors should have make a better work at convincing us that this is not a marginal extension of previous work by the authors. I was not convinced either by the evaluation criteria and there is no evidence that the model can be used for large datasets.
This paper proposes a method to model time changing dynamics in collaborative filtering. Comments: 1) The main idea of the paper is build upon similar to a previous work by the same group of author (Wang et.al KDD), the major difference appears to be change some of the latent factors to be RNN 2) The author describes a BPTT technique to train the model 3) The author introduced time prediction as a new metric to evaluate the effectiveness of time dependent model. However, this need to be condition on a given user-item pair. 4) It would be interesting to consider other metrics, for example - The switching time where a user changes his/her to another item - Jointly predict the next item and switching time. In summary, this is a paper that improves over an existing work on time dynamics model in recommender system. The time prediction metric is interesting and opens up interesting discussion on how we should evaluate recommender systems when time is involved (see also comments).
The paper seeks to predict user events (interactions with items at a particular point in time). Roughly speaking the contributions are as follows: (a) the paper models the co-evolutionary process of users' preferences toward items (b) the paper is able to incorporate external sources of information, such as user and item features (c) the process proposed is generative, so is able to estimate specific time-points at which events occur (d) the model is able to account for non-linearities in the above Following the pre-review questions, I understand that it is the combination of (a) and (c) that is the most novel aspect of the paper. A fully generative process which can be sampled is certainly nice (though of course, non-generative processes like regular old regression can estimate specific time points and such too, so not sure in practice how relevant this distinction is). Other than that the above parts have all appeared in some combination in previous work, though the combination of parts here certainly passes the novelty bar. I hadn't quite followed the issue mentioned in the pre-review discussion that the model requires multiple interactions per userXitem pair in order to fit the model (e.g. a user interacts with the same business multiple times). This is a slightly unusual setting compared to most temporal recommender systems work. I question to some extent whether this problem setting isn't a bit restrictive. That being said I take the point about why the authors had to subsample the Yelp data, but keeping only users with "hundreds" of events means that you're left with a very biased sample of the user base. Other than the above issues, the paper is technically nice, and the experiments include strong baselines and reports good performance.
Nice work, but is seems to me that it was already published at DLRS 2016 in September (
This paper explores the topic of intrinsic motivation in the context of deep RL. It proposes a couple of variants derived from an auxiliary model-learning process (prediction error, surprise and learning progress), and shows that those can help exploration on a number of continuous control tasks (and the Atari game “venture”, maybe). Novelty: none of the proposed types of intrinsic motivation are novel, and it’s arguable whether the application to deep RL is novel (see e.g. Kompella et al 2012). Potential: the idea of seeking out states where a transition model is uncertain is sensible, but also limited -- I would encourage the authors to also discuss the limitations. For example in a game like Go the transition model is trivially learned, so this approach would revert to random exploration. So other forms of learning progress or surprise derived from the agent’s competence instead might be more promising in the long run? See also Srivastava et al 2012 for further thoughts. Computation time: I find the paper’s claimed superiority over VIME to be overblown: the gain seems to stem almost exclusively from a faster initialization, but have very similar per-step cost? So given that VIME is also performing very competitively, what arguments can you advance for your own method(s)?
The paper proposes an intuitive method for exploration, namely to build a model of the system dynamics and explore regions where this approximation differs from the observed data (i.e., how "surprised" the agent was by an observation). The idea is a nice one, and part of the benefit comes from the simplicity and wide applicability of the approach. The main drawback of this paper is simply that the resulting approach doesn't substantially outperform existing approaches, at least not to a degree where it seem like the paper should should be clearly accepted to ICLR. On the continuous control tasks, the advantage over VIME seems very unclear (at best the results are mixed, showing sometimes surprisal and sometime VIME do better), and on the Atari games no comparison is made against many of the methods tuned for this setting, such as Gorila (Nair, 2015) which achieves some of the best results we are aware of on the Venture game, which is definitely the strongest result in this current paper, but not as good as this previous work. We know the settings are different, but overall it seems like the approach is largely outperformed by existing approaches, and thus the advantage mainly comes from runtime. This is certainly an interesting take, but needs to be studied a lot more thoroughly before it would make a really compelling case. We would like to recomend this paper to the workshop track. The pros/cons are as follows: Pros: + Simple and intuitive method for exploration Cons: - Doesn't seem to substantially outperform existing methods - No comparison to many alternative approaches for some of the "better" results in the paper.
The authors present a novel approach to surprise-based intrinsic motivation in deep reinforcement learning. The authors clearly explain the difference from other recent approaches to intrinsic motivation and back up their method with results from a broad class of discrete and continuous action domains. They present two tractable approximations to their framework - one which ignores the stochasticity of the true environmental dynamics, and one which approximates the rate of information gain (somewhat similar to Schmidhuber's formal theory of creativity, fun and intrinsic motivation). The results of this exploration bonus when added to TRPO are generally better than standard TRPO. However, I would have appreciated a more thorough comparison against other recent work on intrinsic motivation. For instance, Bellemare et al 2016 recently achieved significant performance gains on challenging Atari games like Montezuma's Revenge by combining DQN with an exploration bonus, however Montezuma's Revenge is not presented as an experiment here. Such comparisons would significantly improve the strength of the paper.
This paper provides a surprise-based intrinsic reward method for reinforcement learning, along with two practical algorithms for estimating those rewards. The ideas are similar to previous work in intrinsic motivation (including VIME and other work in intrinsic motivation). As a positive, the methods are simple to implement, and provide benefits on a number of tasks. However, they are almost always outmatched by VIME, and not one of their proposed method is consistently the best of those proposed (perhaps the most consistent is the surprisal, which is unfortunately not asymptotically equal to the true reward). The authors claim massive speed up, but the numerical measurements show that VIME is slower to initialize but not significantly slower per iteration otherwise (perhaps a big O analysis would clarify the claims). Overall it's a decent, simple technique, perhaps slightly incremental on previous state of the art.
This paper explores the topic of intrinsic motivation in the context of deep RL. It proposes a couple of variants derived from an auxiliary model-learning process (prediction error, surprise and learning progress), and shows that those can help exploration on a number of continuous control tasks (and the Atari game “venture”, maybe). Novelty: none of the proposed types of intrinsic motivation are novel, and it’s arguable whether the application to deep RL is novel (see e.g. Kompella et al 2012). Potential: the idea of seeking out states where a transition model is uncertain is sensible, but also limited -- I would encourage the authors to also discuss the limitations. For example in a game like Go the transition model is trivially learned, so this approach would revert to random exploration. So other forms of learning progress or surprise derived from the agent’s competence instead might be more promising in the long run? See also Srivastava et al 2012 for further thoughts. Computation time: I find the paper’s claimed superiority over VIME to be overblown: the gain seems to stem almost exclusively from a faster initialization, but have very similar per-step cost? So given that VIME is also performing very competitively, what arguments can you advance for your own method(s)?
This paper explores the topic of intrinsic motivation in the context of deep RL. It proposes a couple of variants derived from an auxiliary model-learning process (prediction error, surprise and learning progress), and shows that those can help exploration on a number of continuous control tasks (and the Atari game “venture”, maybe). Novelty: none of the proposed types of intrinsic motivation are novel, and it’s arguable whether the application to deep RL is novel (see e.g. Kompella et al 2012). Potential: the idea of seeking out states where a transition model is uncertain is sensible, but also limited -- I would encourage the authors to also discuss the limitations. For example in a game like Go the transition model is trivially learned, so this approach would revert to random exploration. So other forms of learning progress or surprise derived from the agent’s competence instead might be more promising in the long run? See also Srivastava et al 2012 for further thoughts. Computation time: I find the paper’s claimed superiority over VIME to be overblown: the gain seems to stem almost exclusively from a faster initialization, but have very similar per-step cost? So given that VIME is also performing very competitively, what arguments can you advance for your own method(s)?
The paper proposes an intuitive method for exploration, namely to build a model of the system dynamics and explore regions where this approximation differs from the observed data (i.e., how "surprised" the agent was by an observation). The idea is a nice one, and part of the benefit comes from the simplicity and wide applicability of the approach. The main drawback of this paper is simply that the resulting approach doesn't substantially outperform existing approaches, at least not to a degree where it seem like the paper should should be clearly accepted to ICLR. On the continuous control tasks, the advantage over VIME seems very unclear (at best the results are mixed, showing sometimes surprisal and sometime VIME do better), and on the Atari games no comparison is made against many of the methods tuned for this setting, such as Gorila (Nair, 2015) which achieves some of the best results we are aware of on the Venture game, which is definitely the strongest result in this current paper, but not as good as this previous work. We know the settings are different, but overall it seems like the approach is largely outperformed by existing approaches, and thus the advantage mainly comes from runtime. This is certainly an interesting take, but needs to be studied a lot more thoroughly before it would make a really compelling case. We would like to recomend this paper to the workshop track. The pros/cons are as follows: Pros: + Simple and intuitive method for exploration Cons: - Doesn't seem to substantially outperform existing methods - No comparison to many alternative approaches for some of the "better" results in the paper.
The authors present a novel approach to surprise-based intrinsic motivation in deep reinforcement learning. The authors clearly explain the difference from other recent approaches to intrinsic motivation and back up their method with results from a broad class of discrete and continuous action domains. They present two tractable approximations to their framework - one which ignores the stochasticity of the true environmental dynamics, and one which approximates the rate of information gain (somewhat similar to Schmidhuber's formal theory of creativity, fun and intrinsic motivation). The results of this exploration bonus when added to TRPO are generally better than standard TRPO. However, I would have appreciated a more thorough comparison against other recent work on intrinsic motivation. For instance, Bellemare et al 2016 recently achieved significant performance gains on challenging Atari games like Montezuma's Revenge by combining DQN with an exploration bonus, however Montezuma's Revenge is not presented as an experiment here. Such comparisons would significantly improve the strength of the paper.
This paper provides a surprise-based intrinsic reward method for reinforcement learning, along with two practical algorithms for estimating those rewards. The ideas are similar to previous work in intrinsic motivation (including VIME and other work in intrinsic motivation). As a positive, the methods are simple to implement, and provide benefits on a number of tasks. However, they are almost always outmatched by VIME, and not one of their proposed method is consistently the best of those proposed (perhaps the most consistent is the surprisal, which is unfortunately not asymptotically equal to the true reward). The authors claim massive speed up, but the numerical measurements show that VIME is slower to initialize but not significantly slower per iteration otherwise (perhaps a big O analysis would clarify the claims). Overall it's a decent, simple technique, perhaps slightly incremental on previous state of the art.
This paper explores the topic of intrinsic motivation in the context of deep RL. It proposes a couple of variants derived from an auxiliary model-learning process (prediction error, surprise and learning progress), and shows that those can help exploration on a number of continuous control tasks (and the Atari game “venture”, maybe). Novelty: none of the proposed types of intrinsic motivation are novel, and it’s arguable whether the application to deep RL is novel (see e.g. Kompella et al 2012). Potential: the idea of seeking out states where a transition model is uncertain is sensible, but also limited -- I would encourage the authors to also discuss the limitations. For example in a game like Go the transition model is trivially learned, so this approach would revert to random exploration. So other forms of learning progress or surprise derived from the agent’s competence instead might be more promising in the long run? See also Srivastava et al 2012 for further thoughts. Computation time: I find the paper’s claimed superiority over VIME to be overblown: the gain seems to stem almost exclusively from a faster initialization, but have very similar per-step cost? So given that VIME is also performing very competitively, what arguments can you advance for your own method(s)?
The paper proposes a novel algorithm to estimate graph structures by using a convolutional neural network to approximate the function that maps from empirical covariance matrix to the sparsity pattern of the graph. Compared with existing approaches, the new algorithm can adapt to different network structures, e.g. small-world networks, better under the same empirical risk minimization framework. Experiments on synthetic and real-world datasets show promising results compared with baselines. In general, I think it is an interesting and novel paper. The idea of framing structure estimation as a learning problem is especially interesting and may inspire further research on related topics. The advantage of such an approach is that it allows easier adaptation to different network structure properties without designing specific regularization terms as in graph lasso. The experiment results are also promising. In both synthetic and real-world datasets, the proposed algorithm outperforms other baselines in the small sample region. However, the paper can be made clearer in describing the network architectures. For example, in page 5, each ok_i,j is said be a d-dimensional vector. But from the context, it seems ok_i,j is a scalar (from o0_i,j = p_i,j). It is not clear what ok_i,j is exactly and what d is. Is it the number of channels for the convolutional filters? Figure 1 is also quite confusing. Why in (b) the table is 16 x 16 whereas in (a) there are only six nodes? And from the figure, it seems there is only one channel in each layer? What do the black squares represent and why are there three blocks of them. There are some descriptions in the text, but it is still not clear what they mean exactly. For real-world data, how are the training data (Y, Sigma) generated? Are they generated in the same way as in the synthetic experiments where the entries are uniformly sparse? This is also related to the more general question of how to sample from the distribution P, in the case of real-world data.
The authors provide a modern twist to the classical problem of graphical model selection. Traditionally, the sparsity priors to encourage selection of specific structures is hand-engineered. Instead, the authors propose using a neural network to train for these priors. Since graphical models are useful in the small-sample regime, using neural networks directly on the training data is not effective. Instead, the authors propose generating data based on the desired graph structures to train the neural network. While this is a nice idea, the paper is not clear and convincing enough to be accepted to the conference, and instead, recommend it to the workshop track.
We thank the reviewers for their comments and for providing us with excellent feedback. We have updated the paper with clarifications in Section 2.3 as well as an Appendix (A.4) with some additional experiments which analyze permuted inputs. We have also made an early release of our code available at
I sincerely apologize for the late-arriving review. This paper proposes to frame the problem of structure estimation as a supervised classification problem. The input is an empirical covariance matrix of the observed data, the output the binary decision whether or not two variables share a link. The paper is sufficiently clear, the goals are clear and everything is well described. The main interesting point is the empirical results of the experimental section. The approach is simple and performs better than previous non-learning based methods. This observation is interesting and will be of interest in structure discovery problems. I rate the specific construction of the supervised learning method as a reasonable attempt attempt to approach this problem. There is not very much technical novelty in this part. E.g., an algorithmic contribution would have been a method that is invariant to data permutation could have been a possible target for a technical contribution. The paper makes no claims on this technical part, as said, the method is well constructed and well executed. It is good to precisely state the theoretical parts of a paper, the authors do this well. All results are rather straight-forward, I like that the claims are written down, but there is little surprise in the statements. In summary, the paper makes a very interesting observation. Graph estimation can be posed as a supervised learning problem and training data from a separate source is sufficient to learn structure in novel and unseen test data from a new source. Practically this may be relevant, on one hand the empirical results are stronger with this method, on the other hand a practitioner who is interested in structural discovery may have side constraints about interpretability of the deriving method. From the Discussion and Conclusion I understand that the authors consider this as future work. It is a good first step, it could be stronger but also stands on its own already.
The paper proposes a novel algorithm to estimate graph structures by using a convolutional neural network to approximate the function that maps from empirical covariance matrix to the sparsity pattern of the graph. Compared with existing approaches, the new algorithm can adapt to different network structures, e.g. small-world networks, better under the same empirical risk minimization framework. Experiments on synthetic and real-world datasets show promising results compared with baselines. In general, I think it is an interesting and novel paper. The idea of framing structure estimation as a learning problem is especially interesting and may inspire further research on related topics. The advantage of such an approach is that it allows easier adaptation to different network structure properties without designing specific regularization terms as in graph lasso. The experiment results are also promising. In both synthetic and real-world datasets, the proposed algorithm outperforms other baselines in the small sample region. However, the paper can be made clearer in describing the network architectures. For example, in page 5, each ok_i,j is said be a d-dimensional vector. But from the context, it seems ok_i,j is a scalar (from o0_i,j = p_i,j). It is not clear what ok_i,j is exactly and what d is. Is it the number of channels for the convolutional filters? Figure 1 is also quite confusing. Why in (b) the table is 16 x 16 whereas in (a) there are only six nodes? And from the figure, it seems there is only one channel in each layer? What do the black squares represent and why are there three blocks of them. There are some descriptions in the text, but it is still not clear what they mean exactly. For real-world data, how are the training data (Y, Sigma) generated? Are they generated in the same way as in the synthetic experiments where the entries are uniformly sparse? This is also related to the more general question of how to sample from the distribution P, in the case of real-world data.
This paper proposes a new method for learning graphical models. Combined with a neural network architecture, some sparse edge structure is estimated via sampling methods. In introduction, the authors say that a problem in graphical lasso is model selection. However, the proposed method still implicitly includes model selection. In the proposed method, $P(G)$ is a sparse prior, and should include some hyper-parameters. How do you tune the hyper-parameters? Is this tuning an equivalent problem to model section? Therefore, I do not understand real advantage of this method over previous methods. What is the advantage of the proposed method? Another concern is that this paper is unorganized. In Algorithm 1, first, G_i and Sigma_i are sampled, and then x_j is sampled from N(0, Sigma). Here, what is Sigma? Is it different from Sigma_i? Furthermore, how do you construct (Y_i, hatSigma_i) from (G_i, X_i )? Finally, I have a simple question: Where is input data X (not sampled data) is used in Algorithm 1? What is the definition of the receptive field in Proposition 2 and Proposition 3?
The paper proposes a novel algorithm to estimate graph structures by using a convolutional neural network to approximate the function that maps from empirical covariance matrix to the sparsity pattern of the graph. Compared with existing approaches, the new algorithm can adapt to different network structures, e.g. small-world networks, better under the same empirical risk minimization framework. Experiments on synthetic and real-world datasets show promising results compared with baselines. In general, I think it is an interesting and novel paper. The idea of framing structure estimation as a learning problem is especially interesting and may inspire further research on related topics. The advantage of such an approach is that it allows easier adaptation to different network structure properties without designing specific regularization terms as in graph lasso. The experiment results are also promising. In both synthetic and real-world datasets, the proposed algorithm outperforms other baselines in the small sample region. However, the paper can be made clearer in describing the network architectures. For example, in page 5, each ok_i,j is said be a d-dimensional vector. But from the context, it seems ok_i,j is a scalar (from o0_i,j = p_i,j). It is not clear what ok_i,j is exactly and what d is. Is it the number of channels for the convolutional filters? Figure 1 is also quite confusing. Why in (b) the table is 16 x 16 whereas in (a) there are only six nodes? And from the figure, it seems there is only one channel in each layer? What do the black squares represent and why are there three blocks of them. There are some descriptions in the text, but it is still not clear what they mean exactly. For real-world data, how are the training data (Y, Sigma) generated? Are they generated in the same way as in the synthetic experiments where the entries are uniformly sparse? This is also related to the more general question of how to sample from the distribution P, in the case of real-world data.
The authors provide a modern twist to the classical problem of graphical model selection. Traditionally, the sparsity priors to encourage selection of specific structures is hand-engineered. Instead, the authors propose using a neural network to train for these priors. Since graphical models are useful in the small-sample regime, using neural networks directly on the training data is not effective. Instead, the authors propose generating data based on the desired graph structures to train the neural network. While this is a nice idea, the paper is not clear and convincing enough to be accepted to the conference, and instead, recommend it to the workshop track.
We thank the reviewers for their comments and for providing us with excellent feedback. We have updated the paper with clarifications in Section 2.3 as well as an Appendix (A.4) with some additional experiments which analyze permuted inputs. We have also made an early release of our code available at
I sincerely apologize for the late-arriving review. This paper proposes to frame the problem of structure estimation as a supervised classification problem. The input is an empirical covariance matrix of the observed data, the output the binary decision whether or not two variables share a link. The paper is sufficiently clear, the goals are clear and everything is well described. The main interesting point is the empirical results of the experimental section. The approach is simple and performs better than previous non-learning based methods. This observation is interesting and will be of interest in structure discovery problems. I rate the specific construction of the supervised learning method as a reasonable attempt attempt to approach this problem. There is not very much technical novelty in this part. E.g., an algorithmic contribution would have been a method that is invariant to data permutation could have been a possible target for a technical contribution. The paper makes no claims on this technical part, as said, the method is well constructed and well executed. It is good to precisely state the theoretical parts of a paper, the authors do this well. All results are rather straight-forward, I like that the claims are written down, but there is little surprise in the statements. In summary, the paper makes a very interesting observation. Graph estimation can be posed as a supervised learning problem and training data from a separate source is sufficient to learn structure in novel and unseen test data from a new source. Practically this may be relevant, on one hand the empirical results are stronger with this method, on the other hand a practitioner who is interested in structural discovery may have side constraints about interpretability of the deriving method. From the Discussion and Conclusion I understand that the authors consider this as future work. It is a good first step, it could be stronger but also stands on its own already.
The paper proposes a novel algorithm to estimate graph structures by using a convolutional neural network to approximate the function that maps from empirical covariance matrix to the sparsity pattern of the graph. Compared with existing approaches, the new algorithm can adapt to different network structures, e.g. small-world networks, better under the same empirical risk minimization framework. Experiments on synthetic and real-world datasets show promising results compared with baselines. In general, I think it is an interesting and novel paper. The idea of framing structure estimation as a learning problem is especially interesting and may inspire further research on related topics. The advantage of such an approach is that it allows easier adaptation to different network structure properties without designing specific regularization terms as in graph lasso. The experiment results are also promising. In both synthetic and real-world datasets, the proposed algorithm outperforms other baselines in the small sample region. However, the paper can be made clearer in describing the network architectures. For example, in page 5, each ok_i,j is said be a d-dimensional vector. But from the context, it seems ok_i,j is a scalar (from o0_i,j = p_i,j). It is not clear what ok_i,j is exactly and what d is. Is it the number of channels for the convolutional filters? Figure 1 is also quite confusing. Why in (b) the table is 16 x 16 whereas in (a) there are only six nodes? And from the figure, it seems there is only one channel in each layer? What do the black squares represent and why are there three blocks of them. There are some descriptions in the text, but it is still not clear what they mean exactly. For real-world data, how are the training data (Y, Sigma) generated? Are they generated in the same way as in the synthetic experiments where the entries are uniformly sparse? This is also related to the more general question of how to sample from the distribution P, in the case of real-world data.
This paper proposes a new method for learning graphical models. Combined with a neural network architecture, some sparse edge structure is estimated via sampling methods. In introduction, the authors say that a problem in graphical lasso is model selection. However, the proposed method still implicitly includes model selection. In the proposed method, $P(G)$ is a sparse prior, and should include some hyper-parameters. How do you tune the hyper-parameters? Is this tuning an equivalent problem to model section? Therefore, I do not understand real advantage of this method over previous methods. What is the advantage of the proposed method? Another concern is that this paper is unorganized. In Algorithm 1, first, G_i and Sigma_i are sampled, and then x_j is sampled from N(0, Sigma). Here, what is Sigma? Is it different from Sigma_i? Furthermore, how do you construct (Y_i, hatSigma_i) from (G_i, X_i )? Finally, I have a simple question: Where is input data X (not sampled data) is used in Algorithm 1? What is the definition of the receptive field in Proposition 2 and Proposition 3?
This paper proposes a new approach to model based reinforcement learning and evaluates it on 3 ATARI games. The approach involves training a model that predicts a sequence of rewards and probabilities of losing a life given a context of frames and a sequence of actions. The controller samples random sequences of actions and executes the one that balances the probabilities of earning a point and losing a life given some thresholds. The proposed system learns to play 3 Atari games both individually and when trained on all 3 in a multi-task setup at super-human level. The results presented in the paper are very encouraging but there are many ad-hoc design choices in the design of the system. The paper also provides little insight into the importance of the different components of the system. Main concerns: - The way predicted rewards and life loss probabilities are combined is very ad-hoc. The natural way to do this would be by learning a Q-value, instead different rules are devised for different games. - Is a model actually being learned and improved? It would be good to see predictions for several actions sequences from some carefully chosen start states. This would be good to see both on a game where the approach works and on a game where it fails. The learning progress could also be measured by plotting the training loss on a fixed holdout set of sequences. - How important is the proposed RRNN architecture? Would it still work without the residual connections? Would a standard LSTM also work? Minor points: - Intro, paragraph 2 - There is a lot of much earlier work on using models in RL. For example, see Dyna and "Memory approaches to reinforcement learning in non-Markovian domains" by Lin and Mitchell to name just two. - Section 3.1 - Minor point, but using a_i to represent the observation is unusual. Why not use o_i for observations and a_i for actions? - Section 3.2.2 - Notation again, r_i was used earlier to represent the reward at time i but it is being used again for something else. - Observation 1 seems somewhat out of place. Citing the layer normalization paper for the motivation is enough. - Section 3.2.2, second last paragraph - How is memory decoupled from computation here? Models like neural turning machines accomplish this by using an external memory, but this looks like an RNN with skip connections. - Section 3.3, second paragraph - Whether the model overfits or not depends on the data. The approach doesn't work with demonstrations precisely because it would overfit. - Figure 4 - The reference for Batch Normalization should be Ioffe and Szegedy instead of Morimoto et al. Overall I think the paper has some really promising ideas and encouraging results but is missing a few exploratory/ablation experiments and some polish.
The authors have proposed a new method for deep RL that uses model-based evaluation of states and actions and reward/life loss predictions. The evaluation, on just 3 ATari games with no comparisons to state of the art methods, is insufficient, and the method seems ad-hoc and unclear. Design choices are not clearly described or justified. The paper gives no insight as to how the different aspects of the approach relate or contribute to the overall results.
This paper proposes a model-based reinforcement learning approach focusing on predicting future rewards given a current state and future actions. This is achieved with a "residual recurrent neural network", that outputs the expected reward increase at various time steps in the future. To demonstrate the usefulness of this approach, experiments are conducted on Atari games, with a simple playing strategy that consists in evaluating random sequences of moves and picking the one with highest expected reward (and low enough chance of dying). Interestingly, out of the 3 games tested, one of them exhibits better performance when the agent is trained in a multitask setting (i.e. learning all games simultaneously), hinting that transfer learning is occurring. This submission is easy enough to read, and the reward prediction architecture looks like an original and sound idea. There are however several points that I believe prevent this work from reaching the ICLR bar, as detailed below. The first issue is the discrepancy between the algorithm proposed in Section 3 vs its actual implementation in Section 4 (experiments): in Section 3 the output is supposed to be the expected accumulated reward in future time steps (as a single scalar), while in experiments it is instead two numbers, one which is the probability of dying and another one which is the probability of having a higher score without dying. This might work better, but it also means the idea as presented in the main body of the paper is not actually evaluated (and I guess it would not work well, as otherwise why implement it differently?) In addition, the experimental results are quite limited: only on 3 games that were hand-picked to be easy enough, and no comparison to other RL techniques (DQN & friends). I realize that the main focus of the paper is not about exhibiting state-of-the-art results, since the policy being used is only a simple heuristic to show that the model predictions can ne used to drive decisions. That being said, I think experiments should have tried to demonstrate how to use this model to obtain better reinforcement learning algorithms: there is actually no reinforcement learning done here, since the model is a supervised algorithm, used in a manually-defined hardcoded policy. Another question that could have been addressed (but was not) in the experiments is how good these predictions are (e.g. classification error on dying probability, MSE on future rewards, ...), compared to simpler baselines. Finally, the paper's "previous work" section is too limited, focusing only on DQN and in particular saying very little on the topic of model-based RL. I think a paper like for instance "Action-Conditional Video Prediction using Deep Networks in Atari Games" should have been an obvious "must cite". Minor comments: - Notations are unusual, with "a" denoting a state rather than an action, this is potentially confusing and I see no reason to stray away from standard RL notations - Using a dot for tensor concatenation is not a great choice either, since the dot usually indicates a dot product - The r_i in 3.2.2 is a residual that has nothing to do with r_i the reward - c_i is defined as "The control that was performed at time i", but instead it seems to be the control performed at time i-1 - There is a recurrent confusion between mean and median in 3.2.2 - x should not be used in Observation 1 since the x from Fig. 3 does not go through layer normalization - The inequality in Observation 1 should be about |x_i|, not x_i - Observation 1 (with its proof) takes too much space for such a simple result - In 3.2.3 the first r_j should be r_i - The probability of dying comes out of nowhere in 3.3, since we do not know yet it will be an output of the model - "Our approach is not able to learn from good strategies" => did you mean "*only* from good strategies"? - Please say that in Fig. 4 "fc" means "fully connected" - It would be nice also to say how the architecture of Fig. 4 differs from the classical DQN architecture from Mnih et al (2015) - Please clarify r_j2 as per your answer in OpenReview comments - Table 3 says "After one iteration" but has "PRL Iteration 2" in it, which is confusing - "Figure 5 shows that not only there
The term strategy is a bit ambiguous. Could you please explain more in formal terms what is strategy? Is r the discounted Return at time t, or the reward at time t? Could the author compare the method to TD learning? The paper is vague and using many RL terms with different meanings without clarifying those diversions. "So, the output for a given state-actions pair is always same". Q function by definition is the value of (state, action). So as long as the policy is deterministic the output would be always same too. How's this different from Q learning? The model description doesn't specify what is the policy, and it's only being mentioned in data generation part. Why is it a model based approach? The learning curves are only for 19 iterations, which does not give any useful information. The final results are clearly nothing comparable to previous works. The model is only being tested on three games. The paper is vague and using informal language or sometimes misusing the common RL terms. The experiments are very small scale and even in that scenario performing very bad. It's not clear, why it's a model-based approach.
This paper proposes a new approach to model based reinforcement learning and evaluates it on 3 ATARI games. The approach involves training a model that predicts a sequence of rewards and probabilities of losing a life given a context of frames and a sequence of actions. The controller samples random sequences of actions and executes the one that balances the probabilities of earning a point and losing a life given some thresholds. The proposed system learns to play 3 Atari games both individually and when trained on all 3 in a multi-task setup at super-human level. The results presented in the paper are very encouraging but there are many ad-hoc design choices in the design of the system. The paper also provides little insight into the importance of the different components of the system. Main concerns: - The way predicted rewards and life loss probabilities are combined is very ad-hoc. The natural way to do this would be by learning a Q-value, instead different rules are devised for different games. - Is a model actually being learned and improved? It would be good to see predictions for several actions sequences from some carefully chosen start states. This would be good to see both on a game where the approach works and on a game where it fails. The learning progress could also be measured by plotting the training loss on a fixed holdout set of sequences. - How important is the proposed RRNN architecture? Would it still work without the residual connections? Would a standard LSTM also work? Minor points: - Intro, paragraph 2 - There is a lot of much earlier work on using models in RL. For example, see Dyna and "Memory approaches to reinforcement learning in non-Markovian domains" by Lin and Mitchell to name just two. - Section 3.1 - Minor point, but using a_i to represent the observation is unusual. Why not use o_i for observations and a_i for actions? - Section 3.2.2 - Notation again, r_i was used earlier to represent the reward at time i but it is being used again for something else. - Observation 1 seems somewhat out of place. Citing the layer normalization paper for the motivation is enough. - Section 3.2.2, second last paragraph - How is memory decoupled from computation here? Models like neural turning machines accomplish this by using an external memory, but this looks like an RNN with skip connections. - Section 3.3, second paragraph - Whether the model overfits or not depends on the data. The approach doesn't work with demonstrations precisely because it would overfit. - Figure 4 - The reference for Batch Normalization should be Ioffe and Szegedy instead of Morimoto et al. Overall I think the paper has some really promising ideas and encouraging results but is missing a few exploratory/ablation experiments and some polish.
This paper proposes a new approach to model based reinforcement learning and evaluates it on 3 ATARI games. The approach involves training a model that predicts a sequence of rewards and probabilities of losing a life given a context of frames and a sequence of actions. The controller samples random sequences of actions and executes the one that balances the probabilities of earning a point and losing a life given some thresholds. The proposed system learns to play 3 Atari games both individually and when trained on all 3 in a multi-task setup at super-human level. The results presented in the paper are very encouraging but there are many ad-hoc design choices in the design of the system. The paper also provides little insight into the importance of the different components of the system. Main concerns: - The way predicted rewards and life loss probabilities are combined is very ad-hoc. The natural way to do this would be by learning a Q-value, instead different rules are devised for different games. - Is a model actually being learned and improved? It would be good to see predictions for several actions sequences from some carefully chosen start states. This would be good to see both on a game where the approach works and on a game where it fails. The learning progress could also be measured by plotting the training loss on a fixed holdout set of sequences. - How important is the proposed RRNN architecture? Would it still work without the residual connections? Would a standard LSTM also work? Minor points: - Intro, paragraph 2 - There is a lot of much earlier work on using models in RL. For example, see Dyna and "Memory approaches to reinforcement learning in non-Markovian domains" by Lin and Mitchell to name just two. - Section 3.1 - Minor point, but using a_i to represent the observation is unusual. Why not use o_i for observations and a_i for actions? - Section 3.2.2 - Notation again, r_i was used earlier to represent the reward at time i but it is being used again for something else. - Observation 1 seems somewhat out of place. Citing the layer normalization paper for the motivation is enough. - Section 3.2.2, second last paragraph - How is memory decoupled from computation here? Models like neural turning machines accomplish this by using an external memory, but this looks like an RNN with skip connections. - Section 3.3, second paragraph - Whether the model overfits or not depends on the data. The approach doesn't work with demonstrations precisely because it would overfit. - Figure 4 - The reference for Batch Normalization should be Ioffe and Szegedy instead of Morimoto et al. Overall I think the paper has some really promising ideas and encouraging results but is missing a few exploratory/ablation experiments and some polish.
The authors have proposed a new method for deep RL that uses model-based evaluation of states and actions and reward/life loss predictions. The evaluation, on just 3 ATari games with no comparisons to state of the art methods, is insufficient, and the method seems ad-hoc and unclear. Design choices are not clearly described or justified. The paper gives no insight as to how the different aspects of the approach relate or contribute to the overall results.
This paper proposes a model-based reinforcement learning approach focusing on predicting future rewards given a current state and future actions. This is achieved with a "residual recurrent neural network", that outputs the expected reward increase at various time steps in the future. To demonstrate the usefulness of this approach, experiments are conducted on Atari games, with a simple playing strategy that consists in evaluating random sequences of moves and picking the one with highest expected reward (and low enough chance of dying). Interestingly, out of the 3 games tested, one of them exhibits better performance when the agent is trained in a multitask setting (i.e. learning all games simultaneously), hinting that transfer learning is occurring. This submission is easy enough to read, and the reward prediction architecture looks like an original and sound idea. There are however several points that I believe prevent this work from reaching the ICLR bar, as detailed below. The first issue is the discrepancy between the algorithm proposed in Section 3 vs its actual implementation in Section 4 (experiments): in Section 3 the output is supposed to be the expected accumulated reward in future time steps (as a single scalar), while in experiments it is instead two numbers, one which is the probability of dying and another one which is the probability of having a higher score without dying. This might work better, but it also means the idea as presented in the main body of the paper is not actually evaluated (and I guess it would not work well, as otherwise why implement it differently?) In addition, the experimental results are quite limited: only on 3 games that were hand-picked to be easy enough, and no comparison to other RL techniques (DQN & friends). I realize that the main focus of the paper is not about exhibiting state-of-the-art results, since the policy being used is only a simple heuristic to show that the model predictions can ne used to drive decisions. That being said, I think experiments should have tried to demonstrate how to use this model to obtain better reinforcement learning algorithms: there is actually no reinforcement learning done here, since the model is a supervised algorithm, used in a manually-defined hardcoded policy. Another question that could have been addressed (but was not) in the experiments is how good these predictions are (e.g. classification error on dying probability, MSE on future rewards, ...), compared to simpler baselines. Finally, the paper's "previous work" section is too limited, focusing only on DQN and in particular saying very little on the topic of model-based RL. I think a paper like for instance "Action-Conditional Video Prediction using Deep Networks in Atari Games" should have been an obvious "must cite". Minor comments: - Notations are unusual, with "a" denoting a state rather than an action, this is potentially confusing and I see no reason to stray away from standard RL notations - Using a dot for tensor concatenation is not a great choice either, since the dot usually indicates a dot product - The r_i in 3.2.2 is a residual that has nothing to do with r_i the reward - c_i is defined as "The control that was performed at time i", but instead it seems to be the control performed at time i-1 - There is a recurrent confusion between mean and median in 3.2.2 - x should not be used in Observation 1 since the x from Fig. 3 does not go through layer normalization - The inequality in Observation 1 should be about |x_i|, not x_i - Observation 1 (with its proof) takes too much space for such a simple result - In 3.2.3 the first r_j should be r_i - The probability of dying comes out of nowhere in 3.3, since we do not know yet it will be an output of the model - "Our approach is not able to learn from good strategies" => did you mean "*only* from good strategies"? - Please say that in Fig. 4 "fc" means "fully connected" - It would be nice also to say how the architecture of Fig. 4 differs from the classical DQN architecture from Mnih et al (2015) - Please clarify r_j2 as per your answer in OpenReview comments - Table 3 says "After one iteration" but has "PRL Iteration 2" in it, which is confusing - "Figure 5 shows that not only there
The term strategy is a bit ambiguous. Could you please explain more in formal terms what is strategy? Is r the discounted Return at time t, or the reward at time t? Could the author compare the method to TD learning? The paper is vague and using many RL terms with different meanings without clarifying those diversions. "So, the output for a given state-actions pair is always same". Q function by definition is the value of (state, action). So as long as the policy is deterministic the output would be always same too. How's this different from Q learning? The model description doesn't specify what is the policy, and it's only being mentioned in data generation part. Why is it a model based approach? The learning curves are only for 19 iterations, which does not give any useful information. The final results are clearly nothing comparable to previous works. The model is only being tested on three games. The paper is vague and using informal language or sometimes misusing the common RL terms. The experiments are very small scale and even in that scenario performing very bad. It's not clear, why it's a model-based approach.
This paper proposes a new approach to model based reinforcement learning and evaluates it on 3 ATARI games. The approach involves training a model that predicts a sequence of rewards and probabilities of losing a life given a context of frames and a sequence of actions. The controller samples random sequences of actions and executes the one that balances the probabilities of earning a point and losing a life given some thresholds. The proposed system learns to play 3 Atari games both individually and when trained on all 3 in a multi-task setup at super-human level. The results presented in the paper are very encouraging but there are many ad-hoc design choices in the design of the system. The paper also provides little insight into the importance of the different components of the system. Main concerns: - The way predicted rewards and life loss probabilities are combined is very ad-hoc. The natural way to do this would be by learning a Q-value, instead different rules are devised for different games. - Is a model actually being learned and improved? It would be good to see predictions for several actions sequences from some carefully chosen start states. This would be good to see both on a game where the approach works and on a game where it fails. The learning progress could also be measured by plotting the training loss on a fixed holdout set of sequences. - How important is the proposed RRNN architecture? Would it still work without the residual connections? Would a standard LSTM also work? Minor points: - Intro, paragraph 2 - There is a lot of much earlier work on using models in RL. For example, see Dyna and "Memory approaches to reinforcement learning in non-Markovian domains" by Lin and Mitchell to name just two. - Section 3.1 - Minor point, but using a_i to represent the observation is unusual. Why not use o_i for observations and a_i for actions? - Section 3.2.2 - Notation again, r_i was used earlier to represent the reward at time i but it is being used again for something else. - Observation 1 seems somewhat out of place. Citing the layer normalization paper for the motivation is enough. - Section 3.2.2, second last paragraph - How is memory decoupled from computation here? Models like neural turning machines accomplish this by using an external memory, but this looks like an RNN with skip connections. - Section 3.3, second paragraph - Whether the model overfits or not depends on the data. The approach doesn't work with demonstrations precisely because it would overfit. - Figure 4 - The reference for Batch Normalization should be Ioffe and Szegedy instead of Morimoto et al. Overall I think the paper has some really promising ideas and encouraging results but is missing a few exploratory/ablation experiments and some polish.
This is an interesting paper on a VAE framework for topic models. The main idea is to train a recognition model for the inference phase which, because of so called “amortized inference” can be much faster than normal inference where inference must be run iteratively for every document. Some comments: Eqn 5: I find the notation p(theta(h)|alpha) awkward. Why not P(h|alpha) ? The generative model seems agnostic to document length, meaning that the latent variables only generate probabilities over word space. However, the recognition model is happy to radically change the probabilities q(z|x) if the document length changes because the input to q changes. This seems undesirable. Maybe they should normalize the input to the recognition network? The ProdLDA model might well be equivalent to exponential family PCA or some variant thereof:
The reviewers agree that the approach is interesting and the paper presents useful findings. They also raise enough questions and suggestions for improvements that I believe the paper will be much stronger after further revision, though these seem straightforward to address.
The authors propose NVI for LDA variants. The authors compare NVI-LDA to standard inference schemes such as CGS and online SVI. The authors also evaluate NVI on a different model ProdLDA (not sure this model has been proposed before in the topic modeling literature though?) In general, I like the direction of this paper and NVI looks promising for LDA. The experimental results however confound model vs inference which makes it hard to understand the significance of the results. Furthermore, the authors don't discuss hyper-parameter selection which is known to significantly impact performance of topic models. This makes it hard to understand when the proposed method can be expected to work. Can you maybe generate synthetic datasets with different Dirichlet distributions and assess when the proposed method recovers the true parameters? Figure 1: Is this prior or posterior? The text talks about sparsity whereas the y-axis reads "log p(topic proportions)" which is a bit confusing. Section 3.2: it is not clear what you mean by unimodal in softmax basis. Consider a Dirichlet on K-dimensional simplex with concentration parameter alpha/K where alpha1 makes it multimodal. Isn't the softmax basis still multimodal? None of the numbers include error bars. Are the results statistically significant? Minor comments: Last term in equation (3) is not "error"; reconstruction accuracy or negative reconstruction error perhaps? The idea of using an inference network is much older, cf. Helmholtz machine.
This paper proposes the use of neural variational inference method for topic models. The paper shows a nice trick to approximate Dirichlet prior using softmax basis with a Gaussian and then the model is trained to maximize the variational lower bound. Also, the authors study a better way to alleviate the component collapsing issue, which has been problematic for continuous latent variables that follow Gaussian distribution. The results look promising and the experimental protocol sounds fine. Minor comments: Please add citation to [1] or [2] for neural variational inference, and [2] for VAE. A typo in “This approximation to the Dirichlet prior p(|) is results in the distribution”, it should be “This approximation to the Dirichlet prior p(|) results in the distribution” In table 2, it is written that DMFVI was trained more than 24hrs but failed to deliver any result, but why not wait until the end and report the numbers? In table 3, why are the perplexities of LDA-Collapsed Gibbs and NVDM are lower while the proposed models (ProdLDA) generates more coherent topics? What is your intuition on this? How does the training speed (until the convergence) differs by using different learning-rate and momentum scheduling approaches shown as in figure 1? It may be also interesting to add some more analysis on the latent variables z (component collapsing and etc., although your results indirectly show that the learning-rate and momentum scheduling trick removes this issue). Overall, the paper clearly proposes its main idea, explain why it is good to use NVI, and its experimental results support the original claim. It explains well what are the challenges and demonstrate their solutions. [1] Minh et al., Neural Variational Inference and Learning in Belief Networks, ICML’14 [2] Rezende et al., Stochastic Backpropagation and Approximate Inference in Deep Generative Models, ICML’14
This is an interesting paper on a VAE framework for topic models. The main idea is to train a recognition model for the inference phase which, because of so called “amortized inference” can be much faster than normal inference where inference must be run iteratively for every document. Some comments: Eqn 5: I find the notation p(theta(h)|alpha) awkward. Why not P(h|alpha) ? The generative model seems agnostic to document length, meaning that the latent variables only generate probabilities over word space. However, the recognition model is happy to radically change the probabilities q(z|x) if the document length changes because the input to q changes. This seems undesirable. Maybe they should normalize the input to the recognition network? The ProdLDA model might well be equivalent to exponential family PCA or some variant thereof:
The comparison to NVDM looks unfair since the user introduces a couples tricks (Dirichlet prior, batch normalisation, high momentum training, etc.) which NVDM doesn't use. A more convincing experimental design is to explore the effect of each trick separately in neural variational inference.
The perplexity you're reporting for the 20 Newsgroups dataset using LDA Collapsed Gibbs is better than for any other method I've seen. Would you mind sharing the parameters you used and/or the preprocessed dataset?
This is an interesting paper on a VAE framework for topic models. The main idea is to train a recognition model for the inference phase which, because of so called “amortized inference” can be much faster than normal inference where inference must be run iteratively for every document. Some comments: Eqn 5: I find the notation p(theta(h)|alpha) awkward. Why not P(h|alpha) ? The generative model seems agnostic to document length, meaning that the latent variables only generate probabilities over word space. However, the recognition model is happy to radically change the probabilities q(z|x) if the document length changes because the input to q changes. This seems undesirable. Maybe they should normalize the input to the recognition network? The ProdLDA model might well be equivalent to exponential family PCA or some variant thereof:
The reviewers agree that the approach is interesting and the paper presents useful findings. They also raise enough questions and suggestions for improvements that I believe the paper will be much stronger after further revision, though these seem straightforward to address.
The authors propose NVI for LDA variants. The authors compare NVI-LDA to standard inference schemes such as CGS and online SVI. The authors also evaluate NVI on a different model ProdLDA (not sure this model has been proposed before in the topic modeling literature though?) In general, I like the direction of this paper and NVI looks promising for LDA. The experimental results however confound model vs inference which makes it hard to understand the significance of the results. Furthermore, the authors don't discuss hyper-parameter selection which is known to significantly impact performance of topic models. This makes it hard to understand when the proposed method can be expected to work. Can you maybe generate synthetic datasets with different Dirichlet distributions and assess when the proposed method recovers the true parameters? Figure 1: Is this prior or posterior? The text talks about sparsity whereas the y-axis reads "log p(topic proportions)" which is a bit confusing. Section 3.2: it is not clear what you mean by unimodal in softmax basis. Consider a Dirichlet on K-dimensional simplex with concentration parameter alpha/K where alpha1 makes it multimodal. Isn't the softmax basis still multimodal? None of the numbers include error bars. Are the results statistically significant? Minor comments: Last term in equation (3) is not "error"; reconstruction accuracy or negative reconstruction error perhaps? The idea of using an inference network is much older, cf. Helmholtz machine.
This paper proposes the use of neural variational inference method for topic models. The paper shows a nice trick to approximate Dirichlet prior using softmax basis with a Gaussian and then the model is trained to maximize the variational lower bound. Also, the authors study a better way to alleviate the component collapsing issue, which has been problematic for continuous latent variables that follow Gaussian distribution. The results look promising and the experimental protocol sounds fine. Minor comments: Please add citation to [1] or [2] for neural variational inference, and [2] for VAE. A typo in “This approximation to the Dirichlet prior p(|) is results in the distribution”, it should be “This approximation to the Dirichlet prior p(|) results in the distribution” In table 2, it is written that DMFVI was trained more than 24hrs but failed to deliver any result, but why not wait until the end and report the numbers? In table 3, why are the perplexities of LDA-Collapsed Gibbs and NVDM are lower while the proposed models (ProdLDA) generates more coherent topics? What is your intuition on this? How does the training speed (until the convergence) differs by using different learning-rate and momentum scheduling approaches shown as in figure 1? It may be also interesting to add some more analysis on the latent variables z (component collapsing and etc., although your results indirectly show that the learning-rate and momentum scheduling trick removes this issue). Overall, the paper clearly proposes its main idea, explain why it is good to use NVI, and its experimental results support the original claim. It explains well what are the challenges and demonstrate their solutions. [1] Minh et al., Neural Variational Inference and Learning in Belief Networks, ICML’14 [2] Rezende et al., Stochastic Backpropagation and Approximate Inference in Deep Generative Models, ICML’14
This is an interesting paper on a VAE framework for topic models. The main idea is to train a recognition model for the inference phase which, because of so called “amortized inference” can be much faster than normal inference where inference must be run iteratively for every document. Some comments: Eqn 5: I find the notation p(theta(h)|alpha) awkward. Why not P(h|alpha) ? The generative model seems agnostic to document length, meaning that the latent variables only generate probabilities over word space. However, the recognition model is happy to radically change the probabilities q(z|x) if the document length changes because the input to q changes. This seems undesirable. Maybe they should normalize the input to the recognition network? The ProdLDA model might well be equivalent to exponential family PCA or some variant thereof:
The comparison to NVDM looks unfair since the user introduces a couples tricks (Dirichlet prior, batch normalisation, high momentum training, etc.) which NVDM doesn't use. A more convincing experimental design is to explore the effect of each trick separately in neural variational inference.
The perplexity you're reporting for the 20 Newsgroups dataset using LDA Collapsed Gibbs is better than for any other method I've seen. Would you mind sharing the parameters you used and/or the preprocessed dataset?
This paper presents a new theoretically-principled method of representing sentences as vectors. The experiments show that vectors produced by this method perform well on similarity and entailment benchmarks, surpassing some RNN-based methods too. Overall, this is an interesting empirical result, especially since the model is not order-sensitive (as far as I can tell). I would like to see some more discussion on why such a simple model does better than LSTMs at capturing similarity and entailment. Could this be an artifact of these benchmarks?
A new method for sentence embedding that is simple and performs well. Important contribution that will attract attention and help move the field forward.
This is a good paper with an interesting probabilistic motivation for weighted bag of words models. The (hopefully soon) added comparison to Wang and Manning will make it stronger. Though it is sad that for sufficiently large datasets, NB-SVM still works better. In the second to last paragraph of the introduction you describe a problem of large cooccurrence counts which was already fixed by the Glove embeddings with their weighting function f. Minor comments: "The capturing the similarities" -- typo in line 2 of intro. "Recently, (Wieting et al.,2016) learned" -- use citet instead of parenthesized citation
This paper proposes a simple way to reweight the word embedding in the simple composition function for sentence representation. This paper also shows the connection between this new weighting scheme and some previous work. Here are some comments on technical details: - The word "discourse" is confusing. I am not sure whether the words "discourse" in "discourse vector c_s" and the one in "most frequent discourse" have the same meaning. - Is there any justification about $c_0$ related to syntac? - Not sure what thie line means: "In fact the new model was discovered by our detecting the common component c0 in existing embeddings." in section "Computing the sentence embedding" - Is there any explanation about the results on sentiment in Table 2?
This paper presents a new theoretically-principled method of representing sentences as vectors. The experiments show that vectors produced by this method perform well on similarity and entailment benchmarks, surpassing some RNN-based methods too. Overall, this is an interesting empirical result, especially since the model is not order-sensitive (as far as I can tell). I would like to see some more discussion on why such a simple model does better than LSTMs at capturing similarity and entailment. Could this be an artifact of these benchmarks?
We (J. Mu and P. Viswanath) thoroughly enjoyed the authors' previous work on linear algebraic structure of word senses (cf.
Dear Authors, Please resubmit your paper in the ICLR 2017 format with the correct font for your submission to be considered. Thank you!
This paper presents a new theoretically-principled method of representing sentences as vectors. The experiments show that vectors produced by this method perform well on similarity and entailment benchmarks, surpassing some RNN-based methods too. Overall, this is an interesting empirical result, especially since the model is not order-sensitive (as far as I can tell). I would like to see some more discussion on why such a simple model does better than LSTMs at capturing similarity and entailment. Could this be an artifact of these benchmarks?
A new method for sentence embedding that is simple and performs well. Important contribution that will attract attention and help move the field forward.
This is a good paper with an interesting probabilistic motivation for weighted bag of words models. The (hopefully soon) added comparison to Wang and Manning will make it stronger. Though it is sad that for sufficiently large datasets, NB-SVM still works better. In the second to last paragraph of the introduction you describe a problem of large cooccurrence counts which was already fixed by the Glove embeddings with their weighting function f. Minor comments: "The capturing the similarities" -- typo in line 2 of intro. "Recently, (Wieting et al.,2016) learned" -- use citet instead of parenthesized citation
This paper proposes a simple way to reweight the word embedding in the simple composition function for sentence representation. This paper also shows the connection between this new weighting scheme and some previous work. Here are some comments on technical details: - The word "discourse" is confusing. I am not sure whether the words "discourse" in "discourse vector c_s" and the one in "most frequent discourse" have the same meaning. - Is there any justification about $c_0$ related to syntac? - Not sure what thie line means: "In fact the new model was discovered by our detecting the common component c0 in existing embeddings." in section "Computing the sentence embedding" - Is there any explanation about the results on sentiment in Table 2?
This paper presents a new theoretically-principled method of representing sentences as vectors. The experiments show that vectors produced by this method perform well on similarity and entailment benchmarks, surpassing some RNN-based methods too. Overall, this is an interesting empirical result, especially since the model is not order-sensitive (as far as I can tell). I would like to see some more discussion on why such a simple model does better than LSTMs at capturing similarity and entailment. Could this be an artifact of these benchmarks?
We (J. Mu and P. Viswanath) thoroughly enjoyed the authors' previous work on linear algebraic structure of word senses (cf.
Dear Authors, Please resubmit your paper in the ICLR 2017 format with the correct font for your submission to be considered. Thank you!
This paper presents an approach to tag word senses with temporal information (past, present, future or atemporal). They model the problem using a graph-based semi-supervised classification algorithm that allows to combine item specific information - such as the presence of some temporal indicators in the glosses - and the structure of Wordnet - that is semantic relations between synsets â, and to take into account unlabeled data. They perform a full annotation of Wordnet, based on a set of training data labeled in a previous work and using the rest of Wordnet as unlabeled data. Specifically, they take advantage of the structure of the label set by breaking the task into a binary formulation (temporal vs atemporal), then using the data labeled as temporal to perform a finer grained tagging (past, present or future). In order to intrinsically evaluate their approach, they annotate a subset of synsets in Wordnet using crowd-sourcing. They compare their system to the results obtained by a state-of-the-art time tagger (Stanford's SUTime) using an heuristic as a backup strategy, and to previous works. They obtain improvements around 11% in accuracy, and show that their approach allows performance higher than previous systems using only 400 labeled data. Finally, they perform an evaluation of their resource on an existing task (TempEval-3) and show improvements of about 10% in F1 on 4 labels. This paper is well-constructed and generally clear, the approach seems sound and well justified. This work led to the development of a resource with fine grained temporal information at the word sense level that would be made available and could be used to improve various NLP tasks. I have a few remarks, especially concerning the settings of the experiments. I think that more information should be given on the task performed in the extrinsic evaluation section. An example could be useful to understand what the system is trying to predict (the features describe âentity pairsâ but it has not been made clear before what are these pairs) and what are the features (especially, what are the entity attributes? What is the POS for a pair, is it one dimension or two? Are the lemmas obtained automatically?). The sentence describing the labels used is confusing, I'm not sure to understand what âevent to document creation timeâ and âevent to same sentence eventâ means, are they the kind of pairs considered? Are they relations (as they are described as relation at the beginning of p.8)? I find unclear the footnote about the 14 relations: why the other relations have to be ignored, what makes a mapping too âcomplexâ? Also, are the scores macro or micro averaged? Finally, the ablation study seems to indicate a possible redundancy between Lexica and Entity with quite close scores, any clue about this behavior? I have also some questions about the use of the SVM. For the extrinsic evaluation, the authors say that they optimized the parameters of the algorithm: what are these parameters? And since a SVM is also used within the MinCut framework, is it optimized and how? Finally, if it's the LibSVM library that is used (Weka wrapper), I think a reference to LibSVM should be included. Other remarks: - It would be interesting to have the number of examples per label in the gold data, the figures are given for coarse grained labels (127 temporal vs 271 atemporal), but not for the finer grained. - It would also be nice to have an idea of the number of words that are ambiguous at the temporal level, words like âpresentâ. - It is said in the caption of the table 3 that the results presented are âsignificantly betterâ but no significancy test is indicated, neither any p-value. Minor remarks: - Related work: what kind of task was performed in (Filannino and Nenadic, 2014)? - Related work: ârequires a post-calibration procedureâ, needs a reference (and p.4 in 3.3 footnote it would be clearer to explain calibration) - Related work: âtheir model differ from oursâ, in what? - Table 3 is really too small: maybe, remove the parenthesis, put the â(p,r,f1)â in the caption
- Strengths: Relatively clear description of context and structure of proposed approach. Relatively complete description of the math. Comparison to an extensive set of alternative systems. - Weaknesses: Weak results/summary of "side-by-side human" comparison in Section 5. Some disfluency/agrammaticality. - General Discussion: The article proposes a principled means of modeling utterance context, consisting of a sequence of previous utterances. Some minor issues: 1. Past turns in Table 1 could be numbered, making the text associated with this table (lines 095-103) less difficult to ingest. Currently, readers need to count turns from the top when identifying references in the authors' description, and may wonder whether "second", "third", and "last" imply a side-specific or global enumeration. 2. Some reader confusion may be eliminated by explicitly defining what "segment" means in "segment level", as occurring on line 269. Previously, on line 129, this seemingly same thing was referred to as "a sequence-sequence [similarity matrix]". The two terms appear to be used interchangeably, but it is not clear what they actually mean, despite the text in section 3.3. It seems the authors may mean "word subsequence" and "word subsequence to word subsequence", where "sub-" implies "not the whole utterance", but not sure. 3. Currently, the variable symbol "n" appears to be used to enumerate words in an utterance (line 306), as well as utterances in a dialogue (line 389). The authors may choose two different letters for these two different purposes, to avoid confusing readers going through their equations. 4. The statement "This indicates that a retrieval based chatbot with SMN can provide a better experience than the state-of-the-art generation model in practice." at the end of section 5 appears to be unsupported. The two approaches referred to are deemed comparable in 555 out of 1000 cases, with the baseline better than the proposed method in 238 our of the remaining 445 cases. The authors are encouraged to assess and present the statistical significance of this comparison. If it is weak, their comparison permits to at best claim that their proposed method is no worse (rather than "better") than the VHRED baseline. 5. The authors may choose to insert into Figure 1 the explicit "first layer", "second layer" and "third layer" labels they use in the accompanying text. 6. Their is a pervasive use of "to meet" as in "a response candidate can meet each utterace" on line 280 which is difficult to understand. 7. Spelling: "gated recurrent unites"; "respectively" on line 133 should be removed; punctuation on line 186 and 188 is exchanged; "baseline model over" -> "baseline model by"; "one cannot neglects".
- Strengths: - Weaknesses: Many grammar errors, such as the abstract - General Discussion:
- Strengths: Introduces a new document clustering approach and compares it to several established methods, showing that it improves results in most cases. The analysis is very detailed and thorough--quite dense in many places and requires careful reading. The presentation is organized and clear, and I am impressed by the range of comparisons and influential factors that were considered. Argument is convincing and the work should influence future approaches. - Weaknesses: The paper does not provide any information on the availability of the software described. - General Discussion: Needs some (minor) editing for English and typos--here are just a few: Line 124: regardless the size > regardless of the size Line 126: resources. Because > resources, because Line 205: consist- ing mk > consisting of mk Line 360: versionand > version and
COMMENTS AFTER AUTHOR RESPONSE: Thanks for your response, particularly for the clarification wrt the hypothesis. I agree with the comment wrt cross-modal mapping. What I don't share is the kind of equation "visual = referential" that you seem to assume. A referent can be visually presented, but visual information can be usefully added to a word's representation in aggregate form to encode perceptual aspects of the words' meaning, the same way that it is done for textual information; for instance, the fact that bananas are yellow will not frequently be mentioned in text, and adding visual information extracted from images will account for this aspect of the semantic representation of the word. This is kind of technical and specific to how we build distributional models, but it's also relevant if you think of human cognition (probably our representation for "banana" has some aggregate information about all the bananas we've seen --and touched, tasted, etc.). It would be useful if you could discuss this issue explicitly, differentiating between multi-modal distributional semantics in general and the use of cross-modal mapping in particular. Also, wrt the "all models perform similarly" comment: I really urge you, if the paper is accepted, to state it in this form, even if it doesn't completely align with your hypotheses/goals (you have enough results that do). It is a better description of the results, and more useful for the community, than clinging to the n-th digit difference (and this is to a large extent independent of whether the difference is actually statistical significant or not: If one bridge has 49% chances of collapsing and another one 50%, the difference may be statistically significant, but that doesn't really make the first bridge a better bridge to walk on). Btw, small quibble, could you find a kind of more compact and to the point title? (More geared towards either generally what you explore or to what you find?) ---------- The paper tackles an extremely interesting issue, that the authors label "referential word meaning", namely, the connection between a word's meaning and the referents (objects in the external world) it is applied to. If I understood it correctly, they argue that this is different from a typical word meaning representation as obtained e.g. with distributional methods, because one thing is the abstract "lexical meaning" of a word and the other which label is appropriate for a given referent with specific properties (in a specific context, although context is something they explicitly leave aside in this paper). This hypothesis has been previously explored in work by Schlangen and colleagues (cited in the paper). The paper explores referential word meaning empirically on a specific version of the task of Referential Expression Generation (REG), namely, generating the appropriate noun for a given visually represented object. - Strengths: 1) The problem they tackle I find extremely interesting; as they argue, REG is a problem that had previously been addressed mainly using symbolic methods, that did not easily allow for an exploration of how speakers choose the names of the objects. The scope of the research goes beyond REG as such, as it addresses the link between semantic representations and reference more broadly. 2) I also like how they use current techniques and datasets (cross-modal mapping and word classifiers, the ReferIt dataset containing large amounts of images with human-generated referring expressions) to address the problem at hand. 3) There are a substantial number of experiments as well as analysis into the results. - Weaknesses: 1) The main weakness for me is the statement of the specific hypothesis, within the general research line, that the paper is probing: I found it very confusing. As a result, it is also hard to make sense of the kind of feedback that the results give to the initial hypothesis, especially because there are a lot of them and they don't all point in the same direction. The paper says: "This paper pursues the hypothesis that an accurate model of referential word meaning does not need to fully integrate visual and lexical knowledge (e.g. as expressed in a distributional vector space), but at the same time, has to go beyond treating words as independent labels." The first part of the hypothesis I don't understand: What is it to fully integrate (or not to fully integrate) visual and lexical knowledge? Is the goal simply to show that using generic distributional representation yields worse results than using specific, word-adapted classifiers trained on the dataset? If so, then the authors should explicitly discuss
AFTER AUTHOR RESPONSE I accept the response about emphasizing novelty of the task and comparison with previous work. Also increase ratings for the dataset and software that are promised to become public before the article publishing. ====================== GENERAL The paper presents an interesting empirical comparison of 3 referring expression generation models. The main novelty lies in the comparison of a yet unpublished model called SIM-WAP (in press by Anonymous). The model is described in SECTION 4.3 but it is not clear whether it is extended or modified anyhow in the current paper. The novelty of the paper may be considered as the comparison of the unpublished SIM-WAP model to existing 2 models. This complicates evaluation of the novelty because similar experiments were already performed for the other two models and it is unclear why this comparison was not performed in the paper where SIM-WAP model was presented. A significant novelty might be the combined model yet this is not stated clearly and the combination is not described with enough details. The contribution of the paper may be considered the following: the side-by-side comparison of the 3 methods for REG; analysis of zero-shot experiment results which mostly confirms similar observations in previous works; analysis of the complementarity of the combined model. WEAKNESSES Unclear novelty and significance of contributions. The work seems like an experimental extension of the cited Anonymous paper where the main method was introduced. Another weakness is the limited size of the vocabulary in the zero-shot experiments that seem to be the most contributive part. Additionally, the authors never presented significance scores for their accuracy results. This would have solidified the empirical contribution of the work which its main value. My general feeling is that the paper is more appropriate for a conference on empirical methods such as EMNLP. Lastly, I have not found any link to any usable software. Existing datasets have been used for the work. Observations by Sections: ABSTRACT "We compare three recent models" -- Further in the abstract you write that you also experiment with the combination of approaches. In Section 2 you write that "we present a model that exploits distributional knowledge for learning referential word meaning as well, but explore and compare different ways of combining visual and lexical aspects of referential word meaning" which eventually might be a better summarization of the novelty introduced in the paper and give more credit to the value of your work. My suggestion is to re-write the abstract (and eventually even some sections in the paper) focusing on the novel model and results and not just stating that you compare models of others. INTRODUCTION "Determining such a name is is" - typo "concerning e.g." -> "concerning, e.g.," "having disjunct extensions." - specify or exemplify, please "building in Figure 1" -> "building in Figure 1 (c)" SECTION 4 "Following e.g. Lazaridou et al. (2014)," - "e.g." should be omitted SECTION 4.2 "associate the top n words with their corresponding distributional vector" - What are the values of N that you used? If there were any experiments for finding the optimal values, please, describe because this is original work. The use top N = K is not obvious and not obvious why it should be optimal (how about finding similar vectors to each 5 in top 20?) SECTION 4.3 "we annotate its training instances with a fine-grained similarity signal according to their object names." - please, exemplify. LANGUAGE Quite a few typos in the draft. Generally, language should be cleaned up ("as well such as"). Also, I believe the use of American English spelling standard is preferable (e.g., "summarise" -> "summarize"). Please, double check with your conference track chairs.
The paper describes an idea to learn phrasal representation and facilitate them in RNN-based language models and neural machine translation -Strengths: The idea to incorporate phrasal information into the task is interesting. - Weaknesses: - The description is hard to follow. Proof-reading by an English native speaker would benefit the understanding - The evaluation of the approach has several weaknesses - General discussion - In Equation 1 and 2 the authors mention a phrase representation give a fix-length word embedding vector. But this is not used in the model. The representation is generated based on an RNN. What the propose of this description? - Why are you using GRU for the Pyramid and LSTM for the sequential part? Is the combination of two architectures a reason for your improvements? - What is the simplified version of the GRU? Why is it performing better? How is it performing on the large data set? - What is the difference between RNNsearch (groundhog) and RNNsearch(baseline) in Table 4? - What is the motivation for only using the ending phrases and e.g. not using the starting phrases? - Did you use only the pyramid encoder? How is it performing? That would be a more fair comparison since it normally helps to make the model more complex. - Why did you run RNNsearch several times, but PBNMT only once? - Section 5.2: What is the intent of this section
This paper proposed a new phrasal RNN architecture for sequence to sequence generation. They have evaluated their architecture based on (i) the language modelling test evaluated on PTB and FBIS and (ii) Chinese-English machine translation task on NIST MT02-08 evaluation sets. The phrasal RNN (pRNN) architecture is achieved by generating subnetworks of phrases. Strengths ==== A new phrasal architecture. Weaknesses ==== **Technical**: It's unclear whether there is a limit set on the phrase length of the pRNN. Maybe I've missed this in the paper, if there is, please be more explicit about it because it affects the model quite drastically if for every sentence the largest phrase length is the sentence length. - It's because if the largest phrase length is the sentence length, then model can be simplified into a some sort of convolution RNN where the each state of the RNN goes through some convolution layer before a final softmax and attention. - If there is a limit set on the phrase length of pRNN, then it makes the system more tractable. But that would also mean that the phrases are determined by token ngrams which produces a sliding window of the "pyramid encoders" for each sentence where there are instance where the parameter for these phrases will be set close to zero to disable the phrases and these phrases would be a good intrinsic evaluation of the pRNN in addition to evaluating it purely on perplexity and BLEU extrinsically. The usage of attention mechanism without some sort of pruning might be problematic at the phrasal level. The author have opted for some sort of greedy pruning as described in the caption of figure 4. But I support given a fixed set of phrase pairs at train time, the attention mechanism at the phrasal level can be pre-computed but at inference (apply the attention on new data at test time), this might be kind of problematic when the architecture is scaled to a larger dataset. **Empirical**: One issue with the language modelling experiment is the choice of evaluation and train set. Possibly a dataset like common crawl or enwiki8 would be more appropriate for language modelling experiments. The main issue of the paper is in the experiments and results reporting, it needs quite a bit of reworking. - The evaluation on PTB (table 2) isn't a fair one since the model was trained on a larger corpus (FBIS) and then tested on PTB. The fact that the previous study reported a 126 perplexity baseline using LSTM and the LSTM's perplexity of 106.9 provided by the author showed that the FBIS gives an advantage to computing the language model's perplexity when tested on PTB. - Also, regarding section 3.3, please cite appropriate publications the "previous work" presented in the tables. And are the previous work using the same training set? - Additionally, why isn't the the GRU version of pRNNv reported in the FBIS evaluation in Table 3? The result section cannot be simply presenting a table without explanation: - Still on the result sections, although it's clear that BLEU and perplexity are objective automatic measure to evaluate the new architecture. It's not really okay to put up the tables and show the perplexity and BLEU scores without some explanation. E.g. in Table 2, it's necessary to explain why the LSTM's perplexity from previous work is higher than the author's implementation. Same in Table 3. The result presented in Table 4 don't match the description in Section 4.3: - It's not true that the pRNN outperforms both PBSMT and Enc-Dec model. The authors should make it clear that on different evaluation sets, the scores differs. And it's the averaged test scores that pRNN performs better - Please also make it clear whether the "Test Avg." is a micro-average (all testsets are concatenated and evaluated as one set) or macro-average (average taken across the scores of individual test sets) score. For table 4, please also include the significance of the BLEU improvement made by the pRNN with respect to the the baseline, see https://github.com/jhclark/multeval General Discussion ==== As the main contribution of this work is on the phrasal effect of the new RNN architecture, it's
The paper presents two approaches for generating English poetry. The first approach combine a neural phonetic encoder predicting the next phoneme with a phonetic-orthographic HMM decoder computing the most likely word corresponding to a sequence of phonemes. The second approach combines a character language model with a weigthed FST to impose rythm constraints on the output of the language model. For the second approach, the authors also present a heuristic approach which permit constraining the generated poem according to theme (e.g;, love) or poetic devices (e.g., alliteration). The generated poems are evaluated both instrinsically by comparing the rythm of the generated lines with a gold standard and extrinsically by asking 70 human evaluators to (i) determine whether the poem was written by a human or a machine and (ii) rate poems wrt to readability, form and evocation. The results indicate that the second model performs best and that human evaluators find it difficult to distinguish between human written and machine generated poems. This is an interesting, clearly written article with novel ideas (two different models for poetry generation, one based on a phonetic language model the other on a character LM) and convincing results. For the evaluation, more precision about the evaluators and the protocol would be good. Did all evaluators evaluate all poems and if not how many judgments were collected for each poem for each task ? You mention 9 non English native speakers. Poems are notoriously hard to read. How fluent were these ? In the second model (character based), perhaps I missed it, but do you have a mechanism to avoid generating non words ? If not, how frequent are non words in the generated poems ? In the first model, why use an HMM to transliterate from phonetic to an orhographic representation rather than a CRF? Since overall, you rule out the first model as a good generic model for generating poetry, it might have been more interesting to spend less space on that model and more on the evaluation of the second model. In particular, I would have been interested in a more detailed discussion of the impact of the heuristic you use to constrain theme or poetic devices. How do these impact evaluation results ? Could they be combined to jointly constrain theme and poetic devices ? The combination of a neural mode with a WFST is reminiscent of the following paper which combine character based neural model to generate from dialog acts with an WFST to avoid generating non words. YOu should relate your work to theirs and cite them. Natural Language Generation through Character-Based RNNs with Finite-State Prior Knowledge Goyal, Raghav and Dymetman, Marc and Gaussier, Eric and LIG, Uni COLING 2016
The paper describes two methodologies for the automatic generation of rhythmic poetry. Both rely on neural networks, but the second one allows for better control of form. - Strengths: Good procedure for generating rhythmic poetry. Proposals for adding control of theme and poetic devices (alliteration, consonance, asonance). Strong results in evaluation of rhythm. - Weaknesses: Poor coverage of existing literature on poetry generation. No comparison with existing approaches to poetry generation. No evaluation of results on theme and poetic devices. - General Discussion: The introduction describes the problem of poetry generation as divided into two subtasks: the problem of content (the poem's semantics) and the problem of form (the aesthetic rules the poem follows). The solutions proposed in the paper address both of these subtasks in a limited fashion. They rely on neural networks trained over corpora of poetry (represented at the phonetic or character level, depending on the solution) to encode the linguistic continuity of the outputs. This does indeed ensure that the outputs resemble meaningful text. To say that this is equivalent to having found a way of providing the poem with appropriate semantics would be an overstatement. The problem of form can be said to be addressed for the case of rhythm, and partial solutions are proposed for some poetic devices. Aspects of form concerned with structure at a larger scale (stanzas and rhyme schemes) remain beyond the proposed solutions. Nevertheless, the paper constitutes a valuable effort in the advancement of poetry generation. The review of related work provided in section 2 is very poor. It does not even cover the set of previous efforts that the authors themselves consider worth mentioning in their paper (the work of Manurung et al 2000 and Misztal and Indurkhya 2014 is cited later in the paper - page 4 - but it is not placed in section 2 with respect to the other authors mentioned there). A related research effort of particular relevance that the authors should consider is: - Gabriele Barbieri, François Pachet, Pierre Roy, and Mirko Degli Esposti. 2012. Markov constraints for generating lyrics with style. In Proceedings of the 20th European Conference on Artificial Intelligence (ECAI'12), Luc De Raedt, Christian Bessiere, Didier Dubois, Patrick Doherty, and Paolo Frasconi (Eds.). IOS Press, Amsterdam, The Netherlands, The Netherlands, 115-120. DOI: https://doi.org/10.3233/978-1-61499-098-7-115 This work addresses very similar problems to those discussed in the present paper (n-gram based generation and the problem of driving generation process with additional constraints). The authors should include a review of this work and discuss the similarities and differences with their own. Another research effort that is related to what the authors are attempting (and has bearing on their evaluation process) is: - Stephen McGregor, Matthew Purver and Geraint Wiggins, Process Based Evaluation of Computer Generated Poetry, in: Proceedings of the INLG 2016 Workshop on Computational Creativity and Natural Language Generation, pages 51–60,Edinburgh, September 2016.c2016 Association for Computational Linguistics This work is also similar to the current effort in that it models language initially at a phonological level, but considers a word n-gram level superimposed on that, and also features a layer representint sentiment. Some of the considerations McGregor et al make on evaluation of computer generated poetry are also relevant for the extrinsic evaluation described in the present paper. Another work that I believe should be considered is: - "Generating Topical Poetry" (M. Ghazvininejad, X. Shi, Y. Choi, and K. Knight), Proc. EMNLP, 2016. This work generates iambic pentameter by combining finite-state machinery with deep learning. It would be interesting to see how the proposal in the current paper constrasts with this particular approach. Although less relevant to the present paper, the authors should consider extending their classification of poetry generation systems (they mention rule-based expert systems and statistical approaches) to include evolutionary solutions. They already mention in their paper the work of Manurung, which is evolutionary in nature, operating over TAG grammars. In any case, the paper as it stands holds little to no effort of comparison to prior approaches to poetry generation. The authors should make an effort to contextualise their work with respect to previous efforts, specially in the case were similar problems are being addressed (Barbieri et al, 2012
This paper introduces new configurations and training objectives for neural sequence models in a multi-task setting. As the authors describe well, the multi-task setting is important because some tasks have shared information and in some scenarios learning many tasks can improve overall performance. The methods section is relatively clear and logical, and I like where it ended up, though it could be slightly better organized. The organization that I realized after reading is that there are two problems: 1) shared features end up in the private feature space, and 2) private features end up in the shared space. There is one novel method for each problem. That organization up front would make the methods more cohesive. In any case, they introduce one method that keeps task-specific features out of shared representation (adversarial loss) and another to keep shared features out of task-specific representations (orthogonality constraints). My only point of confusion is the adversarial system. After LSTM output there is another layer, D(sk_T, theta_D), relying on parameters U and b. This output is considered a probability distribution which is compared against the actual. This means it is possible it will just learn U and b that effectively mask task-specific information from the LSTM outputs, and doesn't seem like it can guarantee task-specific information is removed. Before I read the evaluation section I wrote down what I hoped the experiments would look like and it did most of it. This is an interesting idea and there are a lot more experiments one can imagine but I think here they have the basics to show the validity of their methods. It would be helpful to have best known results on these tasks. My primary concern with this paper is the lack of deeper motivation for the approach. I think it is easy to understand that in a totally shared model there will be problems due to conflicts in feature space. The extension to partially shared features seems like a reaction to that issue -- one would expect that the useful shared information is in the shared latent space and each task-specific space would learn features for that space. Maybe this works and maybe it doesn't, but the logic is clear to me. In contrast, the authors seem to start from the assumption that this "shared-private" model has this issue. I expected the argument flow to be 1) Fully-shared obviously has this problem; 2) shared-private seems to address this; 3) in practice shared-private does not fully address this issue for reasons a,b,c.; 4) we introduce a method that more effectively constrains the spaces. Table 4 helped me to partially understand what's going wrong with shared-private and what your methods do; some terms are _usually_ one connotation or another, and that general trend can probably get them into the shared feature space. This simple explanation, an example, and a more logical argument flow would help the introduction and make this a really nice reading paper. Finally, I think this research ties into some other uncited MTL work [1], which does deep hierarchical MTL - supervised POS tagging at a lower level, chunking at the next level up, ccg tagging higher, etc. They then discuss at the end some of the qualities that make MTL possible and conclude that MTL only works "when tasks are sufficiently similar." The ASP-MTL paper made me think of this previous work because potentially this model could learn what sufficiently similar is -- i.e., if two tasks are not sufficiently similar the shared model would learn nothing and it would fall back to learning two independent systems, as compared to a shared-private model baseline that might overfit and perform poorly. [1] @inproceedingssogaard2016deep, title=Deep multi-task learning with low level tasks supervised at lower layers, author=Sogaard, Anders and Goldberg, Yoav, booktitle=Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, volume=2, pages=231--235, year=2016, organization=Association for Computational Linguistics
# Paper summary This paper presents a method for learning well-partitioned shared and task-specific feature spaces for LSTM text classifiers. Multiclass adversarial training encourages shared space representations from which a discriminative classifier cannot identify the task source (and are thus generic). The models evaluates are a fully-shared, shared-private and adversarial shared-private -- the lattermost ASP model is one of the main contributions. They also use orthogonality constraints to help reward shared and private spaces that are distinct. The ASP model has lower error rate than single-task and other multi-task neural models. They also experiment with a task-level cross validation to explore whether the shared representation can transfer across tasks, and it seems to favourably. Finally, there is some analysis of shared layer activations suggesting that the ASP model is not being misled by strong weights learned on a specific (inappropriate) task. # Review summary Good ideas, well expressed and tested. Some minor comments. # Strengths * This is a nice set of ideas working well together. I particularly like the focus on explicitly trying to create useful shared representations. These have been quite successful in the CV community, but it appears that one needs to work quite hard to create them for NLP. * Sections 2, 3 and 4 are very clearly expressed. * The task-level cross-validation in Section 5.5 is a good way to evaluate the transfer. * There is an implementation and data. # Weaknesses * There are a few minor typographic and phrasing errors. Individually, these are fine, but there are enough of them to warrant fixing: ** l:84 the “infantile cart” is slightly odd -- was this a real example from the data? ** l:233 “are different in” -> “differ in” ** l:341 “working adversarially towards” -> “working against” or “competing with”? ** l:434 “two matrics” -> “two matrices” ** l:445 “are hyperparameter” -> “are hyperparameters” ** Section 6 has a number of number agreement errors (l:745/746/765/766/767/770/784) and should be closely re-edited. ** The shading on the final row of Tables 2 and 3 prints strangely... * There is mention of unlabelled data in Table 1 and semi-supervised learning in Section 4.2, but I didn’t see any results on these experiments. Were they omitted, or have I misunderstood? * The error rate differences are promising in Tables 2 and 3, but statistical significance testing would help make them really convincing. Especially between SP-MLT and ASP-MTL results to highlight the benefit of adversarial training. It should be pretty straightforward to adapt the non-parametric approximate randomisation test (see http://www.lr.pi.titech.ac.jp/takamura/pubs/randtest.pdf for promising notes a reference to the Chinchor paper) to produce these. * The colours are inconsistent in the caption of Figure 5 (b). In 5 (a), blue is used for “Ours”, but this seems to have swapped for 5 (b). This is worth checking, or I may have misunderstood the caption. # General Discussion * I wonder if there’s some connection with regularisation here, as the effect of the adversarial training with orthogonal training is to help limit the shared feature space. It might be worth drawing that connection to other regularisation literature.
- Strengths: The paper makes several novel contributions to (transition-based) dependency parsing by extending the notion of non-monotonic transition systems and dynamic oracles to unrestricted non-projective dependency parsing. The theoretical and algorithmic analysis is clear and insightful, and the paper is admirably clear. - Weaknesses: Given that the main motivation for using Covington's algorithm is to be able to recover non-projective arcs, an empirical error analysis focusing on non-projective structures would have further strengthened the paper. And even though the main contributions of the paper are on the theoretical side, it would have been relevant to include a comparison to the state of the art on the CoNLL data sets and not only to the monotonic baseline version of the same parser. - General Discussion: The paper extends the transition-based formulation of Covington's dependency parsing algorithm (for unrestricted non-projective structures) by allowing non-monotonicity in the sense that later transitions can change structure built by earlier transitions. In addition, it shows how approximate dynamic oracles can be formulated for the new system. Finally, it shows experimentally that the oracles provide a tight approximation and that the non-monotonic system leads to improved parsing accuracy over its monotonic counterpart for the majority of the languages included in the study. The theoretical contributions are in my view significant enough to merit publication, but I also think the paper could be strengthened on the empirical side. In particular, it would be relevant to investigate, in an error analysis, whether the non-monotonic system improves accuracy specifically on non-projective structures. Such an analysis can be motivated on two grounds: (i) the ability to recover non-projective structures is the main motivation for using Covington's algorithm in the first place; (ii) non-projective structures often involved long-distance dependencies that are hard to predict for a greedy transition-based parser, so it is plausible that the new system would improve the situation. Another point worth discussion is how the empirical results relate to the state of the art in light of recent improvements thanks to word embeddings and neural network techniques. For example, the non-monotonicity is claimed to mitigate the error propagation typical of classical greedy transition-based parsers. But another way of mitigating this problem is to use recurrent neural networks as preprocessors to the parser in order to capture more of the global sentence context in word representations. Are these two techniques competing or complementary? A full investigation of these issues is clearly outside the scope of the paper, but some discussion would be highly relevant. Specific questions: Why were only 9 out of the 13 data sets from the CoNLL-X shared task used? I am sure there is a legitimate reason and stating it explicitly may prevent readers from becoming suspicious. Do you have any hypothesis about why accuracy decreases for Basque with the non-monotonic system? Similar (but weaker) trends can be seen also for Turkish, Catalan, Hungarian and (perhaps) German. How do your results compare to the state of the art on these data sets? This is relevant for contextualising your results and allowing readers to estimate the significance of your improvements. Author response: I am satisfied with the author's response and see no reason to change my previous review.
