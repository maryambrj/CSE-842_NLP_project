{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe046d09-9f5b-4d8c-9a28-40413066b983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Feedback:\n",
      " \n",
      "\n",
      "Feedback for the new essay:\n",
      "\n",
      "- Strengths:\n",
      "\n",
      "  1) The introduction of ngrams into existing word representation methods is a novel approach.\n",
      "  \n",
      "  2) The paper presents comprehensive experiments on word analogy and similarity tasks, which are crucial for evaluating the effectiveness of the proposed method.\n",
      "  \n",
      "  3) The demonstration of the usefulness of trained ngram representations in finding antonyms and collocations is a significant contribution.\n",
      "  \n",
      "  4) The proposed approach for building the co-occurrence matrix to alleviate hardware burden is practical and valuable.\n",
      "\n",
      "- Weaknesses:\n",
      "\n",
      "  1) The paper lacks a detailed explanation of how ngrams are integrated into the existing methods (SGNS, GloVe, PPMI matrix, and its SVD factorization). A more in-depth description of the methodology would strengthen the paper.\n",
      "  \n",
      "  2) The paper does not provide a clear comparison with state-of-the-art methods. It is essential to show how the proposed method compares to the current best-performing models in word representation tasks.\n",
      "  \n",
      "  3) The paper could benefit from a more thorough analysis of the results. For instance, discussing why the inclusion of ngrams improves word representations and how the improvements vary across different tasks would add depth to the paper.\n",
      "  \n",
      "  4) The paper does not address potential limitations or drawbacks of the proposed method. A discussion on scenarios where the method might not perform well or areas for future improvement would provide a more balanced view.\n",
      "\n",
      "- General Discussion:\n",
      "\n",
      "This paper introduces a novel approach to word representation by incorporating ngram co-occurrence statistics into existing methods. The experiments conducted are comprehensive, and the results are promising. However, the paper could be improved by providing more details on the methodology, comparing the proposed method with state-of-the-art models, and conducting a deeper analysis of the results. Additionally, discussing potential limitations and future work would make the paper more robust. Overall, the idea is innovative and has the potential to contribute significantly to the field of word representation.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Step 2: Configure API access for DeepSeek\n",
    "DEEPSEEK_API_ENDPOINT = \"https://api.deepseek.com/beta/v1/completions\"\n",
    "API_KEY = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "# Step 3: Function to extract data from the given data_split_path\n",
    "def extract_data(data_split_path):\n",
    "    reviews_path = os.path.join(data_split_path, 'reviews')\n",
    "    parsed_pdfs_path = os.path.join(data_split_path, 'parsed_pdfs')\n",
    "    data_list = []\n",
    "\n",
    "    if not os.path.exists(reviews_path) or not os.path.exists(parsed_pdfs_path):\n",
    "        logging.error(\"Reviews path or parsed PDFs path does not exist.\")\n",
    "        return data_list\n",
    "\n",
    "    for review_filename in os.listdir(reviews_path):\n",
    "        if review_filename.endswith('.json'):\n",
    "            review_file_path = os.path.join(reviews_path, review_filename)\n",
    "            with open(review_file_path, 'r', encoding='utf-8') as f:\n",
    "                paper_data = json.load(f)\n",
    "\n",
    "            paper_id = os.path.splitext(review_filename)[0]\n",
    "            parsed_pdf_file = os.path.join(parsed_pdfs_path, f\"{paper_id}.pdf.json\")\n",
    "\n",
    "            if not os.path.exists(parsed_pdf_file):\n",
    "                continue\n",
    "\n",
    "            with open(parsed_pdf_file, 'r', encoding='utf-8') as f_pdf:\n",
    "                parsed_pdf = json.load(f_pdf)\n",
    "                metadata = parsed_pdf.get('metadata', {})\n",
    "                title = metadata.get('title', 'No title')\n",
    "                abstract_text = metadata.get('abstractText', '')\n",
    "                sections = parsed_pdf.get('pdf_parse', {}).get('body_text', [])\n",
    "\n",
    "                # Combine section texts\n",
    "                section_texts = ' '.join(\n",
    "                    f\"{section.get('section', '')}: {section.get('text', '')}\"\n",
    "                    for section in sections\n",
    "                )\n",
    "                full_body = f\"{title} {abstract_text} {section_texts}\".strip()\n",
    "\n",
    "            # Extract reviews\n",
    "            reviews = paper_data.get('reviews', [])\n",
    "            review_texts = [\n",
    "                review.get('comments', '').strip()\n",
    "                for review in reviews if review.get('comments', '').strip()\n",
    "            ]\n",
    "\n",
    "            if review_texts:\n",
    "                data_list.append({\n",
    "                    'title': title,\n",
    "                    'abstract': abstract_text,\n",
    "                    'paper_content': full_body,\n",
    "                    'reviews': review_texts\n",
    "                })\n",
    "\n",
    "    return data_list\n",
    "\n",
    "# Step 4: Function to generate responses using DeepSeek API\n",
    "def generate_response_with_deepseek(prompt, model=\"deepseek-chat\", max_tokens=700): \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(DEEPSEEK_API_ENDPOINT, headers=headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        return response.json().get(\"choices\", [{}])[0].get(\"text\", \"\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Request failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# MODIFIED THIS \n",
    "def generate_few_shot_feedback(paper_text, example_essays, max_examples=3):\n",
    "    # Limit the number of examples\n",
    "    selected_examples = example_essays[:max_examples]\n",
    "\n",
    "    # Build the prompt with examples\n",
    "    prompt = \"You are an expert reviewer. Provide detailed feedback for the essays.\\n\\n\"\n",
    "    for idx, example in enumerate(selected_examples):\n",
    "        prompt += (\n",
    "            f\"Essay {idx + 1}:\\n{example['essay']}\\n\"\n",
    "            f\"Feedback {idx + 1}:\\n{example['feedback']}\\n\\n\"\n",
    "        )\n",
    "    prompt += f\"Now, here is a new essay:\\n{paper_text}\\nProvide detailed feedback for this essay.\"\n",
    "\n",
    "    # Generate the feedback using DeepSeek API\n",
    "    feedback = generate_response_with_deepseek(prompt)\n",
    "    return feedback\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the path where data is stored\n",
    "    data_split_path = './data/acl_2017/train'\n",
    "\n",
    "    # Extract data using the extract_data function\n",
    "    extracted_data = extract_data(data_split_path)\n",
    "    if not extracted_data:\n",
    "        logging.error(\"No data extracted.\")\n",
    "        exit()\n",
    "\n",
    "    # Prepare example essays and feedback from extracted data\n",
    "    example_essays = []\n",
    "    new_essays = []\n",
    "    for data_item in extracted_data:\n",
    "        if 'paper_content' in data_item and 'reviews' in data_item:\n",
    "            for review in data_item['reviews']:\n",
    "                example_essays.append({\n",
    "                    \"essay\": data_item['paper_content'],\n",
    "                    \"feedback\": review\n",
    "                })\n",
    "            new_essays.append(data_item['paper_content'])\n",
    "\n",
    "    # Shuffle and select examples\n",
    "    random.shuffle(example_essays)\n",
    "    example_essays = example_essays[:3]\n",
    "\n",
    "    # Select a new essay not in examples\n",
    "    new_essay_candidates = [\n",
    "        essay for essay in new_essays\n",
    "        if essay not in [ex['essay'] for ex in example_essays]\n",
    "    ]\n",
    "    if not new_essay_candidates:\n",
    "        logging.error(\"No new essay available for feedback generation.\")\n",
    "        exit()\n",
    "    new_essay = random.choice(new_essay_candidates)\n",
    "\n",
    "    # Generate feedback\n",
    "    feedback = generate_few_shot_feedback(new_essay, example_essays)\n",
    "    if feedback:\n",
    "        print(\"Generated Feedback:\\n\", feedback)\n",
    "    else:\n",
    "        logging.error(\"Feedback generation failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31e7b3c-c0b1-45e6-9cee-f673ccd1b100",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
