{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in ./CSE842/venv/lib/python3.12/site-packages (8.1.5)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./CSE842/venv/lib/python3.12/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./CSE842/venv/lib/python3.12/site-packages (from ipywidgets) (8.29.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./CSE842/venv/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in ./CSE842/venv/lib/python3.12/site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in ./CSE842/venv/lib/python3.12/site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: decorator in ./CSE842/venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./CSE842/venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in ./CSE842/venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in ./CSE842/venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./CSE842/venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in ./CSE842/venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in ./CSE842/venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./CSE842/venv/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./CSE842/venv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./CSE842/venv/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./CSE842/venv/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./CSE842/venv/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in ./CSE842/venv/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: six>=1.12.0 in ./CSE842/venv/lib/python3.12/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 12:41:02.882 python[8209:189268] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2024-11-28 12:41:02.882 python[8209:189268] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import requests\n",
    "import logging\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import sacrebleu\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "#import Moverscore_v2 as mv\n",
    "from dotenv import load_dotenv\n",
    "import nltk\n",
    "from tqdm import TqdmWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=TqdmWarning)\n",
    "load_dotenv()\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepSeek API configuration\n",
    "DEEPSEEK_API_ENDPOINT = \"https://api.deepseek.com/beta/v1/completions\"\n",
    "API_KEY = os.getenv('DEEPSEEK_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(data_split_path):\n",
    "    reviews_path = os.path.join(data_split_path, 'reviews')\n",
    "    parsed_pdfs_path = os.path.join(data_split_path, 'parsed_pdfs')\n",
    "    data_list = []\n",
    "\n",
    "    if not os.path.exists(reviews_path) or not os.path.exists(parsed_pdfs_path):\n",
    "        logging.error(\"Reviews path or parsed PDFs path does not exist.\")\n",
    "        return data_list\n",
    "\n",
    "    for review_filename in os.listdir(reviews_path):\n",
    "        if review_filename.endswith('.json'):\n",
    "            review_file_path = os.path.join(reviews_path, review_filename)\n",
    "            with open(review_file_path, 'r', encoding='utf-8') as f:\n",
    "                paper_data = json.load(f)\n",
    "\n",
    "            paper_id = os.path.splitext(review_filename)[0]\n",
    "            parsed_pdf_file = os.path.join(parsed_pdfs_path, f\"{paper_id}.pdf.json\")\n",
    "\n",
    "            if not os.path.exists(parsed_pdf_file):\n",
    "                continue\n",
    "\n",
    "            with open(parsed_pdf_file, 'r', encoding='utf-8') as f_pdf:\n",
    "                parsed_pdf = json.load(f_pdf)\n",
    "                metadata = parsed_pdf.get('metadata', {})\n",
    "                title = metadata.get('title', 'No title')\n",
    "                abstract_text = metadata.get('abstractText', '')\n",
    "                sections = parsed_pdf.get('pdf_parse', {}).get('body_text', [])\n",
    "\n",
    "                section_texts = ' '.join(\n",
    "                    f\"{section.get('section', '')}: {section.get('text', '')}\"\n",
    "                    for section in sections\n",
    "                )\n",
    "                full_body = f\"{title} {abstract_text} {section_texts}\".strip()\n",
    "\n",
    "            reviews = paper_data.get('reviews', [])\n",
    "            review_texts = [\n",
    "                review.get('comments', '').strip()\n",
    "                for review in reviews if review.get('comments', '').strip()\n",
    "            ]\n",
    "\n",
    "            if review_texts:\n",
    "                data_list.append({\n",
    "                    'title': title,\n",
    "                    'abstract': abstract_text,\n",
    "                    'paper_content': full_body,\n",
    "                    'reviews': review_texts\n",
    "                })\n",
    "\n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_with_deepseek(prompt, model=\"deepseek-chat\", max_tokens=1000):\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(DEEPSEEK_API_ENDPOINT, headers=headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        return response.json().get(\"choices\", [{}])[0].get(\"text\", \"\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Request failed: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_few_shot_feedback(paper_text, example_essays, max_examples=3):\n",
    "    selected_examples = example_essays[:max_examples]\n",
    "    prompt = \"You are an expert reviewer. Provide detailed feedback for the essays.\\n\\n\"\n",
    "    for idx, example in enumerate(selected_examples):\n",
    "        prompt += (\n",
    "            f\"Essay {idx + 1}:\\n{example['essay']}\\n\"\n",
    "            f\"Feedback {idx + 1}:\\n{example['feedback']}\\n\\n\"\n",
    "        )\n",
    "    prompt += f\"Now, here is a new essay:\\n{paper_text}\\nProvide detailed feedback for this essay.\"\n",
    "    feedback = generate_response_with_deepseek(prompt)\n",
    "    return feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ground_truth_file(reviews_path, output_file):\n",
    "    ground_truths = []\n",
    "    if not os.path.exists(reviews_path):\n",
    "        print(f\"Reviews path {reviews_path} does not exist.\")\n",
    "        return\n",
    "\n",
    "    for review_filename in os.listdir(reviews_path):\n",
    "        if review_filename.endswith('.json'):\n",
    "            review_file_path = os.path.join(reviews_path, review_filename)\n",
    "            with open(review_file_path, 'r', encoding='utf-8') as f:\n",
    "                paper_data = json.load(f)\n",
    "\n",
    "            reviews = paper_data.get('reviews', [])\n",
    "            for review in reviews:\n",
    "                if 'comments' in review:\n",
    "                    ground_truths.append({\n",
    "                        'true_feedback': review['comments'].strip()\n",
    "                    })\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(ground_truths, f, indent=4, ensure_ascii=False)\n",
    "    print(f\"Ground truth file saved as '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add these imports at the top of your script\n",
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(predictions_file, ground_truth_file):\n",
    "    with open(predictions_file, 'r', encoding='utf-8') as f:\n",
    "        predictions = json.load(f)\n",
    "\n",
    "    with open(ground_truth_file, 'r', encoding='utf-8') as f:\n",
    "        ground_truths = json.load(f)\n",
    "\n",
    "    bleu_scores = []\n",
    "    rouge_scores = []\n",
    "    meteor_scores = []\n",
    "    gleu_scores = []\n",
    "    # Moverscore_scores = []\n",
    "\n",
    "    # Initialize the ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "    for pred, truth in zip(predictions, ground_truths):\n",
    "        pred_feedback = pred['predicted_feedback']\n",
    "        true_feedback = truth['true_feedback']\n",
    "        if not pred_feedback or not true_feedback:\n",
    "            continue  # Skip empty cases\n",
    "\n",
    "        from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "        smoothie = SmoothingFunction().method1  # You can choose other methods as well\n",
    "\n",
    "        bleu_score = sentence_bleu(\n",
    "            [true_feedback.split()], \n",
    "            pred_feedback.split(), \n",
    "            smoothing_function=smoothie)\n",
    "\n",
    "        # Compute BLEU score\n",
    "        #bleu_score = sentence_bleu([true_feedback.split()], pred_feedback.split())\n",
    "        bleu_scores.append(bleu_score)\n",
    "\n",
    "        # Compute ROUGE score\n",
    "        rouge_score = scorer.score(true_feedback, pred_feedback)\n",
    "        rouge_scores.append(rouge_score)\n",
    "\n",
    "        # Tokenize before using METEOR\n",
    "        true_tokens = word_tokenize(true_feedback.lower())\n",
    "        pred_tokens = word_tokenize(pred_feedback.lower())\n",
    "\n",
    "        meteor = meteor_score([true_tokens], pred_tokens)\n",
    "        meteor_scores.append(meteor)\n",
    "\n",
    "        # Compute GLEU score using NLTK\n",
    "        reference_tokens = [true_tokens]  # NLTK expects a list of reference token lists\n",
    "        hypothesis_tokens = pred_tokens\n",
    "        gleu_score = sentence_gleu(reference_tokens, hypothesis_tokens)\n",
    "        gleu_scores.append(gleu_score)\n",
    "\n",
    "    # Calculate averages as before...\n",
    "    avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0\n",
    "    avg_rouge = {\n",
    "        key: sum(score[key].fmeasure for score in rouge_scores) / len(rouge_scores) if rouge_scores else 0\n",
    "        for key in rouge_scores[0]\n",
    "    }\n",
    "    avg_meteor = sum(meteor_scores) / len(meteor_scores) if meteor_scores else 0\n",
    "    avg_gleu = sum(gleu_scores) / len(gleu_scores) if gleu_scores else 0\n",
    "\n",
    "    print(f\"Evaluation for {predictions_file}:\")\n",
    "    print(f\"Average BLEU Score: {avg_bleu}\")\n",
    "    print(f\"Average ROUGE Scores: {avg_rouge}\")\n",
    "    print(f\"Average METEOR Score: {avg_meteor}\")\n",
    "    print(f\"Average GLEU Score: {avg_gleu}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth file saved as 'ground_truths_conll_2016.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for predictions_conll_2016.json:\n",
      "Average BLEU Score: 0.006195286224538256\n",
      "Average ROUGE Scores: {'rouge1': 0.2726837883582309, 'rougeL': 0.12571442588368043}\n",
      "Average METEOR Score: 0.24545376522767928\n",
      "Average GLEU Score: 0.06414602078967911\n",
      "Ground truth file saved as 'ground_truths_acl_2017.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for predictions_acl_2017.json:\n",
      "Average BLEU Score: 0.008030556143367505\n",
      "Average ROUGE Scores: {'rouge1': 0.3871730133105085, 'rougeL': 0.16380530317289382}\n",
      "Average METEOR Score: 0.24615929728705915\n",
      "Average GLEU Score: 0.09851655413118575\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "def load_bleurt_model():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bleurt-base-128\", use_auth_token=\"paste hugging face token\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"bleurt-base-128\", use_auth_token=\"paste hugging face token\")\n",
    "    bleurt_scorer = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, return_all_scores=True)\n",
    "    return bleurt_scorer\n",
    "'''\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #folders = ['iclr_2017', 'conll_2016', 'acl_2017']\n",
    "    folders = ['conll_2016', 'acl_2017','iclr_2017']\n",
    "    base_path = 'PeerRead/data'\n",
    "\n",
    "    for folder_name in folders:\n",
    "        test_data_path = os.path.join(base_path, folder_name, 'test')\n",
    "        reviews_path = os.path.join(test_data_path, 'reviews')\n",
    "        ground_truth_file = f'ground_truths_{folder_name}.json'\n",
    "        create_ground_truth_file(reviews_path, ground_truth_file)\n",
    "\n",
    "        train_data_path = os.path.join(base_path, folder_name, 'train')\n",
    "        train_data = extract_data(train_data_path)\n",
    "        if not train_data:\n",
    "            logging.error(f\"No training data extracted for {folder_name}.\")\n",
    "            continue\n",
    "\n",
    "        example_essays = []\n",
    "        for data_item in train_data:\n",
    "            if 'paper_content' in data_item and 'reviews' in data_item:\n",
    "                for review in data_item['reviews']:\n",
    "                    example_essays.append({\n",
    "                        \"essay\": data_item['paper_content'],\n",
    "                        \"feedback\": review\n",
    "                    })\n",
    "        random.shuffle(example_essays)\n",
    "        example_essays = example_essays[:3]\n",
    "\n",
    "        test_data = extract_data(test_data_path)\n",
    "        if not test_data:\n",
    "            logging.error(f\"No test data extracted for {folder_name}.\")\n",
    "            continue\n",
    "\n",
    "        predictions = []\n",
    "        for data_item in test_data:\n",
    "            if 'paper_content' in data_item:\n",
    "                new_essay = data_item['paper_content']\n",
    "                feedback = generate_few_shot_feedback(new_essay, example_essays)\n",
    "                predictions.append({\n",
    "                    'paper_content': new_essay,\n",
    "                    'predicted_feedback': feedback\n",
    "                })\n",
    "\n",
    "        predictions_file = f'predictions_{folder_name}.json'\n",
    "        with open(predictions_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(predictions, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "        evaluate_metrics(predictions_file, ground_truth_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
